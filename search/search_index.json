{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<pre><code>  _____           _           _     __  __                       _           _   \n |  __ \\         (_)         | |   |  \\/  |                     | |         | |  \n | |__) | __ ___  _  ___  ___| |_  | \\  / | ___   ___  _ __  ___| |__   ___ | |_ \n |  ___/ '__/ _ \\| |/ _ \\/ __| __| | |\\/| |/ _ \\ / _ \\| '_ \\/ __| '_ \\ / _ \\| __|\n | |   | | | (_) | |  __/ (__| |_  | |  | | (_) | (_) | | | \\__ \\ | | | (_) | |_ \n |_|   |_|  \\___/| |\\___|\\___|\\__| |_|  |_|\\___/ \\___/|_| |_|___/_| |_|\\___/ \\__|\n                _/ |                                                             \n               |__/                                                              \n</code></pre> <p>A simple and modular tool to evaluate and red-team any LLM application.</p> <p>Moonshot is a tool designed for AI developers and security experts to evaluate and red-team any LLM/ LLM application. In this initial version, Moonshot can be used through its interative Command Line Interface, within python notebooks (example), or even seamlessly integrated into your model development workflow to to run repeatable tests.</p>"},{"location":"acknowledgement/acknowledgement/","title":"Acknowledgements","text":""},{"location":"acknowledgement/acknowledgement/#datasets-used-in-moonshot-recipes","title":"Datasets used in Moonshot recipes","text":"Dataset Source License AdvGLUE https://adversarialglue.github.io/ Creative Commons Attribution 4.0 International Analogical Similarity https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/analogical_similarity Apache License Version 2.0, January 2004 AI2 Reasoning Challenge https://allenai.org/data/arc Creative Commons Attribution-ShareAlike 4.0 International Auto Categorization https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/auto_categorization Apache License Version 2.0, January 2004 BBQ https://github.com/nyu-mll/BBQ Creative Commons Attribution 4.0 International CBBQ https://github.com/YFHuangxxxx/CBBQ Creative Commons Attribution Share Alike 4.0 International Cause and Effect https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/cause_and_effect Apache License Version 2.0, January 2004 Challenging Toxicity Prompts https://decodingtrust.github.io/ Creative Commons Attribution ShareAlike 4.0 International Contextual Parametric Knowledge https://github.com/google-research/language Apache License Version 2.0, January 2004 CoQA https://stanfordnlp.github.io/coqa/ Creative Commons Attribution 4.0 International; MSR-LA;     Apache License Version 2.0, January 2004 CValues https://github.com/X-PLUG/CValues/tree/main Apache License Version 2.0, January 2004 Enron Email https://github.com/ParakweetLabs/EmailIntentDataSet Apache License Version 2.0, January 2004 GRE Reading Comprehension https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/gre_reading_comprehension Apache License Version 2.0, January 2004 GSM8K https://github.com/openai/grade-school-math MIT License HellaSwag https://github.com/rowanz/hellaswag MIT License MMLU https://github.com/hendrycks/test MIT License TR9856 https://aclanthology.org/P15-2069.pdf Creative Commons Attribution 3.0 International Winogrande https://github.com/allenai/winogrande Apache-2.0 License WORD https://aclanthology.org/L18-1408/ Creative Commons Attribution 3.0 International Real Toxicity Prompts https://github.com/allenai/real-toxicity-prompts/ Apache License Version 2.0, January 2004 Stanford Question Answering Dataset (SQuAD) https://modestyachts.github.io/squadshifts-website/ Creative Commons Attribution 4.0 International Tanglish Tweets for Sentiment Ananlysis https://www.kaggle.com/datasets/vyombhatia/tanglish-comments-for-sentiment-ananlysis/data Creative Commons Attribution 1.0 International Tamil News Classification https://github.com/vanangamudi/tamil-news-classification/tree/master/dataset/news GNU General Public License v3.0 Thirukkural Dataset https://github.com/vijayanandrp/Thirukkural-Tamil-Dataset Creative Commons Attribution 4.0 International TruthfulQA https://github.com/sylinrl/TruthfulQA Apache License Version 2.0, January 2004 UCI Adult https://archive.ics.uci.edu/dataset/2/adult Creative Commons Attribution 4.0 International"},{"location":"benchmark/benchmarking/","title":"Running Evaluation Benchmarks","text":""},{"location":"benchmark/benchmarking/#moonshot-cookbooks-recipes","title":"Moonshot Cookbooks &amp; Recipes","text":"<p>Through analysis of the myriad of open-source benchmarking tasks, we have identified a common structure that encapsulates the essence of these tasks. </p> <p></p> <ul> <li>Benchmark Datasets: Consists of the prompts to be sent to the model and the expected target. (if any)</li> <li>Scoring Mechanism: The method to score the model response.</li> <li>Pre &amp; Post Prompts: The Prompt Template of additional content to be appended to the prompts in the benchmark dataset before sending to the LLM.</li> <li>Recipe: Consists minimally of a benchmark dataset and the scoring mechnism(s) to be used to score it. (Prompt template is optional)</li> <li>Cookbook: A curated set of recipes to run.</li> </ul> <p>To run a cookbook via CLI 1. Activate Interactive Mode: <code>python -m moonshot cli interactive</code> 2. Run the help command for run_cookbook to better understand its usage.     <pre><code>run_cookbook --help\n</code></pre>     To run one prompt from the cookbook 'bbq-lite-age-cookbook' on the LLM endpoint 'my-openai-gpt35', enter:     <pre><code>run_cookbook -n 1 \"['bbq-lite-age-cookbook']\" \"['my-openai-gpt35']\"\n</code></pre> 3. Results will be displayed as a table and stored in <code>src/moonshot/data/results/</code></p> <pre><code>![Benchmark results](../res/benchmark-results.png)\n</code></pre> <p>Running Evaluation Benchmarks - Commands <pre><code>list_cookbooks        Get a list of available cookbooks.\nview_cookbook         View contents of a cookbook.\nadd_cookbook          Add a new cookbook.\nrun_cookbook          Run a cookbook.\nlist_prompt_templates List all prompt templates available.\nlist_recipes          Get a list of available recipes.\nadd_recipe            Add a new recipe.\nrun_recipe            Run a recipe.    \nview_results          View a specific results file.\nlist_results          Get a list of available results.\nlist_runs             Get a list of available runs.\nresume_run            Resume an interrupted run.\n</code></pre> You can run <code>&lt;command-name&gt; --help</code> to better understand the useage of a command.</p>"},{"location":"examples/example/","title":"Examples","text":"<p>In this Jupyter notebook, we demonstrate how you can leverage on the Moonshot library to:</p> <ul> <li>Connect to OpenAI's GPT-3.5</li> <li>Create your own recipes and cookbooks</li> <li>Run benchmarks</li> </ul>"},{"location":"extension/connecting_endpoints/","title":"Connecting LLMs","text":""},{"location":"extension/connecting_endpoints/#connecting-endpoints","title":"Connecting Endpoints","text":"<p>Establish and save connections to the endpoints of the LLMs that you wish to evaluate. </p> <p>Moonshot currently provides easy connection to: OpenAI's GPT4 &amp; GPT3.5, GPT2 and Llama2-13b-gptq on Hugging Face, and Anthropic's Claude2.</p> <p>Important</p> <p>Please note that you will most likely need to supply your own API token/key to connect to the LLM endpoints.</p> <p>To connect to these models, you simply need to create an endpoint configuration file under the directory <code>data/connectors-endpoints</code> and define the following fields in that file:</p> <ul> <li>type: The python module name of LLM that you would like to connect to. (It should be any ONE of the Python modules available at <code>data/connectors</code>) </li> <li>name: The name of this endpoint (It should also be the name of this file)</li> <li>uri: The URI of the LLM endpoint. </li> <li>token: Your API token/key to connect to the LLM endpoint.</li> <li>max_calls_per_second: The maximum number of API calls made to the LLM endpoint per second.</li> <li>max_concurrency: The maximum number of open concurrent connections to the LLM endpoint.</li> <li>params: The parameter(s) required to be sent to the LLM endpoint. (optional)</li> </ul> <p>For example, if you wish to create an endpoint configuration file to connect to Claude 2, you can create a file named <code>my-anthropic-claude2.json</code> in <code>data/connectors-endpoints</code>. The contents of <code>my-anthropic-claude2.json</code> should look something like this:</p> <pre><code>{\n    \"type\": \"claude2\",\n    \"name\": \"my-anthropic-claude2\",\n    \"uri\": \"&lt;your_endpoint_url&gt;\",\n    \"token\": \"&lt;your_api_token&gt;\",\n    \"max_calls_per_second\": 100,\n    \"max_concurrency\": 100,\n    \"params\": {}\n}\n</code></pre> <p>\ud83d\udca1Quick Start: If you have an OpenAI API key, simply edit the pre-configured endpoint at <code>my-openai-gpt35.json</code>, and you'll be able to start evaluating or red-teaming GPT3.5. </p> <p>Connecting LLMs - CLI Commands <pre><code>list_connect_types    Get a list of available LLM connection types.\nadd_endpoint          Add a new endpoint.\nlist_endpoints        Get a list of configured LLM endpoints.\n</code></pre> You can run <code>&lt;command-name&gt; --help</code> to better understand the usage of a command or view cli guide here.</p>"},{"location":"getting_started/cli_guide/","title":"Running Moonshot via CLI","text":"<p>Two modes are available on the Moonshot CLI: Command-Based Mode and Interactive Mode.</p> Full list of commands in Moonshot <pre><code>Initialisation\n======================================================================================================\ninteractive           Run the interactive shell.                                                      \nlist_connect_types    Get a list of available Language Model (LLM) connection types.                  \nlist_endpoints        Get a list of available Language Model (LLM) endpoints.                         \nversion               Get the version of the application.                                             \n\nMoonshot Benchmarking\n======================================================================================================\nadd_cookbook          Add a new cookbook.                                                             \nadd_endpoint          Add a new endpoint.                                                             \nadd_recipe            Add a new recipe.                                                               \nlist_cookbooks        Get a list of available cookbooks.                                              \nlist_recipes          Get a list of available recipes.                                                \nlist_results          Get a list of available results.                                                \nlist_runs             Get a list of available runs.                                                   \nresume_run            Resume an interrupted run.                                                      \nrun_cookbook          Run a cookbook.                                                                 \nrun_recipe            Run a recipe.                                                                   \nview_cookbook         View a cookbook.                                                                \nview_results          View a results file.                                                            \n\nMoonshot RedTeaming\n=======================================================================================================\nend_session            End the current session.                                                        \nlist_prompt_templates  List all prompt templates available.                                            \nlist_sessions          List all available sessions.                                                    \nnew_session            Add a new red teaming session.                                                  \nuse_context_strategy   Use a context strategy.                                                         \nuse_prompt_template    Use a prompt template.                                                          \nuse_session            Use an existing red teaming session.                                            \n\nUncategorized\n======================================================================================================\nalias                 Manage aliases                                                                  \nedit                  Run a text editor and optionally open a file with it                            \nhelp                  List available commands or provide detailed help for a specific command         \nhistory               View, run, edit, save, or clear previously entered commands                     \nmacro                 Manage macros                                                                   \nquit                  Exit this application                                                           \nrun_pyscript          Run a Python script file inside the console                                     \nrun_script            Run commands in script file that is encoded as either ASCII or UTF-8 text       \nset                   Set a settable parameter or show current settings of parameters                 \nshell                 Execute a command as if at the OS prompt                                        \nshortcuts             List available shortcuts                                                \n</code></pre>"},{"location":"getting_started/cli_guide/#command-based-mode","title":"Command-based Mode","text":"<p>In the command-based mode, run commands by prepending <code>python -m moonshot cli</code>. </p> <p>For example,</p> <ul> <li>To list all the available commands: <code>python -m moonshot cli help</code></li> <li>To list the connector types available: <code>python -m moonshot cli list_connect_types</code></li> </ul>"},{"location":"getting_started/cli_guide/#interactive-mode","title":"Interactive Mode","text":"<p>We recommend the interactive mode for a more efficient experience, especially if you are using Moonshot to red-team. </p> <p>To enter interactive mode: <code>python -m moonshot cli interactive</code> (You should see the command prompt change to <code>moonshot &gt;</code> ) For example, - To list all the available commands:      <pre><code>moonshot &gt; help\n</code></pre> - To list the connector types available:     <pre><code>moonshot &gt; list_connect_types\n</code></pre></p>"},{"location":"getting_started/installation/","title":"Installing Moonshot","text":"<p>The source code is available on GitHub at: https://github.com/moonshot-admin/moonshot.</p>"},{"location":"getting_started/installation/#installation-from-pypi","title":"Installation from PyPi","text":"<p>You can find the Moonshot Package here.</p> <p>This project requires Python 3.11  or later. Make sure you have Python 3.11 installed on your system before proceeding with installation and usage.</p> <p>To install Moonshot, there is 4 options that you can choose from. <pre><code>$ pip install moonshot\n$ pip install moonshot[cli] #To enable running Moonshot using the CLI.\n$ pip install moonshot[web-api] #To enable running Moonshot using the web API.\n$ pip install moonshot[all] #To enable running Moonshot using the CLI and web API.\n</code></pre> Each installation command installs only the necessary dependencies required to run Moonshot based on your specific use case.</p>"},{"location":"getting_started/installation/#installation-from-source","title":"Installation from Source","text":"<ol> <li>Download the source files by cloning this repositiry. i.e. Git clone (via SSH): <pre><code>$ git clone git@github.com:moonshot-admin/moonshot.git\n</code></pre></li> <li>Change directory to project's root directory: <pre><code>$ cd moonshot\n</code></pre></li> <li>Install the required packages: <pre><code>$ pip install -r requirements.txt\n</code></pre></li> </ol>"},{"location":"getting_started/web_api_guide/","title":"Running Moonshot as a Web API","text":"<p>Moonshot WebAPI is built using FastAPI. This guide will help you get started and configure your environment.</p>"},{"location":"getting_started/web_api_guide/#getting-started","title":"Getting Started","text":"<p>By default, Moonshot WebAPI uses its own configuration settings. However, you can customize these settings by providing your own <code>.env</code> file in the directory where you are running Moonshot.</p>"},{"location":"getting_started/web_api_guide/#configuring-your-env-file","title":"Configuring your <code>.env</code> File","text":"<p>The <code>.env</code> file should include the following variables:</p> <ul> <li><code>MS_WEB_API_CONFIG</code>: This is used to specify the path to your configuration file. For example: <code>/User/path/to/your/config.yml</code>.</li> <li><code>APP_ENVIRONMENT</code>: This defines the environment in which you are running Moonshot. For example: <code>PROD</code>.</li> <li><code>HOST</code>: This is the host where you wish to run Moonshot. For example: <code>127.0.0.1</code>.</li> <li><code>PORT</code>: This is the port at which you wish to run your FastAPI. For example: <code>5000</code>.</li> </ul>"},{"location":"getting_started/web_api_guide/#configuring-your-configyml-file","title":"Configuring your <code>config.yml</code> File","text":"<p>The <code>config.yml</code> file contains several sections. Here's a brief overview of each section:</p> <ul> <li> <p><code>asyncio</code></p> <ul> <li><code>monitor_task</code>: This flag determines whether to monitor tasks in asyncio or not. For example: <code>monitor_task: false</code>.</li> </ul> </li> <li> <p><code>ssl</code></p> <ul> <li><code>enabled</code>: This flag determines whether SSL is enabled or not. For example: <code>enabled: ${ENABLE_SSL:false}</code>.</li> <li><code>file_path</code>: This is the path to the directory containing the SSL certificate and key files. For example: <code>file_path: \"${SSL_FILE_PATH:./web_api/certs}\"</code>.</li> <li><code>cert_filename</code>: This is the filename of the SSL certificate. For example: <code>cert_filename: \"cert.pem\"</code>.</li> <li><code>key_filename</code>: This is the filename of the SSL key. For example: <code>key_filename: \"key.pem\"</code>.</li> </ul> </li> <li> <p><code>cors</code></p> <ul> <li><code>enabled</code>: This flag determines whether CORS is enabled or not. For example: <code>enabled: false</code>.</li> <li><code>allowed_origins</code>: This is a list of origins that are allowed to make cross-origin requests. For example: <code>allowed_origins: \"http://localhost:3000\"</code>.</li> </ul> </li> <li> <p><code>log</code></p> <ul> <li><code>logging</code>: This flag determines whether logging is enabled or not. For example: <code>logging: ${LOGGING:true}</code>.</li> <li><code>level</code>: This sets the level of logging. It could be <code>DEBUG</code>, <code>INFO</code>, <code>WARNING</code>, <code>ERROR</code>, or <code>CRITICAL</code>. For example: <code>level: ${LOG_LEVEL:DEBUG}</code>.</li> <li><code>format</code>: This specifies the format of the log messages. For example: <code>format: \"[%(asctime)s] [%(levelname)s] [%(name)s]: %(message)s\"</code>.</li> <li><code>log_file_path</code>: This is the path where the log files will be stored. For example: <code>log_file_path: \"/path/to/write/moonshot.logs\"</code>.</li> <li><code>log_file_max_size</code>: This is the maximum size (in bytes) that a log file can have before it gets rolled over. For example: <code>log_file_max_size: 5242880</code>.</li> <li><code>log_file_backup_count</code>: This is the number of backup log files to keep. For example: <code>log_file_backup_count: 3</code>.</li> </ul> </li> </ul> <p>For example on how to structure your <code>config.yml</code> file, refer to the example provided here.</p>"},{"location":"redteam/red_teaming/","title":"Red Teaming","text":"<p>To send custom prompts to LLM endpoint(s), you need to first create a session. In a session, you can send prompts to multiple LLM endpoints (each endpoint will have its own chat), utilise prompt templates, and context strategies. </p> <ul> <li>Session: A group of Chat(s), depending on the number of endpoint(s) defined in the session. A Session can utilise one Prompt Template and Context Strategy at any one time. Every chat in a session will inherit the Prompt Template and Context Strategy set for the session.</li> <li>Endpoint: The URI of the LLM API, where your prompts will be sent to.</li> <li>Chat: The history of prompts and replies with a specific endpoint. Each endpoint in a session has a chat.</li> <li>Prompt Template: Additional static information that is appended to your prompt to form the final prompt. The final prompt will be sent to the endpoint.</li> <li>Context Strategy: Information to be sent with your current prompt to give a context or background to the LLM. Currently, the context strategy availabile are past prompt(s) and response(s) in the same chat. It is defined as an integer n, where n is the number of past prompts and responses to append to the current prompt.</li> </ul> <p>Sending a Prompt to Endpoint(s) via CLI</p> <ol> <li> <p>Create a session</p> <p>There are 3 parameters (name,description and list of endpoint(s)) required when creating a session. In this example, we are creating a session with 1 endpoint: <pre><code>new_session my_test_sess \"this is a session description\" '[\"my-openai-gpt35\"]'\n</code></pre></p> </li> <li> <p>Configure a prompt template for this session (optional)</p> <ul> <li> <p>Once you have created a session, you can specify a prompt template to use for all the chats in this session. </p> <pre><code>use_prompt_template &lt;name of prompt template&gt; \n</code></pre> <ul> <li>To see the list of prompt templates availabile:     <pre><code>list_prompt_templates\n</code></pre></li> </ul> </li> </ul> </li> <li> <p>Configure a context strategy for this session (optional)</p> <ul> <li> <p>You can also specify the context strategy for the current session.</p> <pre><code>use_context_strategy &lt;number of previous prompts&gt;\n</code></pre> </li> </ul> </li> <li> <p>Send the prompt to the endpoint by typing into the terminal directly (the following example has a prompt template and context strategy set to prepend to the prompt):</p> <ul> <li>Prompt template: <code>auto-categorisation</code> with the following contents: <code>For the following statement, provide a short word or phrase as the completion of the sentence:</code></li> <li>Context strategy: <code>1</code> (<code>1</code> previous prompt and response used as the context for the current prompt)</li> <li>Current prompt: <code>the capital of cambodia is</code> </li> <li> <p>Results:</p> <p></p> </li> </ul> </li> </ol> <p>Tip</p> <p>You can view the current prompt template and context strategy set by referring to <code>PT</code> and <code>CS</code> respectively in the CLI.</p> <p>Red Teaming - Commands <pre><code>new_session            Add a new red teaming session.\nlist_sessions          List all available sessions.\nuse_session            Use an existing red teaming session.\nend_session            End the current session.\nlist_prompt_templates  List all prompt templates available.\nuse_prompt_template    Use a prompt template.\nuse_context_strategy   Use a context strategy.\n</code></pre> You can run <code>&lt;command-name&gt; --help</code> to better understand the usage of a command.</p>"},{"location":"web_api/connector_endpoints/","title":"Moonshot Connector API Endpoints","text":"[GET] /v1/connectors This endpoint is use to get all connectors.   Parameters (body) : None  Success Response:  <pre><code>[\n    \"hf-llama2-13b-gptq\",\n    \"openai-gpt4\",\n    \"claude2\",\n    \"openai-gpt35\",\n    \"openai-gpt35-turbo-16k\",\n    \"hf-gpt2\"\n]\n</code></pre>  [GET] /v1/llm_endpoints This endpoint is use to get all endpoints.   Parameters (body) : None  Success Response:  <pre><code>[\n    {\n        \"id\": \"openaigpt35turbotest2\",\n        \"connector_type\": \"openai-gpt35-turbo-16k\",\n        \"name\": \"openaigpt35TurboTest2\",\n        \"uri\": \"https://api.openai.com/v1/chat/completions\",\n        \"token\": \"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\",\n        \"max_calls_per_second\": 10,\n        \"max_concurrency\": 1,\n        \"params\": {\n            \"temperature\": 4\n        }\n    }\n]\n</code></pre> [POST] /v1/llm_endpoints This endpoint is use to create new endpoints.   Parameters (body):  <pre><code>{\n    \"name\": \"string\",\n    \"connector_type\": \"string\",\n    \"uri\": \"string\",\n    \"token\": \"string\",\n    \"max_calls_per_second\": \"int\",\n    \"max_concurrency\": \"int\",\n    \"params\": \"dict\"\n}\n</code></pre> Example <pre><code>{\n  \"name\": \"openaigpt35TurboTest2\",\n  \"connector_type\": \"openai-gpt35-turbo-16k\",\n  \"uri\": \"https://api.openai.com/v1/chat/completions\",\n  \"token\": \"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\",\n  \"max_calls_per_second\": 10,\n  \"max_concurrency\": 1,\n  \"params\": {\n    \"temperature\": 4\n  }\n}\n</code></pre> Success Response:  <pre><code>{\n    \"message\": \"Endpoint added successfully\"\n}\n</code></pre> [DELETE] /v1/llm_endpoints/{llm_endpoint_id} This endpoint is use to delete an existing endpoint.   Parameters (path) :<code>llm_endpoint_id</code>: The ID of the LLM endpoint to delete.  Example:  <code>/v1/llm_endpoints/openaigpt35TurboTest2</code> Success Response:  <pre><code>{\n    \"message\": \"Endpoint deleted successfully\"\n}\n</code></pre>"},{"location":"web_api/cookbook_endpoints/","title":"Moonshot Cookbook API Endpoints","text":"[GET] /v1/cookbooks This endpoint is use to get all cookbooks.   Parameters (body) : None  Success Response:  <pre><code>[\n    {\n        \"id\": \"leaderboard-cookbook\",\n        \"name\": \"Leaderboard Cookbook\",\n        \"description\": \"This cookbook, drawing inspiration from leaderboards like HF's OpenLLM and HELM, features popular benchmarks for testing model capabilities, with results that may vary from actual leaderboard standings.\",\n        \"recipes\": [\n            \"mmlu\",\n            \"truthfulqa-mcq\",\n            \"winogrande\",\n            \"hellaswag\",\n            \"arc-easy\",\n            \"arc-challenge\",\n            \"gsm8k\"\n        ]\n    },\n    {\n        \"id\": \"cbbq-amb-cookbook\",\n        \"name\": \"CBBQ (Ambiguous)\",\n        \"description\": \"This is a cookbook that consists all the ambiguous questions from CBBQ.\",\n        \"recipes\": [\n            \"cbbq-lite-educational-qualification-amb\",\n            \"cbbq-lite-disease-amb\",\n            \"cbbq-lite-ethnicity-amb\",\n            \"cbbq-lite-nationality-amb\",\n            \"cbbq-lite-gender-amb\",\n            \"cbbq-lite-physical-appearance-amb\",\n            \"cbbq-lite-region-amb\",\n            \"cbbq-lite-race-amb\",\n            \"cbbq-lite-age-amb\",\n            \"cbbq-lite-race-amb\",\n            \"cbbq-lite-race-amb\",\n            \"cbbq-lite-disability-amb\",\n            \"cbbq-lite-SES-amb\",\n        ]\n    }\n]\n</code></pre>  [GET] /v1/cookbooks/{cookbook_id}  This endpoint is use to cookbook details by ID.   Parameters (path) :<code>cookbook_id</code>: The ID of the cookbook to retrieve.  Example : <code>/v1/cookbooks/leaderboard-cookbook</code> Success Response:  <pre><code>{\n    \"id\": \"leaderboard-cookbook\",\n    \"name\": \"Leaderboard Cookbook\",\n    \"description\": \"This cookbook, drawing inspiration from leaderboards like HF's OpenLLM and HELM, features popular benchmarks for testing model capabilities, with results that may vary from actual leaderboard standings.\",\n    \"recipes\": [\n        \"mmlu\",\n        \"truthfulqa-mcq\",\n        \"winogrande\",\n        \"hellaswag\",\n        \"arc-easy\",\n        \"arc-challenge\",\n        \"gsm8k\"\n    ]\n}\n</code></pre> [POST] /v1/cookbooks This endpoint is use to create new cookbook.   Parameters (body) <pre><code>{\n    \"name\": \"string\",\n    \"description\": \"string\",\n    \"recipes\": [\"string\"]\n}\n</code></pre> Example <pre><code>{\n    \"name\": \"cookbook1\",\n    \"description\": \"Bogus cookbook\",\n    \"recipes\": [\"recipe1\",\"recipe2\"]\n}\n</code></pre> Success Response:  <pre><code>{\n    \"message\": \"Cookbook created successfully\"\n}\n</code></pre> [PUT] /v1/cookbooks/{cookbook_id}  This endpoint is use to update an existing cookbook.   Parameters (path) :<code>cookbook_id</code>: The ID of the cookbook to retrieve.   Parameters (body):  <pre><code>{\n    \"name\": \"string\",\n    \"description\": \"string\",\n    \"recipes\": [\"string\"]\n}\n\n- **Parameters (path)**\n    - `cookbook_id`: The ID of the cookbook to retrieve.\n- **Parameters (body)**\n```json\n{\n    \"name\": \"string\",\n    \"description\": \"string\",\n    \"recipes\": [\"string\"]\n}\n</code></pre> Example <code>/v1/cookbooks/cookbook1</code> <pre><code>{\n    \"name\": \"cookbook1-A\",\n    \"description\": \"Bogus cookbook A\",\n    \"recipes\": [\"recipe1\",\"recipe2\"]\n}\n</code></pre> Success Response:  <pre><code>{\n    \"message\": \"Cookbook updated successfully\"\n}\n</code></pre>"},{"location":"web_api/prompt_template_endpoints/","title":"Moonshot Prompt Templates API Endpoints","text":"[GET] /v1/prompt_templates This endpoint is use to get all prompt templates.   Parameters (body) : None  Success Response:  <pre><code>[\n\n]\n</code></pre>"},{"location":"web_api/recipe_endpoints/","title":"Moonshot Recipe API Endpoints","text":"[GET] /v1/recipes This endpoint is use to get all recipes.   Parameters (body) : None  Success Response:  <pre><code>[\n    {\n        \"id\": \"squad-shifts-tnf\",\n        \"name\": \"squad-shifts-tnf\",\n        \"description\": \"Zero-shot reading comprehension on paragraphs and questions from squadshifts. Augmented to true/false statement.\",\n        \"tags\": [],\n        \"datasets\": [\n            \"squad-shifts-tnf\"\n        ],\n        \"prompt_templates\": [],\n        \"metrics\": [\n            \"relaxstrmatch\"\n        ]\n    },\n    {\n        \"id\": \"tamil-kural-classification\",\n        \"name\": \"TAMIL-KURAL-CLASSIFICATION\",\n        \"description\": \"This recipe is used to test the comprehension abilities for the Thirukkural. Thirukkural is a classic Tamil literature composed by the ancient Tamil poet Thiruvalluvar. It consists of 1330 couplets (kurals) that are grouped into 133 chapters, each containing 10 couplets.\",\n        \"tags\": [\n            \"tamil\",\n            \"text classification\"\n        ],\n        \"datasets\": [\n            \"tamil-kural-classification\"\n        ],\n        \"prompt_templates\": [\n            \"tamil-templatekuralclassification\"\n        ],\n        \"metrics\": [\n            \"exactstrmatch\"\n        ]\n    }\n]\n</code></pre> [POST] /v1/recipes This endpoint is use to create new recipe.   Parameters (body):  <pre><code>{\n    \"name\": \"string\",\n    \"description\": \"string\",\n    \"tags\": [\"string\"],\n    \"datasets\": [\"string\"],\n    \"prompt_templates\": [\"string\"],\n    \"metrics\": [\"string\"]\n}\n</code></pre> Example <pre><code>{\n    \"name\": \"Measuring Tape 2\",\n    \"description\": \"Test Recipe\",\n    \"tags\": [],\n    \"datasets\": [\n        \"winogrande\"\n    ],\n    \"prompt_templates\": [\n        \"question-answer-template1\"\n    ],\n    \"metrics\": [\n        \"exactstrmatch\"\n    ]\n}\n</code></pre> Success Response:  <pre><code>{\n    \"message\": \"Recipe created successfully\"\n}\n</code></pre> [DELETE] /v1/recipes/{recipe_id} This endpoint is use to delete an existing recipe.   Parameters (path) :<code>recipe_id</code>: The ID of the recipe to delete.  Example:  <code>/v1/recipe/sample-recipe</code> Success Response:  <pre><code>{\n    \"message\": \"Recipe deleted successfully\"\n}\n</code></pre>"},{"location":"web_api/sessions_endpoints/","title":"Moonshot Sessions API Endpoints","text":"[GET] /v1/sessions This endpoint is use to get all sessions.   Parameters (body) : None  Success Response:  <pre><code>[\n    {\n        \"session_id\": \"testsession1_20240315-043147\",\n        \"name\": \"TestSession1\",\n        \"description\": \"Test 1\",\n        \"created_epoch\": 1710448307.051328,\n        \"created_datetime\": \"20240315-043147\",\n        \"chat_ids\": [\n            \"openaigpt35turbotest1_20240315_043147\",\n            \"openaigpt4test1_20240315_043147\"\n        ],\n        \"endpoints\": [\n            \"openaigpt35turbotest1\",\n            \"openaigpt4test1\"\n        ],\n        \"prompt_template\": null,\n        \"context_strategy\": null,\n        \"filename\": null,\n        \"chat_history\": null\n    }\n]\n</code></pre>  [GET] /v1/sessions/{session_id}?include_history={boolean}&amp;length={int}  This endpoint is use to session details by ID.   Parameters (path):  <code>session_id</code>: The ID of the session to retrieve.  <code>include_history</code>: A boolean to determine if you want to retrieve the history  <code>length</code>: The length of the history you want to retrieve  Example : <code>/v1/sessions/testsession1_20240315-043147?include_history=true&amp;length=2</code> Success Response:  <pre><code>{\n    \"session\": {\n        \"session_id\": \"testsession1_20240315-043147\",\n        \"name\": \"TestSession1\",\n        \"description\": \"Test 1\",\n        \"created_epoch\": 1710448307.051328,\n        \"created_datetime\": \"20240315-043147\",\n        \"chat_ids\": [\n            \"openaigpt35turbotest1_20240315_043147\",\n            \"openaigpt4test1_20240315_043147\"\n        ],\n        \"endpoints\": [\n            \"openaigpt35turbotest1\",\n            \"openaigpt4test1\"\n        ],\n        \"prompt_template\": null,\n        \"context_strategy\": null,\n        \"filename\": null,\n        \"chat_history\": {\n            \"openaigpt35turbotest1_20240315_043147\": [],\n            \"openaigpt4test1_20240315_043147\": []\n        }\n    }\n}\n</code></pre> [POST] /v1/sessions This endpoint is use to create new session.   Parameters (body) <pre><code>{\n    \"name\": \"string\",\n    \"description\": \"string\",\n    \"endpoints\": [\"string\"]\n}\n</code></pre> Example <pre><code>{\n    \"name\": \"TestSession1\",\n    \"description\": \"Test 1\",\n    \"endpoints\": [\"openaigpt35turbotest1\", \"openaigpt4test1\"]\n}\n</code></pre> Success Response:  <pre><code>{\n    \"session\": {\n        \"session_id\": \"testsession1_20240315-043147\",\n        \"name\": \"TestSession1\",\n        \"description\": \"Test 1\",\n        \"created_epoch\": 1710448307.051328,\n        \"created_datetime\": \"20240315-043147\",\n        \"chat_ids\": [\n            \"openaigpt35turbotest1_20240315_043147\",\n            \"openaigpt4test1_20240315_043147\"\n        ],\n        \"endpoints\": [\n            \"openaigpt35turbotest1\",\n            \"openaigpt4test1\"\n        ],\n        \"prompt_template\": null,\n        \"context_strategy\": null,\n        \"filename\": null,\n        \"chat_history\": null\n    }\n}\n</code></pre> [POST] /v1/sessions/{session_id}/prompt This endpoint is use to send prompts in the session.   Parameters (path):  <code>session_id</code>: The ID of the session to retrieve.   Parameters (body) <pre><code>{\n    \"prompt\": \"string\"\n}\n</code></pre> Example <code>/v1/sessions/testsession1_20240315-044154/prompt</code> <pre><code>{\n    \"prompt\": \"Hello World\"\n}\n</code></pre> Success Response:  <pre><code>{\n    \"openaigpt35turbotest1_20240315_044154\": [\n        {\n            \"chat_record_id\": 1,\n            \"conn_id\": \"conn_id_123\",\n            \"context_strategy\": \"\",\n            \"prompt_template\": \"\",\n            \"prompt\": \"hello world\",\n            \"prepared_prompt\": \"hello world\",\n            \"predicted_result\": \"predicted results\",\n            \"duration\": \"2 secs\",\n            \"prompt_time\": \"03/15/2024, 04:42:02\"\n        }\n    ],\n    \"openaigpt4test1_20240315_044154\": [\n        {\n            \"chat_record_id\": 1,\n            \"conn_id\": \"conn_id_123\",\n            \"context_strategy\": \"\",\n            \"prompt_template\": \"\",\n            \"prompt\": \"hello world\",\n            \"prepared_prompt\": \"hello world\",\n            \"predicted_result\": \"predicted results\",\n            \"duration\": \"2 secs\",\n            \"prompt_time\": \"03/15/2024, 04:42:02\"\n        }\n    ]\n}\n</code></pre>"}]}