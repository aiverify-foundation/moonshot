# List of Datasets
| Name | Description | License | Reference |
|------|-------------|---------|-----------|
| squad-shifts-tnf | Zero-shot reading comprehension on paragraphs and questions from squadshifts |  | https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/squad_shifts |
| CyberSecEval Prompt Injection 2 Malay | A dataset of security-related questions and answers. | MIT license | https://github.com/meta-llama/PurpleLlama/tree/main/CybersecurityBenchmarks/datasets/prompt_injection |
| Answercarefully Information Korean | A dataset of security-related questions and answers. | Dataset from NII-LLMC working group - subset created for AISI testing | Dataset from NII-LLMC working group - subset created for AISI testing |
| BBQ-lite on religion - Ambiguous Questions | This dataset is an excerpt from Bias Benchmark from QA on religion, containing only the ambiguous questions. | CC-BY-4.0 license | https://arxiv.org/pdf/2110.08193v2 |
| ML Commons Ailuminte - Violent Crimes - Kwashili | This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well. | CC-BY-4.0 | https://mlcommons.org/ailuminate/ |
| mock-dataset | This is a sample of a dataset to be used for RAG Evaluation.  |  |  |
| CyberSecEval Prompt Injection 2 English | A dataset of security-related questions and answers. | MIT license | https://github.com/meta-llama/PurpleLlama/tree/main/CybersecurityBenchmarks/datasets/prompt_injection |
| ML Commons Ailuminte - Intellectual Property Violation - English | Test whether responses violate the intellectual property rights of any party. | CC-BY-4.0 | https://mlcommons.org/ailuminate/ |
| Gender Occupational Bias | The gender occupational bias is a set of gender neutral text-to-image prompts that are likely to result in models favouring the generation of one gender over the other. The occupations included were proposed in the paper: https://arxiv.org/abs/2211.03759 | MIT License | https://arxiv.org/abs/2211.03759 |
| advglue | Adversarial GLUE Benchmark (AdvGLUE) is a comprehensive robustness evaluation benchmark that focuses on the adversarial robustness evaluation of language models.  |  | https://github.com/AI-secure/adversarial-glue |
| ML Commons Ailuminte - Intellectual Property Violation - Malay | A dataset of security-related questions and answers. | CC-BY-4.0 | https://mlcommons.org/ailuminate/ |
| ML Commons Ailuminte - Violent Crimes - French | A dataset of security-related questions and answers. | CC-BY-4.0 | https://mlcommons.org/ailuminate/ |
| ML Commons Ailuminte - Privacy - Farsi | This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well. | CC-BY-4.0 | https://mlcommons.org/ailuminate/ |
| Food in Singapore | Contain prompts that test model's udnerstanding in Food, in True/False format | Apache-2.0 | IMDA |
| MedMCQA | MedMCQ is a large-scale, multiple-choice question answering dataset designed to address realworld medical entrance exam questions. It contains more than 194k high-quality AIIMS & NEET PG entrnace exam MCQs covering 2.4k healthcare topics and 21 medical subjects. | MIT License | https://github.com/MedMCQA/MedMCQA |
| tamil-thirukural | This dataset is used to test the comprehension abilities for the Thirukkural. Thirukkural is a classic Tamil literature composed by the ancient Tamil poet Thiruvalluvar. It consists of 1330 couplets (kurals) that are grouped into 133 chapters, each containing 10 couplets. | Creative Commons Attribution 4.0 International | https://github.com/vijayanandrp/Thirukkural-Tamil-Dataset |
| ML Commons Ailuminte - Non-Violent Crimes - French | This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well. | CC-BY-4.0 | https://mlcommons.org/ailuminate/ |
| RealtimeQA | RealTime QA is a dynamic question answering (QA) platform that inquires about the present.  |  | https://github.com/realtimeqa/realtimeqa_public |
| sg-university-tutorial-questions-legal | Contain tutorial questions ans answers from Singapore's Universities to test model's ability in understanding legal context in Singapore |  |  |
| ML Commons Ailuminte - Privacy - Kwashili | This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well. | CC-BY-4.0 | https://mlcommons.org/ailuminate/ |
| Chinese Version - Bias Benchmark for QA | This aims to measure social biases across 9 different categories in Chinese language. |  | https://github.com/YFHuangxxxx/CBBQ |
| BBQ-lite on gender - Disambiguated Questions | This dataset is an excerpt from Bias Benchmark from QA on gender, containing only the disambiguated questions. | CC-BY-4.0 license | https://arxiv.org/pdf/2110.08193v2 |
| ML Commons Ailuminte - Non-Violent Crimes - Kwashili | This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well. | CC-BY-4.0 | https://mlcommons.org/ailuminate/ |
| ARC-Easy | A dataset of genuine grade-school level, multiple-choice science questions in advanced Q&A. This is the challenge set. | CC BY-SA | https://allenai.org/data/arc |
| ML Commons Ailuminte - Intellectual Property Violation - Telugu | A dataset of security-related questions and answers. | CC-BY-4.0 | https://mlcommons.org/ailuminate/ |
| CyberSecEval Prompt Injection 2 Chinese | A dataset of security-related questions and answers. | MIT license | https://github.com/meta-llama/PurpleLlama/tree/main/CybersecurityBenchmarks/datasets/prompt_injection |
| ML Commons Ailuminte - Privacy - Japanese | This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well. | CC-BY-4.0 | https://mlcommons.org/ailuminate/ |
| ML Commons Ailuminte - Non-Violent Crimes - Japanese | This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well. | CC-BY-4.0 | https://mlcommons.org/ailuminate/ |
| BBQ-lite on nationality - Ambiguous Questions | This dataset is an excerpt from Bias Benchmark from QA on nationality, containing only the ambiguous questions. | CC-BY-4.0 license | https://arxiv.org/pdf/2110.08193v2 |
| m3exam-vietnamese | This dataset contains local exam questions for testing Vietnamese capability. | CC BY-NC-SA | https://github.com/DAMO-NLP-SG/M3Exam |
| BBQ-lite on physical-appearance - Disambiguated Questions | This dataset is an excerpt from Bias Benchmark from QA on physical-appearance, containing only the disambiguated questions. | CC-BY-4.0 license | https://arxiv.org/pdf/2110.08193v2 |
| tanglish-tweets-SA | Code-mixed Tamil and English tweets curated for the sentiment analysis task. | CC0: Public Domain | https://www.kaggle.com/datasets/vyombhatia/tanglish-comments-for-sentiment-ananlysis/data |
| Ethics-Commonsense-Hard | This dataset contains prompts that embedded the body of moral standards and principles that most people intuitively accept. | MIT License | https://github.com/hendrycks/ethics/ |
| uciadult | The UCI Adult dataset has been used widely used to assess fairness. | Creative Commons Attribution 4.0 International | https://archive.ics.uci.edu/dataset/2/adult |
| BBQ-lite on age - Disambiguated Questions | This dataset is an excerpt from Bias Benchmark from QA on age, containing only the disambiguated questions. | CC-BY-4.0 license | https://arxiv.org/pdf/2110.08193v2 |
| MLC Baseline - Suicide & Self-Harm | Responses that enable, encourage, or endorse acts of intentional self-harm. |  |  |
| ML Commons Ailuminte - Intellectual Property Violation - Chinese | A dataset of security-related questions and answers. | CC-BY-4.0 | https://mlcommons.org/ailuminate/ |
| squad-v2 | Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable. | CC BY-SA 4.0 | rajpurkar/squad_v2 Â· Datasets at Hugging Face |
| sg-legal-glossary | A list of singapore legal terms extracted from SICC and Judiciary websites. |  | https://www.sicc.gov.sg/glossary-of-legal-terms |
| MLC Baseline - Hate | Responses that demean or dehumanize people on the basis of their sensitive, personal characteristics. |  |  |
| BBQ-lite on age - Ambiguous Questions | This dataset is an excerpt from Bias Benchmark from QA on age, containing only the ambiguous questions. | CC-BY-4.0 license | https://arxiv.org/pdf/2110.08193v2 |
| cause_and_effect | This recipe measures if the model is able to distinguish and understand cause and effect. The higher the grade, the better the system can perform this capability. |  | https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/cause_and_effect |
| BBQ-lite on race-x-gender - Ambiguous Questions | This dataset is an excerpt from Bias Benchmark from QA on race-x-gender, containing only the ambiguous questions. | CC-BY-4.0 license | https://arxiv.org/pdf/2110.08193v2 |
| ML Commons Ailuminte - Violent Crimes - Japanese | This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well. | CC-BY-4.0 | https://mlcommons.org/ailuminate/ |
| Chinese Version - Bias Benchmark for QA | This aims to measure social biases across 9 different categories in Chinese language. |  | https://github.com/YFHuangxxxx/CBBQ |
| TruthfulQA (MCQ Version) | TruthfulQA aims to measure the the truthfulness of a model. This dataset uses a multiple choice format. | Apache-2.0 | https://github.com/sylinrl/TruthfulQA |
| Ethics-Commonsense-Hard | This dataset contains prompts that embedded the body of moral standards and principles that most people intuitively accept. | MIT License | https://github.com/hendrycks/ethics/ |
| PubMedQA | PubMedQA is a large-scale, multiple-choice question answering dataset designed to address realworld medical entrance exam questions. It contains more than 194k high-quality AIIMS & NEET PG entrnace exam MCQs covering 2.4k healthcare topics and 21 medical subjects. | MIT License | https://github.com/MedMCQA/MedMCQA |
| Facts about Singapore in True and False | Contain prompts that contains facts about Singapore, in True/False format | Apache-2.0 | IMDA |
| Chinese Version - Bias Benchmark for QA | This aims to measure social biases across 9 different categories in Chinese language. |  | https://github.com/YFHuangxxxx/CBBQ |
| RealToxicityPrompts | RealToxicityPrompts aims to measure the model's ability in identify toxic sentences. | Apache-2.0 | https://github.com/allenai/real-toxicity-prompts/ |
| Chinese Version - Bias Benchmark for QA | This aims to measure social biases across 9 different categories in Chinese language. |  | https://github.com/YFHuangxxxx/CBBQ |
| gre_reading_comprehension | To measure the model/'s ability to summarize text, infer and deduce knowledge from context, and match the context. |  | https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/gre_reading_comprehension |
| MMLU College Medicine | MMLU measure knowledge acquired during pretraining by evaluating models exclusively in zero-shot and few-shot settings. This particular dataset contains questions on MMLU College Medicine | MIT License | https://github.com/hendrycks/test |
| Answercarefully Information Telugu | A dataset of security-related questions and answers. | Dataset from NII-LLMC working group - subset created for AISI testing. | Dataset from NII-LLMC working group - subset created for AISI testing. |
| ML Commons Ailuminte - Non-Violent Crimes - Farsi | This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well. | CC-BY-4.0 | https://mlcommons.org/ailuminate/ |
| BBQ-lite on race-x-ses - Disambiguated Questions | This dataset is an excerpt from Bias Benchmark from QA on race-x-ses, containing only the disambiguated questions. | CC-BY-4.0 license | https://arxiv.org/pdf/2110.08193v2 |
| Gender Occupational Bias | The gender occupational bias is a set of gender neutral text-to-image prompts that are likely to result in models favouring the generation of one gender over the other. The occupations included were proposed in the paper: https://arxiv.org/abs/2211.03759 | MIT License | https://arxiv.org/abs/2211.03759 |
| Chinese Version - Bias Benchmark for QA | This aims to measure social biases across 9 different categories in Chinese language. |  | https://github.com/YFHuangxxxx/CBBQ |
| Singapore Polical History | Contain questions about Singapore's key historical events in political. | Apache-2.0 | IMDA |
| ChallengingToxicityPrompts | ChallengingToxicityPrompts was created by the authors of DecodingTrust, with the aim to measure the model's ability in identify toxic sentences. | CC BY-SA 4.0 | https://github.com/AI-secure/DecodingTrust |
| MLC Baseline - Violent Crimes | Responses that enable, encourage, or endorse the commission of violent crimes. |  |  |
| ML Commons Ailuminte - Intellectual Property Violation - Korean | A dataset of security-related questions and answers. | CC-BY-4.0 | https://mlcommons.org/ailuminate/ |
| Chinese Version - Bias Benchmark for QA | This aims to measure social biases across 9 different categories in Chinese language. |  | https://github.com/YFHuangxxxx/CBBQ |
| MMLU Clinical Knowledge | MMLU measure knowledge acquired during pretraining by evaluating models exclusively in zero-shot and few-shot settings. This particular dataset contains questions on MMLU Clinical Knowledge | MIT License | https://github.com/hendrycks/test |
| Chinese Version - Bias Benchmark for QA | This aims to measure social biases across 9 different categories in Chinese language. |  | https://github.com/YFHuangxxxx/CBBQ |
| coqa_conversational_question_answering | To measure the ability of machines to understand a text passage and answer a series of interconnected questions |  | https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/coqa_conversational_question_answering |
| Singapore Transport System | Contain questions about Singapore's public housing system | Apache-2.0 | IMDA |
| Answercarefully Information Malay | A dataset of security-related questions and answers. | Dataset from NII-LLMC working group - subset created for AISI testing. | Dataset from NII-LLMC working group - subset created for AISI testing. |
| ChallengingToxicityPrompts | ChallengingToxicityPrompts was created by the authors of DecodingTrust, with the aim to measure the model's ability in identify toxic sentences. | CC BY-SA 4.0 | https://github.com/AI-secure/DecodingTrust |
| CyberSecEval Prompt Injection 2 Korean | A dataset of security-related questions and answers. | MIT license | https://github.com/meta-llama/PurpleLlama/tree/main/CybersecurityBenchmarks/datasets/prompt_injection |
| BBQ-lite on race-x-ses - Ambiguous Questions | This dataset is an excerpt from Bias Benchmark from QA on race-x-ses, containing only the ambiguous questions. | CC-BY-4.0 license | https://arxiv.org/pdf/2110.08193v2 |
| MLC Baseline - Chemical, Biological, Radiological, Nuclear, and Explosives | Responses that enable, encourage, or endorse the creation of weapons of mass destruction. |  |  |
| Answercarefully Information English | A dataset of security-related questions and answers. | Dataset from NII-LLMC working group - subset created for AISI testing | Dataset from NII-LLMC working group - subset created for AISI testing |
| BBQ-lite on gender - Ambiguous Questions | This dataset is an excerpt from Bias Benchmark from QA on gender, containing only the ambiguous questions. | CC-BY-4.0 license | https://arxiv.org/pdf/2110.08193v2 |
| GSM8K-TEST | This dataset consists of high quality grade school math problems created by human. These problems take between 2 and 8 steps to solve, and usually requires a sequence of elementary calculations. The dataset includes only the test set of GSM8K. | MIT License | https://github.com/openai/grade-school-math |
| BBQ-lite on sexual-orientation - Ambiguous Questions | This dataset is an excerpt from Bias Benchmark from QA on sexual-orientation, containing only the ambiguous questions. | CC-BY-4.0 license | https://arxiv.org/pdf/2110.08193v2 |
| Chinese Version - Bias Benchmark for QA | This aims to measure social biases across 9 different categories in Chinese language. |  | https://github.com/YFHuangxxxx/CBBQ |
| MLC Baseline - Chemical, Biological, Radiological, Nuclear, and Explosives | Responses that enable, encourage, or endorse the creation of weapons of mass destruction. |  |  |
| MLC Baseline - Hate | Responses that demean or dehumanize people on the basis of their sensitive, personal characteristics. |  |  |
| CyberSecEval Prompt Injection | Check if llm is susceptible to prompt injection | MIT license | https://github.com/meta-llama/PurpleLlama/tree/main/CybersecurityBenchmarks/datasets/prompt_injection |
| Chinese Version - Bias Benchmark for QA | This aims to measure social biases across 9 different categories in Chinese language. |  | https://github.com/YFHuangxxxx/CBBQ |
| Chinese Version - Bias Benchmark for QA | This aims to measure social biases across 9 different categories in Chinese language. |  | https://github.com/YFHuangxxxx/CBBQ |
| Chinese Version - Bias Benchmark for QA | This aims to measure social biases across 9 different categories in Chinese language. |  | https://github.com/YFHuangxxxx/CBBQ |
| Winogrande | This dataset is used for commonsense reasoning, expert-crafted pronoun resolution problems designed to be unsolvable for statistical models. | Apache-2.0 | https://github.com/allenai/winogrande |
| Chinese Version - Bias Benchmark for QA | This aims to measure social biases across 9 different categories in Chinese language. |  | https://github.com/YFHuangxxxx/CBBQ |
| Contextual Parametric Knowledge Conflicts | To measure the model/âs ability in making informed predictions |  | https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/contextual_parametric_knowledge_conflicts |
| MMLU Anatomy | MMLU measure knowledge acquired during pretraining by evaluating models exclusively in zero-shot and few-shot settings. This particular dataset contains questions on MMLU Anatomy | MIT License | https://github.com/hendrycks/test |
| Singapore Transport System | Contain questions about Singapore's transport system | Apache-2.0 | IMDA |
| Answercarefully Information Chinese | A dataset of security-related questions and answers. | Dataset from NII-LLMC working group - subset created for AISI testing | Dataset from NII-LLMC working group - subset created for AISI testing |
| truthfulqa | TruthfulQA aims to measure the the truthfulness of a model. | Apache-2.0 | https://github.com/sylinrl/TruthfulQA |
| ARC-Easy | A dataset of genuine grade-school level, multiple-choice science questions in advanced Q&A. This is the easy set. | CC BY-SA | https://allenai.org/data/arc |
| Chinese Version - Bias Benchmark for QA | This aims to measure social biases across 9 different categories in Chinese language. |  | https://github.com/YFHuangxxxx/CBBQ |
| Iconic Places in Singapore | Contain questions about Singapore's iconic places. | Apache-2.0 | IMDA |
| Inappropriate Image Prompts (I2P) | The I2P benchmark contains real user prompts for generative text2image prompts that are unproportionately likely to produce inappropriate images. I2P was introduced in the 2023 CVPR paper Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models. This benchmark is not specific to any approach or model, but was designed to evaluate mitigating measures against inappropriate degeneration in Stable Diffusion. | MIT License | https://huggingface.co/datasets/AIML-TUDA/i2p |
| Chinese Version - Bias Benchmark for QA | This aims to measure social biases across 9 different categories in Chinese language. |  | https://github.com/YFHuangxxxx/CBBQ |
| Chinese Version - Bias Benchmark for QA | This aims to measure social biases across 9 different categories in Chinese language. |  | https://github.com/YFHuangxxxx/CBBQ |
| MLC Baseline - Sex-Related Crimes | Responses that enable, encourage, or endorse the commission of sex-related crimes. |  |  |
| Analogical Similarity | To measure the modelâs ability in discriminating between different degrees of analogical similarity in two given episodes |  | https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/analogical_similarity |
| Chinese Version - Bias Benchmark for QA | This aims to measure social biases across 9 different categories in Chinese language. |  | https://github.com/YFHuangxxxx/CBBQ |
| BBQ-lite on race-ethnicity - Disambiguated Questions | This dataset is an excerpt from Bias Benchmark from QA on race-ethnicity, containing only the disambiguated questions. | CC-BY-4.0 license | https://arxiv.org/pdf/2110.08193v2 |
| GSM8K | This dataset consists of high quality grade school math problems created by human. These problems take between 2 and 8 steps to solve, and usually requires a sequence of elementary calculations. | MIT License | https://github.com/openai/grade-school-math |
| MLC Baseline - Child Abuse and Exploitation | Responses that contain, describe, enable, encourage, or endorse the sexual abuse of children. |  |  |
| m3exam-indonesian | This dataset contains local exam questions for testing Indonesian capability. | CC BY-NC-SA | https://github.com/DAMO-NLP-SG/M3Exam |
| BBQ-lite on race-x-gender - Disambiguated Questions | This dataset is an excerpt from Bias Benchmark from QA on race-x-gender, containing only the disambiguated questions. | CC-BY-4.0 license | https://arxiv.org/pdf/2110.08193v2 |
| MLC Baseline - Sex-Related Crimes | Responses that enable, encourage, or endorse the commission of sex-related crimes. |  |  |
| BBQ-lite on disability-status - Ambiguous Questions | This dataset is an excerpt from Bias Benchmark from QA on disability-status, containing only the ambiguous questions. | CC-BY-4.0 license | https://arxiv.org/pdf/2110.08193v2 |
| Answercarefully Information Cantonese | A dataset of security-related questions and answers. | Dataset from NII-LLMC working group - subset created for AISI testing. | Dataset from NII-LLMC working group - subset created for AISI testing. |
|  |  |  |  |
| Answercarefully Information Japanese | A dataset of security-related questions and answers. | Dataset from NII-LLMC working group - subset created for AISI testing | Dataset from NII-LLMC working group - subset created for AISI testing |
| MMLU College Biology | MMLU measure knowledge acquired during pretraining by evaluating models exclusively in zero-shot and few-shot settings. This particular dataset contains questions on MMLU College Biology | MIT License | https://github.com/hendrycks/test |
| ChallengingToxicityPrompts | ChallengingToxicityPrompts was created by the authors of DecodingTrust, with the aim to measure the model's ability in identify toxic sentences. | CC BY-SA 4.0 | https://github.com/AI-secure/DecodingTrust |
| ML Commons Ailuminte - Intellectual Property Violation - Farsi | A dataset of security-related questions and answers. | CC-BY-4.0 | https://mlcommons.org/ailuminate/ |
| Answercarefully Information Kwashili | A dataset of security-related questions and answers. | Dataset from NII-LLMC working group - subset created for AISI testing. | Dataset from NII-LLMC working group - subset created for AISI testing. |
| BBQ-lite on religion - Disambiguated Questions | This dataset is an excerpt from Bias Benchmark from QA on religion, containing only the disambiguated questions. | CC-BY-4.0 license | https://arxiv.org/pdf/2110.08193v2 |
| MLC Baseline - Child Abuse and Exploitation | Responses that contain, describe, enable, encourage, or endorse the sexual abuse of children. |  |  |
| Chinese Version - Bias Benchmark for QA | This aims to measure social biases across 9 different categories in Chinese language. |  | https://github.com/YFHuangxxxx/CBBQ |
| ML Commons Ailuminte - Violent Crimes - Korean | This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well. | CC-BY-4.0 | https://mlcommons.org/ailuminate/ |
| Chinese Version - Bias Benchmark for QA | This aims to measure social biases across 9 different categories in Chinese language. |  | https://github.com/YFHuangxxxx/CBBQ |
| Answercarefully Information French | A dataset of security-related questions and answers. | Dataset from NII-LLMC working group - subset created for AISI testing. | Dataset from NII-LLMC working group - subset created for AISI testing. |
| Chinese Version - Bias Benchmark for QA | This aims to measure social biases across 9 different categories in Chinese language. |  | https://github.com/YFHuangxxxx/CBBQ |
| Chinese Version - Bias Benchmark for QA | This aims to measure social biases across 9 different categories in Chinese language. |  | https://github.com/YFHuangxxxx/CBBQ |
| MLC Baseline - Non-Violent Crimes | Responses that enable, encourage, or endorse the commission of non-violent crimes. |  |  |
| MLC Baseline - Non-Violent Crimes | Responses that enable, encourage, or endorse the commission of non-violent crimes. |  |  |
| List of Occupations | This dataset contains a list of occupations collected from various websites (e.g., MoM of Singapore). The dataset can be used to test job-related applications. | MIT License | IMDA |
| In-the-wild Jailbreak Dataset | This dataset contains jailbreak prompts that were collected from various places (e.g., Discord). | MIT license | https://github.com/verazuo/jailbreak_llms |
| MLC Baseline - Suicide & Self-Harm | Responses that enable, encourage, or endorse acts of intentional self-harm. |  |  |
| Jailbreak Prompts | This dataset evaluates whether a model/app will detect attempted jailbreaks and generate unsafe content. These jailbreak techniques are collected from X (e.g., Pliny) and public repository (e.g., in-the-wild jailbreak prompts).  | Apache 2.0 | OWASP |
| MLC Baseline - Violent Crimes | Responses that enable, encourage, or endorse the commission of violent crimes. |  |  |
| Ethics-Commonsense-Hard | This dataset contains prompts that embedded the body of moral standards and principles that most people intuitively accept. | MIT License | https://github.com/hendrycks/ethics/ |
| Chinese Linguistics & Cognition Challenge (CLCC) | This dataset is a subjective benchmark created by BAAI FlagEval group. | CC-BY-4.0 license | https://flageval.baai.ac.cn/ |
| auto_categorization | To measure the model's ability in categorising elements into groups. |  | https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/auto_categorization |
| ML Commons Ailuminte - Privacy - Korean | This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well. | CC-BY-4.0 | https://mlcommons.org/ailuminate/ |
| BBQ-lite on sexual-orientation - Disambiguated Questions | This dataset is an excerpt from Bias Benchmark from QA on sexual-orientation, containing only the disambiguated questions. | CC-BY-4.0 license | https://arxiv.org/pdf/2110.08193v2 |
| tamil-news-classification | This dataset is used to classify a static set of Tamil News. The task is to classify news to its respective category. The dataset has 6 news categories - "tamilnadu", "india", "cinema", "sports", "politics", "world". | GNU General Public License v3.0 | https://github.com/vanangamudi/tamil-news-classification/tree/master/dataset/news |
| Chinese Version - Bias Benchmark for QA | This aims to measure social biases across 9 different categories in Chinese language. |  | https://github.com/YFHuangxxxx/CBBQ |
| ML Commons Ailuminte - Non-Violent Crimes - Korean | This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well. | CC-BY-4.0 | https://mlcommons.org/ailuminate/ |
| ML Commons Ailuminte - Privacy - Chinese | This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well. | CC-BY-4.0 | https://mlcommons.org/ailuminate/ |
| CyberSecEval Prompt Injection 2 Japanese | A dataset of security-related questions and answers. | MIT license | https://github.com/meta-llama/PurpleLlama/tree/main/CybersecurityBenchmarks/datasets/prompt_injection |
| C-Values - Values of Chinese Large Language attack_modules | This dataset contains prompts that can measure the alignment ability of LLMs in both safety and responsibility in Chinese Language. | Apache-2.0 license | https://github.com/X-PLUG/CValues |
| ML Commons Ailuminte - Non-Violent Crimes - Chinese | A dataset of security-related questions and answers. | CC-BY-4.0 | https://mlcommons.org/ailuminate/ |
| Facts about Asia pacific in True and False in 4 languages (Chinese, Malay, Tamil and English) | Contain prompts that contain facts about Asia Pacific and Singapore, in True/False format | Apache-2.0 | facticity.ai |
| BBQ-lite on physical-appearance - Ambiguous Questions | This dataset is an excerpt from Bias Benchmark from QA on physical-appearance, containing only the ambiguous questions. | CC-BY-4.0 license | https://arxiv.org/pdf/2110.08193v2 |
| enronemail | The Enron email dataset is aimed at evaluating if there is leakage of private information. The dataset was used in DecodingTrust as part of its privacy evaluation. | Apache-2.0 | https://github.com/ParakweetLabs/EmailIntentDataSet |
| m3exam-thai | This dataset contains local exam questions for testing Thai capability. | CC BY-NC-SA | https://github.com/DAMO-NLP-SG/M3Exam |
| ML Commons Ailuminte - Intellectual Property Violation - Japanese | A dataset of security-related questions and answers. | CC-BY-4.0 | https://mlcommons.org/ailuminate/ |
| MMLU Medical Genetics | MMLU measure knowledge acquired during pretraining by evaluating models exclusively in zero-shot and few-shot settings. This particular dataset contains questions on MMLU Medical Genetics | MIT License | https://github.com/hendrycks/test |
| ML Commons Ailuminte - Intellectual Property Violation - Cantonese | A dataset of security-related questions and answers. | CC-BY-4.0 | https://mlcommons.org/ailuminate/ |
| Chinese Version - Bias Benchmark for QA | This aims to measure social biases across 9 different categories in Chinese language. |  | https://github.com/YFHuangxxxx/CBBQ |
| BBQ-lite on ses - Ambiguous Questions | This dataset is an excerpt from Bias Benchmark from QA on ses, containing only the ambiguous questions. | CC-BY-4.0 license | https://arxiv.org/pdf/2110.08193v2 |
| truthfulqa | TruthfulQA aims to measure the the truthfulness of a model. | Apache-2.0 | https://github.com/sylinrl/TruthfulQA |
| Ethics-Commonsense-Hard | This dataset contains prompts that embedded the body of moral standards and principles that most people intuitively accept. | MIT License | https://github.com/hendrycks/ethics/ |
| Places in Singapore | Contain prompts that test model's udnerstanding places in Singapore, in True/False format | Apache-2.0 | IMDA |
| BBQ-lite on nationality - Disambiguated Questions | This dataset is an excerpt from Bias Benchmark from QA on nationality, containing only the disambiguated questions. | CC-BY-4.0 license | https://arxiv.org/pdf/2110.08193v2 |
| ML Commons Ailuminte - Violent Crimes - Chinese | This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well. | CC-BY-4.0 | https://mlcommons.org/ailuminate/ |
| ML Commons Ailuminte - Non-Violent Crimes - English | Test whether responses enable, encourage, or endorse the commission of non-violent crimes. | CC-BY-4.0 | https://mlcommons.org/ailuminate/ |
| ML Commons Ailuminte - Privacy - Malay | This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well. | CC-BY-4.0 | https://mlcommons.org/ailuminate/ |
| MMLU Professional Medicine.json | MMLU measure knowledge acquired during pretraining by evaluating models exclusively in zero-shot and few-shot settings. This particular dataset contains questions on MMLU Professional Medicine.json | MIT License | https://github.com/hendrycks/test |
| Chinese Version - Bias Benchmark for QA | This aims to measure social biases across 9 different categories in Chinese language. |  | https://github.com/YFHuangxxxx/CBBQ |
| BBQ-lite on race-ethnicity - Ambiguous Questions | This dataset is an excerpt from Bias Benchmark from QA on race-ethnicity, containing only the ambiguous questions. | CC-BY-4.0 license | https://arxiv.org/pdf/2110.08193v2 |
| ML Commons Ailuminte - Non-Violent Crimes - Malay | This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well. | CC-BY-4.0 | https://mlcommons.org/ailuminate/ |
| ML Commons  Ailuminte - Privacy - English | Test whether responses contain sensitive, nonpublic personal information that could undermine someoneâs physical, digital, or financial security. | CC-BY-4.0 | https://mlcommons.org/ailuminate/ |
| Chinese Version - Bias Benchmark for QA | This aims to measure social biases across 9 different categories in Chinese language. |  | https://github.com/YFHuangxxxx/CBBQ |
| winobias-variation1 | This dataset contains gender-bias based on the professions from the Labor Force Statistics (https://www.bls.gov/cps/cpsaat11.htm), which contain some gender-bias. | MIT License | https://github.com/uclanlp/corefBias/tree/master/WinoBias/wino |
| ML Commons Ailuminte - Violent Crimes - Telugu | This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well. | CC-BY-4.0 | https://mlcommons.org/ailuminate/ |
| Chinese Version - Bias Benchmark for QA | This aims to measure social biases across 9 different categories in Chinese language. |  | https://github.com/YFHuangxxxx/CBBQ |
| Chinese Version - Bias Benchmark for QA | This aims to measure social biases across 9 different categories in Chinese language. |  | https://github.com/YFHuangxxxx/CBBQ |
| ML Commons Ailuminte - Privacy - Telugu | This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well. | CC-BY-4.0 | https://mlcommons.org/ailuminate/ |
| HellaSwag | This dataset is used to evaluate commonsense with questions that are trivial for humans but difficult for state-of-the-art models. | MIT License | https://github.com/rowanz/hellaswag |
| CyberSecEval Prompt Injection 2 French | A dataset of security-related questions and answers. | MIT license | https://github.com/meta-llama/PurpleLlama/tree/main/CybersecurityBenchmarks/datasets/prompt_injection |
| BBQ-lite on ses - Disambiguated Questions | This dataset is an excerpt from Bias Benchmark from QA on ses, containing only the disambiguated questions. | CC-BY-4.0 license | https://arxiv.org/pdf/2110.08193v2 |
| Safety Benchmark (Singapore Context) | Contain prompts that test safety in Singapore-context | Apache-2.0 | IMDA |
| MedQA (US) | MedQA is a free-form multiple-choice OpenQA dataset for solving medical problems. These are collected from the professional medical board exams. We extracted the list of MCQ questions from the US in this dataset. | MIT License | https://github.com/jind11/MedQA |
| uciadult | The UCI adult dataset, created in 1996, is used to train models to predict whether a person's income will exceed $50K/yr based on census data. Also known as "Census Income" dataset.  | Creative Commons Attribution 4.0 International | https://archive.ics.uci.edu/dataset/2/adult |
| ML Commons Ailuminte - Intellectual Property Violation - Kwashili | A dataset of security-related questions and answers. | CC-BY-4.0 | https://mlcommons.org/ailuminate/ |
| Answercarefully Information Farsi | A dataset of security-related questions and answers. | Dataset from NII-LLMC working group - subset created for AISI testing. | Dataset from NII-LLMC working group - subset created for AISI testing. |
| MMLU | This dataset covers 57 tasks including elementary mathemathics, US history, computer science, law, and more. | MIT license | https://github.com/hendrycks/test |
| ML Commons Ailuminte - Non-Violent Crimes - Telugu | This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well. | CC-BY-4.0 | https://mlcommons.org/ailuminate/ |
| Chinese Version - Bias Benchmark for QA | This aims to measure social biases across 9 different categories in Chinese language. |  | https://github.com/YFHuangxxxx/CBBQ |
| ML Commons Ailuminte - Violent Crimes - English | Test whether responses enable, encourage, or endorse the commission of violent crimes. | CC-BY-4.0 | https://mlcommons.org/ailuminate/ |
| CyberSecEval Prompt Injection 2 Kwashili | A dataset of security-related questions and answers. | MIT license | https://github.com/meta-llama/PurpleLlama/tree/main/CybersecurityBenchmarks/datasets/prompt_injection |
| Chinese Version - Bias Benchmark for QA | This aims to measure social biases across 9 different categories in Chinese language. |  | https://github.com/YFHuangxxxx/CBBQ |
| BBQ-lite on disability-status - Disambiguated Questions | This dataset is an excerpt from Bias Benchmark from QA on disability-status, containing only the disambiguated questions. | CC-BY-4.0 license | https://arxiv.org/pdf/2110.08193v2 |
| ML Commons Ailuminte - Intellectual Property Violation - French | A dataset of security-related questions and answers. | CC-BY-4.0 | https://mlcommons.org/ailuminate/ |
| ML Commons Ailuminte - Violent Crimes - Malay | This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well. | CC-BY-4.0 | https://mlcommons.org/ailuminate/ |