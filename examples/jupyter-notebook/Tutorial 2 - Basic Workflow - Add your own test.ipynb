{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdc5b948-4919-47fe-9598-0d4c1bfaf8ea",
   "metadata": {},
   "source": [
    "# Tutorial 2 - Basic Workflow - Add Your Own Tests \n",
    "\n",
    "**Scenario**: You are a model developer and you want to evaluate your custom chatbot in your CI/CD pipeline as your team continuously train and improve the system. In this case, you have already identified a dataset that does not exist in Moonshot to benchmark your model's performance. How can you add this new dataset into Moonshot and run it with your system?\n",
    "\n",
    "In this tutorial, you will learn how to:\n",
    "\n",
    "- Add your own `dataset` into Moonshot\n",
    "- Create and run your own `recipe`\n",
    "- Create and run your own `cookbook`\n",
    "\n",
    "**Prerequisite**:\n",
    "\n",
    "1. You have added your OpenAI connector configuration named `my-openai-endpoint` in Moonshot. If you are unsure how to do it, please refer to \"Tutorial 1\" in the same folder.\n",
    "\n",
    "**Before starting this tutorial, please make sure you have already installed `moonshot` and `moonshot-data`.** Otherwise, please follow this tutorial to install and configure Moonshot first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb78f9e-6842-4524-add7-7434687cc7cc",
   "metadata": {},
   "source": [
    "## Import Moonshot Library API\n",
    "\n",
    "In this section, we prepare our Jupyter notebook environment by importing necessary libraries required to execute an existing benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbde151f-34fb-4e52-88cc-62c87834cea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moonshot Framework API Imports\n",
    "# These imports from the Moonshot framework allow us to interact with the API, \n",
    "# creating and managing various components such as recipes, cookbooks, and endpoints.\n",
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import sys\n",
    "\n",
    "# Ensure that the root of the Moonshot framework is in the system path for module importing.\n",
    "sys.path.insert(0, '../../')\n",
    "\n",
    "from moonshot.api import (\n",
    "    api_get_all_recipe,\n",
    "    api_create_recipe,\n",
    "    api_create_cookbook,\n",
    "    api_get_all_runner,\n",
    "    api_load_runner,\n",
    "    api_read_result,\n",
    "    api_set_environment_variables\n",
    ")\n",
    "\n",
    "# modify moonshot_path to point to your own copy of moonshot-data\n",
    "moonshot_path = \"./moonshot-data\"\n",
    "env = {\n",
    "    \"ATTACK_MODULES\": os.path.join(moonshot_path, \"attack-modules\"),\n",
    "    \"BOOKMARKS\": os.path.join(moonshot_path, \"generated-outputs/bookmarks\"),\n",
    "    \"CONNECTORS\": os.path.join(moonshot_path, \"connectors\"),\n",
    "    \"CONNECTORS_ENDPOINTS\": os.path.join(moonshot_path, \"connectors-endpoints\"),\n",
    "    \"CONTEXT_STRATEGY\": os.path.join(moonshot_path, \"context-strategy\"),\n",
    "    \"COOKBOOKS\": os.path.join(moonshot_path, \"cookbooks\"),\n",
    "    \"DATABASES\": os.path.join(moonshot_path, \"generated-outputs/databases\"),\n",
    "    \"DATABASES_MODULES\": os.path.join(moonshot_path, \"databases-modules\"),\n",
    "    \"DATASETS\": os.path.join(moonshot_path, \"datasets\"),\n",
    "    \"IO_MODULES\": os.path.join(moonshot_path, \"io-modules\"),\n",
    "    \"METRICS\": os.path.join(moonshot_path, \"metrics\"),\n",
    "    \"PROMPT_TEMPLATES\": os.path.join(moonshot_path, \"prompt-templates\"),\n",
    "    \"RECIPES\": os.path.join(moonshot_path, \"recipes\"),\n",
    "    \"RESULTS\": os.path.join(moonshot_path, \"generated-outputs/results\"),\n",
    "    \"RESULTS_MODULES\": os.path.join(moonshot_path, \"results-modules\"),\n",
    "    \"RUNNERS\": os.path.join(moonshot_path, \"generated-outputs/runners\"),\n",
    "    \"RUNNERS_MODULES\": os.path.join(moonshot_path, \"runners-modules\"),\n",
    "}\n",
    "\n",
    "# Apply the environment variables to configure the Moonshot framework.\n",
    "api_set_environment_variables(env)\n",
    "\n",
    "# Note: there will be no printout if the environment variables are set successfully"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38df293d-0758-45cd-9181-cfc5ea39d7ff",
   "metadata": {},
   "source": [
    "## Prepare the Dataset\n",
    "\n",
    "In this section, we show how to prepare Moonshot dataset. Supposed you have a list of \"fruits\" questions to ask your chatbot, you need to prepare them into the data schema that is compatible with Moonshot.\n",
    "\n",
    "- `name` (str): name of the data\n",
    "- `description` (str): description of the dataset\n",
    "- `license` (str): license of the data\n",
    "- `reference` (str): a link/reference to where the dataset is from (or author of the dataset)\n",
    "- `examples` (list): A list of dictionary containing the prompt (`input`) and ground truth (`target`). A `target` can be left blank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c801e9-4050-4dbb-ba42-22e31d5458d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = {\n",
    "    \"name\": \"Fruits Dataset\",\n",
    "    \"description\":\"Measures whether the model knows what is a fruit\",\n",
    "    \"license\": \"MIT license\",\n",
    "    \"reference\": \"\",\n",
    "    \"examples\": [\n",
    "        {\n",
    "            \"input\": \"Is Lemon a Fruit? Answer Yes or No.\",\n",
    "            \"target\": \"Yes.\"\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"Is Apple a Fruit? Answer Yes or No.\",\n",
    "            \"target\": \"Yes.\"\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"Is Bak Choy a Fruit? Answer Yes or No.\",\n",
    "            \"target\": \"No.\"\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"Is Bak Kwa a Fruit? Answer Yes or No.\",\n",
    "            \"target\": \"No.\"\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"Is Dragonfruit a Fruit? Answer Yes or No.\",\n",
    "            \"target\": \"Yes.\"\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"Is Orange a Fruit? Answer Yes or No.\",\n",
    "            \"target\": \"Yes.\"\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"Is Coke Zero a Fruit? Answer Yes or No.\",\n",
    "            \"target\": \"No.\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "in_file = f\"{moonshot_path}/datasets/test-dataset.json\"\n",
    "json.dump(test_dataset, open(in_file, \"w+\"), indent=2)\n",
    "if os.path.exists(in_file):\n",
    "     print(f\"Dataset 'test-dataset' has been created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7acdff5-c59c-4179-badb-ab68560793a6",
   "metadata": {},
   "source": [
    "## Create a new `recipe`\n",
    "\n",
    "To run this dataset, you need to create a new `recipe`. A `recipe` contains all the details required to run a benchmark. A `recipe` guides Moonshot on what data to use, and how to evaluate the model's responses.\n",
    "\n",
    "To create a new recipe, you need the following elements:\n",
    "\n",
    "1. **Name**: A unique name for the recipe.\n",
    "2. **Description**: An explanation of what the recipe does and what it's for.\n",
    "3. **Tags**: Keywords that categorise the recipe, making it easier to find and group with similar recipes.\n",
    "4. **Categories**: Broader classifications that help organise recipes into collections.\n",
    "5. **Datasets**: The data that will be used when running the recipe. This could be a set of prompts, questions, or any input that the model will respond to.\n",
    "6. **Prompt Templates**: Pre-prompt or post-prompt static text that will be appended to the prompt.\n",
    "7. **Metrics**: Criteria or measurements used to evaluate the model's responses, such as accuracy, fluency, or adherence to a prompt.\n",
    "8. **Grading Scale**: A set of thresholds or criteria used to grade or score the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e855bb-1322-4cc5-b775-ddd0dcbdcfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_recipe = api_create_recipe(\n",
    "    \"Fruit Questions\", # name (mandatory)\n",
    "    \"This recipe is created to test model's ability in answering fruits question.\", # description (mandatory)\n",
    "    [\"chatbot\"], # tags (optional)\n",
    "    [\"capability\"], # category (optional)\n",
    "    [\"test-dataset\"], # filename of the dataset (mandatory)\n",
    "    [], # prompt templates (optional)\n",
    "    [\"exactstrmatch\"], # metrics (mandatory)\n",
    "    { # grading scale (optional)\n",
    "        \"A\": [\n",
    "            80,\n",
    "            100\n",
    "        ],\n",
    "        \"B\": [\n",
    "            60,\n",
    "            79\n",
    "        ],\n",
    "        \"C\": [\n",
    "            40,\n",
    "            59\n",
    "        ],\n",
    "        \"D\": [\n",
    "            20,\n",
    "            39\n",
    "        ],\n",
    "        \"E\": [\n",
    "            0,\n",
    "            19\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Recipe '{test_recipe}' has been created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4e7fdb-6de1-4f3a-ac71-9605256f7a29",
   "metadata": {},
   "source": [
    "## Run your new recipe\n",
    "\n",
    "With this new recipe, you can run this on your `connector endpoint`. We will run this on endpoint `my-openai-endpoint`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c6164b-fc57-457a-a640-f11c451fcdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from slugify import slugify\n",
    "from moonshot.api import api_get_all_run, api_create_runner, api_get_all_runner_name\n",
    "\n",
    "name = \"my new recipe runner\" # Indicate the name\n",
    "recipes = [\"fruit-questions\"] # Test one recipe fruit-questions. You can add more recipes in the list to test as well\n",
    "endpoints = [\"my-openai-endpoint\"]  #Test against 1 endpoint, my-openai-endpoint\n",
    "num_of_prompts = 5 # The number of prompt(s) to run from EACH dataset in the cookbook; 0 means using all prompts in dataset\n",
    "\n",
    "# Below are the optional fields\n",
    "random_seed = 0   # Default: 0; this allows for randomness in dataset selection when num_of_prompts are set\n",
    "system_prompt = \"\"  # Default: \"\"; this allows setting the system prompt for the endpoints\n",
    "\n",
    "# Advanced user - Modify runner processing module and result processing module\n",
    "# Default: benchmarking and benchmarking-result\n",
    "runner_proc_module = \"benchmarking\"  # Default: \"benchmarking\"\n",
    "result_proc_module = \"benchmarking-result\"  # Default: \"benchmarking-result\"\n",
    "\n",
    "# Run the recipe with the defined endpoint(s)\n",
    "# If the id exists, it will perform a load on the runner, instead of creating a new runner.\n",
    "# Using an existing runner allows the new run to possibly use cached results from previous runs, which greatly reduces the run time\n",
    "slugify_id = slugify(name, lowercase=True)\n",
    "if slugify_id in api_get_all_runner_name():\n",
    "    rec_runner = api_load_runner(slugify_id)\n",
    "else:\n",
    "    rec_runner = api_create_runner(name, endpoints)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "# run_cookbooks is an async function. Currently there is no sync version.\n",
    "# We will get an existing event loop and execute the run cookbooks process.\n",
    "await rec_runner.run_recipes(\n",
    "    recipes,\n",
    "    num_of_prompts,\n",
    "    random_seed,\n",
    "    system_prompt,\n",
    "    runner_proc_module,\n",
    "    result_proc_module,\n",
    ")\n",
    "await rec_runner.close()  # Perform a close on the runner to allow proper cleanup.\n",
    "\n",
    "# Display results\n",
    "runner_runs = api_get_all_run(rec_runner.id)\n",
    "result_info = runner_runs[-1].get(\"results\")\n",
    "if result_info:\n",
    "    print(json.dumps(result_info, indent=2))\n",
    "else:\n",
    "    raise RuntimeError(\"no run result generated\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90595255-2237-40d7-a06b-90d526322a00",
   "metadata": {},
   "source": [
    "## Beautifying Test Results\n",
    "\n",
    "The result above is shown in our raw JSON file. To beautify the results, we have provided these helper functions to them into a nice table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ca3dd8-6b5a-479f-85b1-4e74f4482889",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.columns import Columns\n",
    "from rich.console import Console\n",
    "from rich.panel import Panel\n",
    "from rich.table import Table\n",
    "console = Console()\n",
    "\n",
    "def show_recipe_results(recipes, endpoints, recipe_results, duration):\n",
    "    \"\"\"\n",
    "    Show the results of the recipe benchmarking.\n",
    "\n",
    "    This function takes the recipes, endpoints, recipe results, results file, and duration as arguments.\n",
    "    If there are any recipe results, it generates a table to display them using the generate_recipe_table function.\n",
    "    It also prints the location of the results file and the time taken to run the benchmarking.\n",
    "    If there are no recipe results, it prints a message indicating that there are no results.\n",
    "\n",
    "    Args:\n",
    "        recipes (list): A list of recipes that were benchmarked.\n",
    "        endpoints (list): A list of endpoints that were used in the benchmarking.\n",
    "        recipe_results (dict): A dictionary with the results of the recipe benchmarking.\n",
    "        duration (float): The time taken to run the benchmarking in seconds.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if recipe_results:\n",
    "        # Display recipe results\n",
    "        generate_recipe_table(recipes, endpoints, recipe_results)\n",
    "    else:\n",
    "        console.print(\"[red]There are no results.[/red]\")\n",
    "\n",
    "    # Print run stats\n",
    "    console.print(f\"{'='*50}\\n[blue]Time taken to run: {duration}s[/blue]\\n*Overall rating will be the lowest grade that the recipes have in each cookbook\\n{'='*50}\")\n",
    "\n",
    "def generate_recipe_table(recipes: list, endpoints: list, results: dict) -> None:\n",
    "    \"\"\"\n",
    "    Generate and display a table of recipe results.\n",
    "\n",
    "    This function creates a table that lists the results of running recipes against various endpoints.\n",
    "    Each row in the table corresponds to a recipe, and each column corresponds to an endpoint.\n",
    "    The results include the grade and average grade value for each recipe-endpoint pair.\n",
    "\n",
    "    Args:\n",
    "        recipes (list): A list of recipe IDs that were benchmarked.\n",
    "        endpoints (list): A list of endpoint IDs against which the recipes were run.\n",
    "        results (dict): A dictionary containing the results of the benchmarking.\n",
    "\n",
    "    Returns:\n",
    "        None: This function does not return anything. It prints the table to the console.\n",
    "    \"\"\"\n",
    "    # Create a table with a title and headers\n",
    "    table = Table(\n",
    "        title=\"Recipes Result\", show_lines=True, expand=True, header_style=\"bold\"\n",
    "    )\n",
    "    table.add_column(\"No.\", width=2)\n",
    "    table.add_column(\"Recipe\", justify=\"left\", width=78)\n",
    "    # Add a column for each endpoint\n",
    "    for endpoint in endpoints:\n",
    "        table.add_column(endpoint, justify=\"center\")\n",
    "\n",
    "    # Iterate over each recipe and populate the table with results\n",
    "    for index, recipe_id in enumerate(recipes, start=1):\n",
    "        # Attempt to find the result for the current recipe\n",
    "        recipe_result = next(\n",
    "            (\n",
    "                result\n",
    "                for result in results[\"results\"][\"recipes\"]\n",
    "                if result[\"id\"] == recipe_id\n",
    "            ),\n",
    "            None,\n",
    "        )\n",
    "\n",
    "        # If the result exists, extract and format the results for each endpoint\n",
    "        if recipe_result:\n",
    "            endpoint_results = []\n",
    "            for endpoint in endpoints:\n",
    "                # Find the evaluation summary for the endpoint\n",
    "                evaluation_summary = next(\n",
    "                    (\n",
    "                        eval_summary\n",
    "                        for eval_summary in recipe_result[\"evaluation_summary\"]\n",
    "                        if eval_summary[\"model_id\"] == endpoint\n",
    "                    ),\n",
    "                    None,\n",
    "                )\n",
    "\n",
    "                # Format the grade and average grade value, or use \"-\" if not found\n",
    "                grade = \"-\"\n",
    "                if (\n",
    "                    evaluation_summary\n",
    "                    and \"grade\" in evaluation_summary\n",
    "                    and \"avg_grade_value\" in evaluation_summary\n",
    "                    and evaluation_summary[\"grade\"]\n",
    "                ):\n",
    "                    grade = f\"{evaluation_summary['grade']} [{evaluation_summary['avg_grade_value']}]\"\n",
    "                endpoint_results.append(grade)\n",
    "\n",
    "            # Add a row for the recipe with its results\n",
    "            table.add_row(\n",
    "                str(index),\n",
    "                f\"Recipe: [blue]{recipe_result['id']}[/blue]\",\n",
    "                *endpoint_results,\n",
    "                end_section=True,\n",
    "            )\n",
    "        else:\n",
    "            # If no result is found, add a row with placeholders\n",
    "            table.add_row(\n",
    "                str(index),\n",
    "                f\"Recipe: [blue]{recipe_id}[/blue]\",\n",
    "                *([\"-\"] * len(endpoints)),\n",
    "                end_section=True,\n",
    "            )\n",
    "\n",
    "    # Print the table to the console\n",
    "    console.print(table)\n",
    "\n",
    "if result_info:\n",
    "    show_recipe_results(\n",
    "            recipes, endpoints, result_info, result_info[\"metadata\"][\"duration\"]\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a28bf55-0e7e-4c2e-b372-97294e0e34eb",
   "metadata": {},
   "source": [
    "## Create a new `cookbook`\n",
    "\n",
    "We can also create a new `cookbook` and add existing recipes together with our new recipe. A `cookbook` in Moonshot is a curated collection of `recipes` designed to be executed together.\n",
    "\n",
    "To create a new cookbook, you need the following fields:\n",
    "\n",
    "1. **Name**: A unique name for the cookbook.\n",
    "2. **Description**: A detailed explanation of the cookbook's purpose and the recipe(s) it contains.\n",
    "3. **Recipes**: A list of recipe(s) that are included in the cookbook. Each recipe represents a specific test or benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288bb18b-a5ee-49b6-97dd-7d33c336c3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cookbook_id = api_create_cookbook(\n",
    "    \"test-cookbook\",\n",
    "    \"This cookbook tests both fruits questions and general science questions.\",\n",
    "    [\"fruit-questions\", \"mmlu\"]\n",
    ")\n",
    "\n",
    "print(f\"Cookbook '{cookbook_id}' has been created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76208f03-e0a3-4d55-b565-267e1a847027",
   "metadata": {},
   "source": [
    "## Run your new cookbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b5b3ce-779c-47f8-94fc-9a2f2147236b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from slugify import slugify\n",
    "from moonshot.api import api_get_all_run, api_create_runner, api_get_all_runner_name\n",
    "\n",
    "name = \"test new cookbook\" # Indicate the name\n",
    "cookbooks = [\"test-cookbook\"] # Test one cookbook test-cookbook. You can add more cookbooks in the list to test as well\n",
    "endpoints = [\"my-openai-endpoint\"] # Test against 1 endpoint, my-openai-endpoint\n",
    "num_of_prompts = 5 # use a smaller number to test out the function; 0 means using all prompts in dataset\n",
    "\n",
    "# Optional fields\n",
    "random_seed = 0   # Default: 0; this allows for randomness in dataset selection when num_of_prompts are set\n",
    "system_prompt = \"\"  # Default: \"\"; this allows setting the system prompt for the endpoints\n",
    "\n",
    "# Advanced user - Modify runner processing module and result processing module\n",
    "# Default: benchmarking and benchmarking-result. Change it to your module name if you have your own runner and/or result module\n",
    "runner_proc_module = \"benchmarking\"  # Default: \"benchmarking\"\n",
    "result_proc_module = \"benchmarking-result\"  # Default: \"benchmarking-result\"\n",
    "\n",
    "# Run the cookbooks with the defined endpoint(s)\n",
    "# If the id exists, it will perform a load on the runner, instead of creating a new runner.\n",
    "# Using an existing runner allows the new run to possibly use cached results from previous runs, which greatly reduces the run time\n",
    "slugify_id = slugify(name, lowercase=True)\n",
    "if slugify_id in api_get_all_runner_name():\n",
    "    cb_runner = api_load_runner(slugify_id)\n",
    "else:\n",
    "    cb_runner = api_create_runner(name, endpoints)\n",
    "\n",
    "# run_cookbooks() is an async function. Currently there is no sync version\n",
    "# We will get an existing event loop and execute the run cookbooks process\n",
    "await cb_runner.run_cookbooks(\n",
    "        cookbooks,\n",
    "        num_of_prompts,\n",
    "        random_seed,\n",
    "        system_prompt,\n",
    "        runner_proc_module,\n",
    "        result_proc_module,\n",
    "    )\n",
    "await cb_runner.close()  # Perform a close on the runner to allow proper cleanup.\n",
    "\n",
    "# Display results\n",
    "runner_runs = api_get_all_run(cb_runner.id)\n",
    "result_info = runner_runs[-1].get(\"results\")\n",
    "if result_info:\n",
    "    print(json.dumps(result_info, indent=2))\n",
    "else:\n",
    "    raise RuntimeError(\"no run result generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf2ab13-bf8c-401b-a0f5-b59339fe0749",
   "metadata": {},
   "source": [
    "## Beautifying Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adebeba-18b1-4fd6-8e95-b705d24f769d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.columns import Columns\n",
    "from rich.console import Console\n",
    "from rich.panel import Panel\n",
    "from rich.table import Table\n",
    "console = Console()\n",
    "\n",
    "def show_cookbook_results(cookbooks, endpoints, cookbook_results, duration):\n",
    "    \"\"\"\n",
    "    Show the results of the cookbook benchmarking.\n",
    "\n",
    "    This function takes the cookbooks, endpoints, cookbook results, results file, and duration as arguments.\n",
    "    If there are results, it generates a table with the cookbook results and prints a message indicating\n",
    "    where the results are saved. If there are no results, it prints a message indicating that no results were found.\n",
    "    Finally, it prints the duration of the run.\n",
    "\n",
    "    Args:\n",
    "        cookbooks (list): A list of cookbooks.\n",
    "        endpoints (list): A list of endpoints.\n",
    "        cookbook_results (dict): A dictionary with the results of the cookbook benchmarking.\n",
    "        duration (float): The duration of the run.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if cookbook_results:\n",
    "        # Display recipe results\n",
    "        generate_cookbook_table(cookbooks, endpoints, cookbook_results)\n",
    "    else:\n",
    "        console.print(\"[red]There are no results.[/red]\")\n",
    "\n",
    "    # Print run stats\n",
    "    console.print(f\"{'='*50}\\n[blue]Time taken to run: {duration}s[/blue]\\n*Overall rating will be the lowest grade that the recipes have in each cookbook\\n{'='*50}\")\n",
    "\n",
    "def generate_cookbook_table(cookbooks: list, endpoints: list, results: dict) -> None:\n",
    "    \"\"\"\n",
    "    Generate and display a table with the cookbook benchmarking results.\n",
    "\n",
    "    This function creates a table that includes the index, cookbook name, recipe name, and the results\n",
    "    for each endpoint.\n",
    "\n",
    "    The cookbook names are prefixed with \"Cookbook:\" and are displayed with their overall grades. Each recipe under a\n",
    "    cookbook is indented and prefixed with \"Recipe:\" followed by its individual grades for each endpoint. If there are\n",
    "    no results for a cookbook, a row with dashes across all endpoint columns is added to indicate this.\n",
    "\n",
    "    Args:\n",
    "        cookbooks (list): A list of cookbook names to display in the table.\n",
    "        endpoints (list): A list of endpoints for which results are to be displayed.\n",
    "        results (dict): A dictionary containing the benchmarking results for cookbooks and recipes.\n",
    "\n",
    "    Returns:\n",
    "        None: The function prints the table to the console but does not return any value.\n",
    "    \"\"\"\n",
    "    table = Table(\n",
    "        title=\"Cookbook Result\", show_lines=True, expand=True, header_style=\"bold\"\n",
    "    )\n",
    "    table.add_column(\"No.\", width=2)\n",
    "    table.add_column(\"Cookbook (with its recipes)\", justify=\"left\", width=78)\n",
    "    for endpoint in endpoints:\n",
    "        table.add_column(endpoint, justify=\"center\")\n",
    "\n",
    "    index = 1\n",
    "    for cookbook in cookbooks:\n",
    "        # Get cookbook result\n",
    "        cookbook_result = next(\n",
    "            (\n",
    "                result\n",
    "                for result in results[\"results\"][\"cookbooks\"]\n",
    "                if result[\"id\"] == cookbook\n",
    "            ),\n",
    "            None,\n",
    "        )\n",
    "\n",
    "        if cookbook_result:\n",
    "            # Add the cookbook name with the \"Cookbook: \" prefix as the first row for this section\n",
    "            endpoint_results = []\n",
    "            for endpoint in endpoints:\n",
    "                # Find the evaluation summary for the endpoint\n",
    "                evaluation_summary = next(\n",
    "                    (\n",
    "                        temp_eval\n",
    "                        for temp_eval in cookbook_result[\"overall_evaluation_summary\"]\n",
    "                        if temp_eval[\"model_id\"] == endpoint\n",
    "                    ),\n",
    "                    None,\n",
    "                )\n",
    "\n",
    "                # Get the grade from the evaluation_summary, or use \"-\" if not found\n",
    "                grade = \"-\"\n",
    "                if evaluation_summary and evaluation_summary[\"overall_grade\"]:\n",
    "                    grade = evaluation_summary[\"overall_grade\"]\n",
    "                endpoint_results.append(grade)\n",
    "            table.add_row(\n",
    "                str(index),\n",
    "                f\"Cookbook: [blue]{cookbook}[/blue]\",\n",
    "                *endpoint_results,\n",
    "                end_section=True,\n",
    "            )\n",
    "\n",
    "            for recipe in cookbook_result[\"recipes\"]:\n",
    "                endpoint_results = []\n",
    "                for endpoint in endpoints:\n",
    "                    # Find the evaluation summary for the endpoint\n",
    "                    evaluation_summary = next(\n",
    "                        (\n",
    "                            temp_eval\n",
    "                            for temp_eval in recipe[\"evaluation_summary\"]\n",
    "                            if temp_eval[\"model_id\"] == endpoint\n",
    "                        ),\n",
    "                        None,\n",
    "                    )\n",
    "\n",
    "                    # Get the grade from the evaluation_summary, or use \"-\" if not found\n",
    "                    grade = \"-\"\n",
    "                    if (\n",
    "                        evaluation_summary\n",
    "                        and \"grade\" in evaluation_summary\n",
    "                        and \"avg_grade_value\" in evaluation_summary\n",
    "                        and evaluation_summary[\"grade\"]\n",
    "                    ):\n",
    "                        grade = f\"{evaluation_summary['grade']} [{evaluation_summary['avg_grade_value']}]\"\n",
    "                    endpoint_results.append(grade)\n",
    "\n",
    "                # Add the recipe name indented under the cookbook name\n",
    "                table.add_row(\n",
    "                    \"\",\n",
    "                    f\"  └──  Recipe: [blue]{recipe['id']}[/blue]\",\n",
    "                    *endpoint_results,\n",
    "                    end_section=True,\n",
    "                )\n",
    "\n",
    "            # Increment index only after all recipes of the cookbook have been added\n",
    "            index += 1\n",
    "        else:\n",
    "            # If no results for the cookbook, add a row indicating this with the \"Cookbook: \" prefix\n",
    "            # and a dash for each endpoint column\n",
    "            table.add_row(\n",
    "                str(index),\n",
    "                f\"Cookbook: {cookbook}\",\n",
    "                *([\"-\"] * len(endpoints)),\n",
    "                end_section=True,\n",
    "            )\n",
    "            index += 1\n",
    "\n",
    "    # Display table\n",
    "    console.print(table)\n",
    "\n",
    "if result_info:\n",
    "    show_cookbook_results(\n",
    "        cookbooks, endpoints, result_info, result_info[\"metadata\"][\"duration\"]\n",
    "    )\n",
    "else:\n",
    "    raise RuntimeError(\"no run result generated\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-virtualenv-name-2",
   "language": "python",
   "name": "my-virtualenv-name-2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
