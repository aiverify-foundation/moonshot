{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdc5b948-4919-47fe-9598-0d4c1bfaf8ea",
   "metadata": {},
   "source": [
    "# Tutorial 2 - Basic Workflow - Add Your Own Tests \n",
    "\n",
    "**Scenario**: You are a model developer and you want to evaluate your custom chatbot in your CI/CD pipeline as your team continuously train and improve the system. In this case, you have already identified a dataset that does not exist in Moonshot to benchmark your model's performance. How can you add this new dataset into Moonshot and run it with your system?\n",
    "\n",
    "In this tutorial, you will learn how to:\n",
    "\n",
    "- Add your own `dataset` into Moonshot\n",
    "- Create and run your own `recipe`\n",
    "- Create and run your own `cookbook`\n",
    "\n",
    "**Prerequisite**:\n",
    "\n",
    "- You have added your `connector endpoint` in Moonshot. If you are unsure how to do it, please refer to \"Tutorial 1\" in the same folder.\n",
    "\n",
    "**Before starting this tutorial, please make sure you have already installed `moonshot` and `moonshot-data`.** Otherwise, please follow this tutorial to install and configure Moonshot first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb78f9e-6842-4524-add7-7434687cc7cc",
   "metadata": {},
   "source": [
    "## Import Moonshot Library API\n",
    "\n",
    "In this section, we prepare our Jupyter notebook environment by importing necessary libraries required to execute an existing benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbde151f-34fb-4e52-88cc-62c87834cea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moonshot Framework API Imports\n",
    "# These imports from the Moonshot framework allow us to interact with the API, \n",
    "# creating and managing various components such as recipes, cookbooks, and endpoints.\n",
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import sys\n",
    "\n",
    "# Ensure that the root of the Moonshot framework is in the system path for module importing.\n",
    "sys.path.insert(0, '../../')\n",
    "\n",
    "from moonshot.api import (\n",
    "    api_get_all_recipe,\n",
    "    api_create_recipe,\n",
    "    api_create_cookbook,\n",
    "    api_get_all_runner,\n",
    "    api_load_runner,\n",
    "    api_read_result,\n",
    "    api_set_environment_variables\n",
    ")\n",
    "\n",
    "moonshot_path = \"./data\"\n",
    "env = {\n",
    "    \"ATTACK_MODULES\": os.path.join(moonshot_path, \"attack-modules\"),\n",
    "    \"CONNECTORS\": os.path.join(moonshot_path, \"connectors\"),\n",
    "    \"CONNECTORS_ENDPOINTS\": os.path.join(moonshot_path, \"connectors-endpoints\"),\n",
    "    \"CONTEXT_STRATEGY\": os.path.join(moonshot_path, \"context-strategy\"),\n",
    "    \"COOKBOOKS\": os.path.join(moonshot_path, \"cookbooks\"),\n",
    "    \"DATABASES\": os.path.join(moonshot_path, \"generated-outputs/databases\"),\n",
    "    \"DATABASES_MODULES\": os.path.join(moonshot_path, \"databases-modules\"),\n",
    "    \"DATASETS\": os.path.join(moonshot_path, \"datasets\"),\n",
    "    \"IO_MODULES\": os.path.join(moonshot_path, \"io-modules\"),\n",
    "    \"METRICS\": os.path.join(moonshot_path, \"metrics\"),\n",
    "    \"PROMPT_TEMPLATES\": os.path.join(moonshot_path, \"prompt-templates\"),\n",
    "    \"RECIPES\": os.path.join(moonshot_path, \"recipes\"),\n",
    "    \"RESULTS\": os.path.join(moonshot_path, \"generated-outputs/results\"),\n",
    "    \"RESULTS_MODULES\": os.path.join(moonshot_path, \"results-modules\"),\n",
    "    \"RUNNERS\": os.path.join(moonshot_path, \"generated-outputs/runners\"),\n",
    "    \"RUNNERS_MODULES\": os.path.join(moonshot_path, \"runners-modules\"),\n",
    "}\n",
    "\n",
    "# Apply the environment variables to configure the Moonshot framework.\n",
    "api_set_environment_variables(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38df293d-0758-45cd-9181-cfc5ea39d7ff",
   "metadata": {},
   "source": [
    "## Prepare the Dataset\n",
    "\n",
    "In this section, we show how to prepare Moonshot dataset. Supposed you have a list of \"fruits\" questions to ask your chatbot, you need to prepare them into the data schema that is compatible with Moonshot.\n",
    "\n",
    "- `name` (str): name of the data\n",
    "- `description` (str): description of the dataset\n",
    "- `license` (str): license of the data\n",
    "- `reference` (str): a link/reference to where the dataset is from (or author of the dataset)\n",
    "- `examples` (list): A list of dictionary containing the prompt (`input`) and ground truth (`target`). A `target` can be left blank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2c801e9-4050-4dbb-ba42-22e31d5458d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = {\n",
    "    \"name\": \"Fruits Dataset\",\n",
    "    \"description\":\"Measures whether the model knows what is a fruit\",\n",
    "    \"license\": \"MIT license\",\n",
    "    \"reference\": \"\",\n",
    "    \"examples\": [\n",
    "        {\n",
    "            \"input\": \"Is Lemon a Fruit? Answer Yes or No.\",\n",
    "            \"target\": \"Yes.\"\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"Is Apple a Fruit? Answer Yes or No.\",\n",
    "            \"target\": \"Yes.\"\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"Is Bak Choy a Fruit? Answer Yes or No.\",\n",
    "            \"target\": \"No.\"\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"Is Bak Kwa a Fruit? Answer Yes or No.\",\n",
    "            \"target\": \"No.\"\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"Is Dragonfruit a Fruit? Answer Yes or No.\",\n",
    "            \"target\": \"Yes.\"\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"Is Orange a Fruit? Answer Yes or No.\",\n",
    "            \"target\": \"Yes.\"\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"Is Coke Zero a Fruit? Answer Yes or No.\",\n",
    "            \"target\": \"No.\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# to change later when notebook is shifted\n",
    "in_file = \"./data/datasets/test-dataset.json\"\n",
    "json.dump(test_dataset, open(in_file, \"w+\"), indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7acdff5-c59c-4179-badb-ab68560793a6",
   "metadata": {},
   "source": [
    "## Create a new `recipe`\n",
    "\n",
    "To run this dataset, you need to create a new `recipe`. A `recipe` contains all the details required to run a benchmark. A `recipe` guides Moonshot on what data to use, and how to evaluate the model's responses.\n",
    "\n",
    "To create a new recipe, you need the following elements:\n",
    "\n",
    "1. **Name**: A unique name for the recipe.\n",
    "2. **Description**: An explanation of what the recipe does and what it's for.\n",
    "3. **Tags**: Keywords that categorize the recipe, making it easier to find and group with similar recipes.\n",
    "4. **Categories**: Broader classifications that help organize recipes into collections.\n",
    "5. **Datasets**: The data that will be used when running the recipe. This could be a set of prompts, questions, or any input that the model will respond to.\n",
    "6. **Prompt Templates**: Pre-prompt or post-prompt\n",
    "7. **Metrics**: Criteria or measurements used to evaluate the model's responses, such as accuracy, fluency, or adherence to a prompt.\n",
    "8. **Attack Strategies**: Optional components that introduce adversarial testing scenarios to probe the model's robustness.\n",
    "9. **Grading Scale**: A set of thresholds or criteria used to grade or score the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1e855bb-1322-4cc5-b775-ddd0dcbdcfcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipe 'fruit-questions' has been created.\n"
     ]
    }
   ],
   "source": [
    "test_recipe = api_create_recipe(\n",
    "    \"Fruit Questions\", # name, mandatory\n",
    "    \"This recipe is created to test model's ability in answering fruits question.\", # description, mandatory\n",
    "    [\"chatbot\"], # tags, optional\n",
    "    [\"capability\"], # category, optional\n",
    "    [\"test-dataset\"], # dataset filename, mandatory\n",
    "    [], # prompt template, optional\n",
    "    [\"exactstrmatch\"], # metrics, mandatory\n",
    "    [], # attack strategy, optional\n",
    "    { # grading scale, optional\n",
    "        \"A\": [\n",
    "            80,\n",
    "            100\n",
    "        ],\n",
    "        \"B\": [\n",
    "            60,\n",
    "            79\n",
    "        ],\n",
    "        \"C\": [\n",
    "            40,\n",
    "            59\n",
    "        ],\n",
    "        \"D\": [\n",
    "            20,\n",
    "            39\n",
    "        ],\n",
    "        \"E\": [\n",
    "            0,\n",
    "            19\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Recipe '{test_recipe}' has been created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4e7fdb-6de1-4f3a-ac71-9605256f7a29",
   "metadata": {},
   "source": [
    "## Run your new recipe\n",
    "\n",
    "With this new recipe, you can run this on your `connector endpoint`. We will run this on endpoint `my-openai-endpoint`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86c6164b-fc57-457a-a640-f11c451fcdd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Established connection to database (data/generated-outputs/databases/my-new-recipe-runner.db)\n",
      "[Runner] my-new-recipe-runner - Running benchmark recipe run...\n",
      "[Run] Part 0: Initialising run...\n",
      "[Run] Initialise run took 0.0021s\n",
      "[Run] Part 1: Loading asyncio running loop...\n",
      "[Run] Part 2: Loading modules...\n",
      "[Run] Module loading took 0.0043s\n",
      "[Run] Part 3: Running runner processing module...\n",
      "[Benchmarking] Load recipe connectors took 0.2175s\n",
      "[Benchmarking] Set connectors system prompt took 0.0000s\n",
      "[Benchmarking] Part 1: Running recipes (['fruit-questions'])...\n",
      "[Benchmarking] Running recipe fruit-questions... (1/1)\n",
      "[Benchmarking] Load required instances...\n",
      "[Benchmarking] Load recipe instance took 0.0255s\n",
      "[Benchmarking] Load recipe metrics took 0.0008s\n",
      "[Benchmarking] Build and execute generator pipeline...\n",
      "[Benchmarking] Dataset test-dataset, using 5 of 7 prompts.\n",
      "[Benchmarking] Predicting prompts for recipe [fruit-questions] took 0.0012s\n",
      "[Benchmarking] Sorting the recipe predictions into groups\n",
      "[Benchmarking] Sorted the recipe predictions into groups for recipe [fruit-questions] took 0.0000s\n",
      "[Benchmarking] Performing metrics calculation\n",
      "[Benchmarking] Running metrics for conn_id (my-openai-endpoint), recipe_id (fruit-questions), dataset_id (test-dataset), prompt_template_id (no-template)\n",
      "[exactstrmatch] Running [get_results] took 0.0000s\n",
      "[Benchmarking] Performing metrics calculation for recipe [fruit-questions] took 0.0000s\n",
      "[Benchmarking] Run took 0.0292s\n",
      "[Benchmarking] Updating completion status...\n",
      "[Benchmarking] Preparing results...\n",
      "[Benchmarking] Preparing results took 0.0000s\n",
      "[Run] Running runner processing module took 0.2473s\n",
      "[Run] Part 4: Running result processing module...\n",
      "[BenchmarkingResult] Generate results took 0.0110s\n",
      "[Run] Running result processing module took 0.0118s\n",
      "[Run] Part 5: Wrap up run...\n",
      "[Runner] my-new-recipe-runner - Benchmark recipe run completed and reset.\n",
      "Closed connection to database (data/generated-outputs/databases/my-new-recipe-runner.db)\n",
      "Established connection to database (data/generated-outputs/databases/my-new-recipe-runner.db)\n",
      "{\n",
      "  \"metadata\": {\n",
      "    \"id\": \"my-new-recipe-runner\",\n",
      "    \"start_time\": \"2024-05-28 18:08:40\",\n",
      "    \"end_time\": \"2024-05-28 18:08:40\",\n",
      "    \"duration\": 0,\n",
      "    \"status\": \"completed\",\n",
      "    \"recipes\": [\n",
      "      \"fruit-questions\"\n",
      "    ],\n",
      "    \"cookbooks\": null,\n",
      "    \"endpoints\": [\n",
      "      \"my-openai-endpoint\"\n",
      "    ],\n",
      "    \"num_of_prompts\": 5,\n",
      "    \"random_seed\": 0,\n",
      "    \"system_prompt\": \"\"\n",
      "  },\n",
      "  \"results\": {\n",
      "    \"recipes\": [\n",
      "      {\n",
      "        \"id\": \"fruit-questions\",\n",
      "        \"details\": [\n",
      "          {\n",
      "            \"model_id\": \"my-openai-endpoint\",\n",
      "            \"dataset_id\": \"test-dataset\",\n",
      "            \"prompt_template_id\": \"no-template\",\n",
      "            \"data\": [\n",
      "              {\n",
      "                \"prompt\": \"Is Apple a Fruit? Answer Yes or No.\",\n",
      "                \"predicted_result\": \"Yes\",\n",
      "                \"target\": \"Yes.\",\n",
      "                \"duration\": 3.0777706670050975\n",
      "              },\n",
      "              {\n",
      "                \"prompt\": \"Is Bak Choy a Fruit? Answer Yes or No.\",\n",
      "                \"predicted_result\": \"No.\",\n",
      "                \"target\": \"No.\",\n",
      "                \"duration\": 3.058509624999715\n",
      "              },\n",
      "              {\n",
      "                \"prompt\": \"Is Dragonfruit a Fruit? Answer Yes or No.\",\n",
      "                \"predicted_result\": \"Yes\",\n",
      "                \"target\": \"Yes.\",\n",
      "                \"duration\": 3.0694101250046515\n",
      "              },\n",
      "              {\n",
      "                \"prompt\": \"Is Coke Zero a Fruit? Answer Yes or No.\",\n",
      "                \"predicted_result\": \"No.\",\n",
      "                \"target\": \"No.\",\n",
      "                \"duration\": 3.067069292002998\n",
      "              }\n",
      "            ],\n",
      "            \"metrics\": [\n",
      "              {\n",
      "                \"accuracy\": 50.0,\n",
      "                \"grading_criteria\": {\n",
      "                  \"accuracy\": 50.0\n",
      "                }\n",
      "              }\n",
      "            ]\n",
      "          }\n",
      "        ],\n",
      "        \"evaluation_summary\": [\n",
      "          {\n",
      "            \"model_id\": \"my-openai-endpoint\",\n",
      "            \"num_of_prompts\": 4,\n",
      "            \"avg_grade_value\": 50.0,\n",
      "            \"grade\": \"C\"\n",
      "          }\n",
      "        ],\n",
      "        \"grading_scale\": {\n",
      "          \"A\": [\n",
      "            80,\n",
      "            100\n",
      "          ],\n",
      "          \"B\": [\n",
      "            60,\n",
      "            79\n",
      "          ],\n",
      "          \"C\": [\n",
      "            40,\n",
      "            59\n",
      "          ],\n",
      "          \"D\": [\n",
      "            20,\n",
      "            39\n",
      "          ],\n",
      "          \"E\": [\n",
      "            0,\n",
      "            19\n",
      "          ]\n",
      "        },\n",
      "        \"total_num_of_prompts\": 4\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from slugify import slugify\n",
    "from moonshot.api import api_get_all_run, api_create_runner, api_get_all_runner_name\n",
    "\n",
    "name = \"my new recipe runner\" # Indicate the name\n",
    "recipes = [\"fruit-questions\"] # Test against 2 recipes, item-category and bbq\n",
    "endpoints = [\"my-openai-endpoint\"]  # Test against 1 endpoint, test-openai-endpoint\n",
    "num_of_prompts = 5 # use a smaller number to test out the function; 0 means using all prompts in dataset\n",
    "\n",
    "# Below are the optional fields\n",
    "random_seed = 0   # Default: 0; this allows for randomness in dataset selection when num_of_prompts are set\n",
    "system_prompt = \"\"  # Default: \"\"; this allows setting the system prompt for the endpoints\n",
    "\n",
    "# Advanced user - Modify runner processing module and result processing module\n",
    "# Default: benchmarking and benchmarking-result\n",
    "runner_proc_module = \"benchmarking\"  # Default: \"benchmarking\"\n",
    "result_proc_module = \"benchmarking-result\"  # Default: \"benchmarking-result\"\n",
    "\n",
    "# Run the recipes with the defined endpoints\n",
    "# If the id exists, it will perform a load on the runner, instead of creating a new runner.\n",
    "# The benefit of this, allows the new run to use possible cached results from previous runs which greatly enhances the run time.\n",
    "slugify_id = slugify(name, lowercase=True)\n",
    "if slugify_id in api_get_all_runner_name():\n",
    "    rec_runner = api_load_runner(slugify_id)\n",
    "else:\n",
    "    rec_runner = api_create_runner(name, endpoints)\n",
    "\n",
    "# run_cookbooks is an async function. Currently there is no sync version.\n",
    "# We will get an existing event loop and execute the run cookbooks process.\n",
    "await rec_runner.run_recipes(\n",
    "    recipes,\n",
    "    num_of_prompts,\n",
    "    random_seed,\n",
    "    system_prompt,\n",
    "    runner_proc_module,\n",
    "    result_proc_module,\n",
    ")\n",
    "rec_runner.close()  # Perform a close on the runner to allow proper cleanup.\n",
    "\n",
    "# Display results\n",
    "runner_runs = api_get_all_run(rec_runner.id)\n",
    "result_info = runner_runs[-1].get(\"results\")\n",
    "if result_info:\n",
    "    print(json.dumps(result_info, indent=2))\n",
    "else:\n",
    "    raise RuntimeError(\"no run result generated\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90595255-2237-40d7-a06b-90d526322a00",
   "metadata": {},
   "source": [
    "## Beautifying Test Results\n",
    "\n",
    "The result above is shown in our raw JSON file. To beautify the results, we have provided these helper functions to them into a nice table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2ca3dd8-6b5a-479f-85b1-4e74f4482889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                                  Recipes Result                                                   </span>\n",
       "┏━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> No. </span>┃<span style=\"font-weight: bold\"> Recipe                                                                              </span>┃<span style=\"font-weight: bold\"> my-openai-endpoint  </span>┃\n",
       "┡━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ 1   │ Recipe: <span style=\"color: #000080; text-decoration-color: #000080\">fruit-questions</span>                                                             │      C [50.0]       │\n",
       "└─────┴─────────────────────────────────────────────────────────────────────────────────────┴─────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                                  Recipes Result                                                   \u001b[0m\n",
       "┏━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mNo.\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mRecipe                                                                             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mmy-openai-endpoint \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ 1   │ Recipe: \u001b[34mfruit-questions\u001b[0m                                                             │      C [50.0]       │\n",
       "└─────┴─────────────────────────────────────────────────────────────────────────────────────┴─────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">==================================================\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">Time taken to run: 0s</span>\n",
       "*Overall rating will be the lowest grade that the recipes have in each cookbook\n",
       "==================================================\n",
       "</pre>\n"
      ],
      "text/plain": [
       "==================================================\n",
       "\u001b[34mTime taken to run: 0s\u001b[0m\n",
       "*Overall rating will be the lowest grade that the recipes have in each cookbook\n",
       "==================================================\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich.columns import Columns\n",
    "from rich.console import Console\n",
    "from rich.panel import Panel\n",
    "from rich.table import Table\n",
    "console = Console()\n",
    "\n",
    "def show_recipe_results(recipes, endpoints, recipe_results, duration):\n",
    "    \"\"\"\n",
    "    Show the results of the recipe benchmarking.\n",
    "\n",
    "    This function takes the recipes, endpoints, recipe results, results file, and duration as arguments.\n",
    "    If there are any recipe results, it generates a table to display them using the generate_recipe_table function.\n",
    "    It also prints the location of the results file and the time taken to run the benchmarking.\n",
    "    If there are no recipe results, it prints a message indicating that there are no results.\n",
    "\n",
    "    Args:\n",
    "        recipes (list): A list of recipes that were benchmarked.\n",
    "        endpoints (list): A list of endpoints that were used in the benchmarking.\n",
    "        recipe_results (dict): A dictionary with the results of the recipe benchmarking.\n",
    "        duration (float): The time taken to run the benchmarking in seconds.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if recipe_results:\n",
    "        # Display recipe results\n",
    "        generate_recipe_table(recipes, endpoints, recipe_results)\n",
    "    else:\n",
    "        console.print(\"[red]There are no results.[/red]\")\n",
    "\n",
    "    # Print run stats\n",
    "    console.print(f\"{'='*50}\\n[blue]Time taken to run: {duration}s[/blue]\\n*Overall rating will be the lowest grade that the recipes have in each cookbook\\n{'='*50}\")\n",
    "\n",
    "def generate_recipe_table(recipes: list, endpoints: list, results: dict) -> None:\n",
    "    \"\"\"\n",
    "    Generate and display a table of recipe results.\n",
    "\n",
    "    This function creates a table that lists the results of running recipes against various endpoints.\n",
    "    Each row in the table corresponds to a recipe, and each column corresponds to an endpoint.\n",
    "    The results include the grade and average grade value for each recipe-endpoint pair.\n",
    "\n",
    "    Args:\n",
    "        recipes (list): A list of recipe IDs that were benchmarked.\n",
    "        endpoints (list): A list of endpoint IDs against which the recipes were run.\n",
    "        results (dict): A dictionary containing the results of the benchmarking.\n",
    "\n",
    "    Returns:\n",
    "        None: This function does not return anything. It prints the table to the console.\n",
    "    \"\"\"\n",
    "    # Create a table with a title and headers\n",
    "    table = Table(\n",
    "        title=\"Recipes Result\", show_lines=True, expand=True, header_style=\"bold\"\n",
    "    )\n",
    "    table.add_column(\"No.\", width=2)\n",
    "    table.add_column(\"Recipe\", justify=\"left\", width=78)\n",
    "    # Add a column for each endpoint\n",
    "    for endpoint in endpoints:\n",
    "        table.add_column(endpoint, justify=\"center\")\n",
    "\n",
    "    # Iterate over each recipe and populate the table with results\n",
    "    for index, recipe_id in enumerate(recipes, start=1):\n",
    "        # Attempt to find the result for the current recipe\n",
    "        recipe_result = next(\n",
    "            (\n",
    "                result\n",
    "                for result in results[\"results\"][\"recipes\"]\n",
    "                if result[\"id\"] == recipe_id\n",
    "            ),\n",
    "            None,\n",
    "        )\n",
    "\n",
    "        # If the result exists, extract and format the results for each endpoint\n",
    "        if recipe_result:\n",
    "            endpoint_results = []\n",
    "            for endpoint in endpoints:\n",
    "                # Find the evaluation summary for the endpoint\n",
    "                evaluation_summary = next(\n",
    "                    (\n",
    "                        eval_summary\n",
    "                        for eval_summary in recipe_result[\"evaluation_summary\"]\n",
    "                        if eval_summary[\"model_id\"] == endpoint\n",
    "                    ),\n",
    "                    None,\n",
    "                )\n",
    "\n",
    "                # Format the grade and average grade value, or use \"-\" if not found\n",
    "                grade = \"-\"\n",
    "                if (\n",
    "                    evaluation_summary\n",
    "                    and \"grade\" in evaluation_summary\n",
    "                    and \"avg_grade_value\" in evaluation_summary\n",
    "                    and evaluation_summary[\"grade\"]\n",
    "                ):\n",
    "                    grade = f\"{evaluation_summary['grade']} [{evaluation_summary['avg_grade_value']}]\"\n",
    "                endpoint_results.append(grade)\n",
    "\n",
    "            # Add a row for the recipe with its results\n",
    "            table.add_row(\n",
    "                str(index),\n",
    "                f\"Recipe: [blue]{recipe_result['id']}[/blue]\",\n",
    "                *endpoint_results,\n",
    "                end_section=True,\n",
    "            )\n",
    "        else:\n",
    "            # If no result is found, add a row with placeholders\n",
    "            table.add_row(\n",
    "                str(index),\n",
    "                f\"Recipe: [blue]{recipe_id}[/blue]\",\n",
    "                *([\"-\"] * len(endpoints)),\n",
    "                end_section=True,\n",
    "            )\n",
    "\n",
    "    # Print the table to the console\n",
    "    console.print(table)\n",
    "\n",
    "if result_info:\n",
    "    show_recipe_results(\n",
    "            recipes, endpoints, result_info, result_info[\"metadata\"][\"duration\"]\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a28bf55-0e7e-4c2e-b372-97294e0e34eb",
   "metadata": {},
   "source": [
    "## Create a new `cookbook`\n",
    "\n",
    "We can also create a new `cookbook` and add existing recipes together with our new recipe. A `cookbook` in Moonshot is a curated collection of `recipes` designed to be executed together.\n",
    "\n",
    "To create a new cookbook, you need the following elements:\n",
    "\n",
    "1. **Name**: A unique name for the cookbook.\n",
    "2. **Description**: A detailed explanation of the cookbook's purpose and the types of recipes it contains.\n",
    "3. **Recipes**: A list of recipe names that are included in the cookbook. Each recipe represents a specific test or benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "288bb18b-a5ee-49b6-97dd-7d33c336c3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cookbook 'test-cookbook' has been created.\n"
     ]
    }
   ],
   "source": [
    "cookbook_id = api_create_cookbook(\n",
    "    \"test-cookbook\",\n",
    "    \"This cookbook tests both fruits questions and general science questions.\",\n",
    "    [\"fruit-questions\", \"mmlu\"]\n",
    ")\n",
    "\n",
    "print(f\"Cookbook '{cookbook_id}' has been created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76208f03-e0a3-4d55-b565-267e1a847027",
   "metadata": {},
   "source": [
    "## Run your new cookbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9b5b3ce-779c-47f8-94fc-9a2f2147236b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Established connection to database (data/generated-outputs/databases/test-new-cookbook.db)\n",
      "[Runner] test-new-cookbook - Running benchmark cookbook run...\n",
      "[Run] Part 0: Initialising run...\n",
      "[Run] Initialise run took 0.0017s\n",
      "[Run] Part 1: Loading asyncio running loop...\n",
      "[Run] Part 2: Loading modules...\n",
      "[Run] Module loading took 0.0032s\n",
      "[Run] Part 3: Running runner processing module...\n",
      "[Benchmarking] Load recipe connectors took 0.0229s\n",
      "[Benchmarking] Set connectors system prompt took 0.0000s\n",
      "[Benchmarking] Part 1: Running cookbooks (['test-cookbook'])...\n",
      "[Benchmarking] Running cookbook test-cookbook... (1/1)\n",
      "[Benchmarking] Load required instances...\n",
      "[Benchmarking] Load cookbook instance took 0.0007s\n",
      "[Benchmarking] Running cookbook recipes...\n",
      "[Benchmarking] Running recipe fruit-questions... (1/2)\n",
      "[Benchmarking] Load required instances...\n",
      "[Benchmarking] Load recipe instance took 0.0013s\n",
      "[Benchmarking] Load recipe metrics took 0.0006s\n",
      "[Benchmarking] Build and execute generator pipeline...\n",
      "[Benchmarking] Dataset test-dataset, using 5 of 7 prompts.\n",
      "[Benchmarking] Predicting prompts for recipe [fruit-questions] took 0.0015s\n",
      "[Benchmarking] Sorting the recipe predictions into groups\n",
      "[Benchmarking] Sorted the recipe predictions into groups for recipe [fruit-questions] took 0.0000s\n",
      "[Benchmarking] Performing metrics calculation\n",
      "[Benchmarking] Running metrics for conn_id (my-openai-endpoint), recipe_id (fruit-questions), dataset_id (test-dataset), prompt_template_id (no-template)\n",
      "[exactstrmatch] Running [get_results] took 0.0000s\n",
      "[Benchmarking] Performing metrics calculation for recipe [fruit-questions] took 0.0000s\n",
      "[Benchmarking] Running recipe mmlu... (2/2)\n",
      "[Benchmarking] Load required instances...\n",
      "[Benchmarking] Load recipe instance took 0.0036s\n",
      "[Benchmarking] Load recipe metrics took 0.0004s\n",
      "[Benchmarking] Build and execute generator pipeline...\n",
      "[Benchmarking] Dataset mmlu-all, using 5 of 17487 prompts.\n",
      "[Benchmarking] Predicting prompts for recipe [mmlu] took 0.0759s\n",
      "[Benchmarking] Sorting the recipe predictions into groups\n",
      "[Benchmarking] Sorted the recipe predictions into groups for recipe [mmlu] took 0.0000s\n",
      "[Benchmarking] Performing metrics calculation\n",
      "[Benchmarking] Running metrics for conn_id (my-openai-endpoint), recipe_id (mmlu), dataset_id (mmlu-all), prompt_template_id (mmlu)\n",
      "[exactstrmatch] Running [get_results] took 0.0000s\n",
      "[Benchmarking] Performing metrics calculation for recipe [mmlu] took 0.0000s\n",
      "[Benchmarking] Running cookbook [test-cookbook] took 0.0862s\n",
      "[Benchmarking] Run took 0.0889s\n",
      "[Benchmarking] Updating completion status...\n",
      "[Benchmarking] Preparing results...\n",
      "[Benchmarking] Preparing results took 0.0000s\n",
      "[Run] Running runner processing module took 0.1129s\n",
      "[Run] Part 4: Running result processing module...\n",
      "[BenchmarkingResult] Generate results took 0.0033s\n",
      "[Run] Running result processing module took 0.0040s\n",
      "[Run] Part 5: Wrap up run...\n",
      "[Runner] test-new-cookbook - Benchmark cookbook run completed and reset.\n",
      "Closed connection to database (data/generated-outputs/databases/test-new-cookbook.db)\n",
      "Established connection to database (data/generated-outputs/databases/test-new-cookbook.db)\n",
      "{\n",
      "  \"metadata\": {\n",
      "    \"id\": \"test-new-cookbook\",\n",
      "    \"start_time\": \"2024-05-28 18:08:52\",\n",
      "    \"end_time\": \"2024-05-28 18:08:52\",\n",
      "    \"duration\": 0,\n",
      "    \"status\": \"completed\",\n",
      "    \"recipes\": null,\n",
      "    \"cookbooks\": [\n",
      "      \"test-cookbook\"\n",
      "    ],\n",
      "    \"endpoints\": [\n",
      "      \"my-openai-endpoint\"\n",
      "    ],\n",
      "    \"num_of_prompts\": 5,\n",
      "    \"random_seed\": 0,\n",
      "    \"system_prompt\": \"\"\n",
      "  },\n",
      "  \"results\": {\n",
      "    \"cookbooks\": [\n",
      "      {\n",
      "        \"id\": \"test-cookbook\",\n",
      "        \"recipes\": [\n",
      "          {\n",
      "            \"id\": \"fruit-questions\",\n",
      "            \"details\": [\n",
      "              {\n",
      "                \"model_id\": \"my-openai-endpoint\",\n",
      "                \"dataset_id\": \"test-dataset\",\n",
      "                \"prompt_template_id\": \"no-template\",\n",
      "                \"data\": [\n",
      "                  {\n",
      "                    \"prompt\": \"Is Apple a Fruit? Answer Yes or No.\",\n",
      "                    \"predicted_result\": \"Yes.\",\n",
      "                    \"target\": \"Yes.\",\n",
      "                    \"duration\": 0.6278555419994518\n",
      "                  },\n",
      "                  {\n",
      "                    \"prompt\": \"Is Bak Choy a Fruit? Answer Yes or No.\",\n",
      "                    \"predicted_result\": \"No.\",\n",
      "                    \"target\": \"No.\",\n",
      "                    \"duration\": 0.6768413749960018\n",
      "                  },\n",
      "                  {\n",
      "                    \"prompt\": \"Is Dragonfruit a Fruit? Answer Yes or No.\",\n",
      "                    \"predicted_result\": \"Yes\",\n",
      "                    \"target\": \"Yes.\",\n",
      "                    \"duration\": 0.5786523329952615\n",
      "                  },\n",
      "                  {\n",
      "                    \"prompt\": \"Is Coke Zero a Fruit? Answer Yes or No.\",\n",
      "                    \"predicted_result\": \"No.\",\n",
      "                    \"target\": \"No.\",\n",
      "                    \"duration\": 0.610678832999838\n",
      "                  }\n",
      "                ],\n",
      "                \"metrics\": [\n",
      "                  {\n",
      "                    \"accuracy\": 75.0,\n",
      "                    \"grading_criteria\": {\n",
      "                      \"accuracy\": 75.0\n",
      "                    }\n",
      "                  }\n",
      "                ]\n",
      "              }\n",
      "            ],\n",
      "            \"evaluation_summary\": [\n",
      "              {\n",
      "                \"model_id\": \"my-openai-endpoint\",\n",
      "                \"num_of_prompts\": 4,\n",
      "                \"avg_grade_value\": 75.0,\n",
      "                \"grade\": \"B\"\n",
      "              }\n",
      "            ],\n",
      "            \"grading_scale\": {\n",
      "              \"A\": [\n",
      "                80,\n",
      "                100\n",
      "              ],\n",
      "              \"B\": [\n",
      "                60,\n",
      "                79\n",
      "              ],\n",
      "              \"C\": [\n",
      "                40,\n",
      "                59\n",
      "              ],\n",
      "              \"D\": [\n",
      "                20,\n",
      "                39\n",
      "              ],\n",
      "              \"E\": [\n",
      "                0,\n",
      "                19\n",
      "              ]\n",
      "            },\n",
      "            \"total_num_of_prompts\": 4\n",
      "          },\n",
      "          {\n",
      "            \"id\": \"mmlu\",\n",
      "            \"details\": [\n",
      "              {\n",
      "                \"model_id\": \"my-openai-endpoint\",\n",
      "                \"dataset_id\": \"mmlu-all\",\n",
      "                \"prompt_template_id\": \"mmlu\",\n",
      "                \"data\": [\n",
      "                  {\n",
      "                    \"prompt\": \"Question:\\nSeedy the watermelon was a very special type of watermelon. He didn't have any seeds. He was green and he had stripes. All of his cousins had seeds, but he didn't have any. He felt very left out. He couldn't thing of why he was different. His mom told him it was because he was a very special watermelon. She also tells him she loves him the way he is. But Seedy didn't think it was a good thing. He wished he could be like everyone else and have seeds. One day, he rolled out to the lawn and looked at all of his new cousins growing in the garden. He rolled around until he found a little baby watermelon that didn't have any seeds either. He sat next to him and talked to him. He told him that he was very special, and was excited for him to be picked off the vine and be his special best friend. He would name him Seedy, Jr. Why was Seedy upset he didn't have any seeds?\\nA. His mom said she loved him.\\nB. He was lonely.\\nC. He felt left out and didn't know why he was different.\\nD. He was green and had stripes.\\n\\nSelect the most appropriate answer or options to fill in the blank (if any): \",\n",
      "                    \"predicted_result\": \"C. He felt left out and didn't know why he was different.\",\n",
      "                    \"target\": \"C. He felt left out and didn't know why he was different.\",\n",
      "                    \"duration\": 0.8354969169959077\n",
      "                  },\n",
      "                  {\n",
      "                    \"prompt\": \"Question:\\nWhich of the following describes the fallacy of appeal to popularity?\\nA. saying someone should do something because he or she dislikes someone else\\nB. saying someone should do something to be better liked by others\\nC. saying someone should do something because it will make him or her feel good\\nD. saying someone should accept an idea because of the source of the idea\\n\\nSelect the most appropriate answer or options to fill in the blank (if any): \",\n",
      "                    \"predicted_result\": \"B. saying someone should do something to be better liked by others\",\n",
      "                    \"target\": \"B. saying someone should do something to be better liked by others\",\n",
      "                    \"duration\": 0.7261419169954024\n",
      "                  },\n",
      "                  {\n",
      "                    \"prompt\": \"Question:\\nA fortune teller told fortunes by means of Tarot cards. An elderly woman, who was worried about her failing health, had heard that the fortuneteller was clairvoyant and could see into the future. Consequently, the woman decided to see the fortuneteller in order to have her Tarot cards read. As the fortuneteller was telling the woman her fortune, she suddenly said, \\\"I have a vision. If you give me $25,000 tomorrow, you will live to be 100 years old. \\\" The woman, who was 72 years of age, believed the fortuneteller and gave her the money the next day. The following week, the woman's physician informed her that she had a serious heart disease and he didn't expect her to live for more than a year or two. If the woman asserts a claim against the fortuneteller based on deceit, the plaintiff should\\nA. prevail, because she relied to her detriment on the fortune teller's foretelling.\\nB. prevail, if the fortuneteller did not honestly believe that the woman would live to be 100 years of age.\\nC. not prevail, unless there was a fiduciary relationship between the parties.\\nD. not prevail, unless the fortuneteller warranted the truth as believed.\\n\\nSelect the most appropriate answer or options to fill in the blank (if any): \",\n",
      "                    \"predicted_result\": \"A. prevail, because she relied to her detriment on the fortune teller's foretelling.\",\n",
      "                    \"target\": \"B. prevail, if the fortuneteller did not honestly believe that the woman would live to be 100 years of age.\",\n",
      "                    \"duration\": 0.8939087500039022\n",
      "                  },\n",
      "                  {\n",
      "                    \"prompt\": \"Question:\\nA woman entered a jewelry store and asked the store's owner if he had any bracelets with turquoise and mother-of-pearl inlay. The owner answered affirmatively and showed the woman two display trays of bracelets. As the woman was looking at a few of the pieces, the telephone in the store began to ring. The owner excused himself and walked to the rear of the store where he answered the telephone call. While the owner was speaking on the phone, the woman placed one of the bracelets in her pocketbook and walked a few feet toward the front door of the store. She was about to leave the store, without paying for the bracelet, when she suddenly noticed one of the other employees. Thinking that the employee had seen her place the bracelet in her pocketbook, the woman walked back to the counter and returned the bracelet to the display tray. In fact, the employee had seen the woman take the bracelet but decided not to say anything after she put it back. If the woman is subsequently prosecuted for larceny of the bracelet, she will most likely be found\\nA. guilty, because it is not relevant that she returned the bracelet to the display tray.\\nB. guilty, because the employee had actually seen her place the bracelet in her pocketbook.\\nC. not guilty, because she returned the bracelet to the display tray.\\nD. not guilty, because she didn't leave the store with the bracelet in her possession.\\n\\nSelect the most appropriate answer or options to fill in the blank (if any): \",\n",
      "                    \"predicted_result\": \"B. guilty, because the employee had actually seen her place the bracelet in her pocketbook.\",\n",
      "                    \"target\": \"A. guilty, because it is not relevant that she returned the bracelet to the display tray.\",\n",
      "                    \"duration\": 0.8522694999992382\n",
      "                  },\n",
      "                  {\n",
      "                    \"prompt\": \"Question:\\nThe buildup of military forces and the formation of a rigid military alliance system were major causes of which of the following wars?\\nA. The Spanish-American War\\nB. The First World War\\nC. The Second World War\\nD. The Vietnam War\\n\\nSelect the most appropriate answer or options to fill in the blank (if any): \",\n",
      "                    \"predicted_result\": \"B. The First World War\",\n",
      "                    \"target\": \"B. The First World War\",\n",
      "                    \"duration\": 0.6516778340010205\n",
      "                  }\n",
      "                ],\n",
      "                \"metrics\": [\n",
      "                  {\n",
      "                    \"accuracy\": 60.0,\n",
      "                    \"grading_criteria\": {\n",
      "                      \"accuracy\": 60.0\n",
      "                    }\n",
      "                  }\n",
      "                ]\n",
      "              }\n",
      "            ],\n",
      "            \"evaluation_summary\": [\n",
      "              {\n",
      "                \"model_id\": \"my-openai-endpoint\",\n",
      "                \"num_of_prompts\": 5,\n",
      "                \"avg_grade_value\": 60.0,\n",
      "                \"grade\": \"B\"\n",
      "              }\n",
      "            ],\n",
      "            \"grading_scale\": {\n",
      "              \"A\": [\n",
      "                80,\n",
      "                100\n",
      "              ],\n",
      "              \"B\": [\n",
      "                60,\n",
      "                79\n",
      "              ],\n",
      "              \"C\": [\n",
      "                40,\n",
      "                59\n",
      "              ],\n",
      "              \"D\": [\n",
      "                20,\n",
      "                39\n",
      "              ],\n",
      "              \"E\": [\n",
      "                0,\n",
      "                19\n",
      "              ]\n",
      "            },\n",
      "            \"total_num_of_prompts\": 5\n",
      "          }\n",
      "        ],\n",
      "        \"overall_evaluation_summary\": [\n",
      "          {\n",
      "            \"model_id\": \"my-openai-endpoint\",\n",
      "            \"overall_grade\": \"B\"\n",
      "          }\n",
      "        ],\n",
      "        \"total_num_of_prompts\": 9\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from slugify import slugify\n",
    "from moonshot.api import api_get_all_run, api_create_runner, api_get_all_runner_name\n",
    "\n",
    "name = \"test new cookbook\" # Indicate the name\n",
    "cookbooks = [\"test-cookbook\"] # Test against 2 cookbooks, test-category-cookbook and common-risk-easy\n",
    "endpoints = [\"my-openai-endpoint\"] # Test against 1 endpoint, test-openai-endpoint\n",
    "num_of_prompts = 5 # use a smaller number to test out the function; 0 means using all prompts in dataset\n",
    "\n",
    "# Below are the optional fields\n",
    "random_seed = 0   # Default: 0; this allows for randomness in dataset selection when num_of_prompts are set\n",
    "system_prompt = \"\"  # Default: \"\"; this allows setting the system prompt for the endpoints\n",
    "\n",
    "# Advanced user - Modify runner processing module and result processing module\n",
    "# Default: benchmarking and benchmarking-result\n",
    "runner_proc_module = \"benchmarking\"  # Default: \"benchmarking\"\n",
    "result_proc_module = \"benchmarking-result\"  # Default: \"benchmarking-result\"\n",
    "\n",
    "# Run the cookbooks with the defined endpoints\n",
    "# If the id exists, it will perform a load on the runner, instead of creating a new runner.\n",
    "# The benefit of this, allows the new run to use possible cached results from previous runs which greatly enhances the run time.\n",
    "slugify_id = slugify(name, lowercase=True)\n",
    "if slugify_id in api_get_all_runner_name():\n",
    "    cb_runner = api_load_runner(slugify_id)\n",
    "else:\n",
    "    cb_runner = api_create_runner(name, endpoints)\n",
    "\n",
    "# run_cookbooks is an async function. Currently there is no sync version.\n",
    "# We will get an existing event loop and execute the run cookbooks process.\n",
    "await cb_runner.run_cookbooks(\n",
    "        cookbooks,\n",
    "        num_of_prompts,\n",
    "        random_seed,\n",
    "        system_prompt,\n",
    "        runner_proc_module,\n",
    "        result_proc_module,\n",
    "    )\n",
    "cb_runner.close()  # Perform a close on the runner to allow proper cleanup.\n",
    "\n",
    "# Display results\n",
    "runner_runs = api_get_all_run(cb_runner.id)\n",
    "result_info = runner_runs[-1].get(\"results\")\n",
    "if result_info:\n",
    "    print(json.dumps(result_info, indent=2))\n",
    "else:\n",
    "    raise RuntimeError(\"no run result generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf2ab13-bf8c-401b-a0f5-b59339fe0749",
   "metadata": {},
   "source": [
    "## Beautifying Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3adebeba-18b1-4fd6-8e95-b705d24f769d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                                  Cookbook Result                                                  </span>\n",
       "┏━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> No. </span>┃<span style=\"font-weight: bold\"> Cookbook (with its recipes)                                                         </span>┃<span style=\"font-weight: bold\"> my-openai-endpoint  </span>┃\n",
       "┡━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ 1   │ Cookbook: <span style=\"color: #000080; text-decoration-color: #000080\">test-cookbook</span>                                                             │          B          │\n",
       "├─────┼─────────────────────────────────────────────────────────────────────────────────────┼─────────────────────┤\n",
       "│     │   └──  Recipe: <span style=\"color: #000080; text-decoration-color: #000080\">fruit-questions</span>                                                      │      B [75.0]       │\n",
       "├─────┼─────────────────────────────────────────────────────────────────────────────────────┼─────────────────────┤\n",
       "│     │   └──  Recipe: <span style=\"color: #000080; text-decoration-color: #000080\">mmlu</span>                                                                 │      B [60.0]       │\n",
       "└─────┴─────────────────────────────────────────────────────────────────────────────────────┴─────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                                  Cookbook Result                                                  \u001b[0m\n",
       "┏━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mNo.\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mCookbook (with its recipes)                                                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mmy-openai-endpoint \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ 1   │ Cookbook: \u001b[34mtest-cookbook\u001b[0m                                                             │          B          │\n",
       "├─────┼─────────────────────────────────────────────────────────────────────────────────────┼─────────────────────┤\n",
       "│     │   └──  Recipe: \u001b[34mfruit-questions\u001b[0m                                                      │      B [75.0]       │\n",
       "├─────┼─────────────────────────────────────────────────────────────────────────────────────┼─────────────────────┤\n",
       "│     │   └──  Recipe: \u001b[34mmmlu\u001b[0m                                                                 │      B [60.0]       │\n",
       "└─────┴─────────────────────────────────────────────────────────────────────────────────────┴─────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">==================================================\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">Time taken to run: 0s</span>\n",
       "*Overall rating will be the lowest grade that the recipes have in each cookbook\n",
       "==================================================\n",
       "</pre>\n"
      ],
      "text/plain": [
       "==================================================\n",
       "\u001b[34mTime taken to run: 0s\u001b[0m\n",
       "*Overall rating will be the lowest grade that the recipes have in each cookbook\n",
       "==================================================\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich.columns import Columns\n",
    "from rich.console import Console\n",
    "from rich.panel import Panel\n",
    "from rich.table import Table\n",
    "console = Console()\n",
    "\n",
    "def show_cookbook_results(cookbooks, endpoints, cookbook_results, duration):\n",
    "    \"\"\"\n",
    "    Show the results of the cookbook benchmarking.\n",
    "\n",
    "    This function takes the cookbooks, endpoints, cookbook results, results file, and duration as arguments.\n",
    "    If there are results, it generates a table with the cookbook results and prints a message indicating\n",
    "    where the results are saved. If there are no results, it prints a message indicating that no results were found.\n",
    "    Finally, it prints the duration of the run.\n",
    "\n",
    "    Args:\n",
    "        cookbooks (list): A list of cookbooks.\n",
    "        endpoints (list): A list of endpoints.\n",
    "        cookbook_results (dict): A dictionary with the results of the cookbook benchmarking.\n",
    "        duration (float): The duration of the run.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if cookbook_results:\n",
    "        # Display recipe results\n",
    "        generate_cookbook_table(cookbooks, endpoints, cookbook_results)\n",
    "    else:\n",
    "        console.print(\"[red]There are no results.[/red]\")\n",
    "\n",
    "    # Print run stats\n",
    "    console.print(f\"{'='*50}\\n[blue]Time taken to run: {duration}s[/blue]\\n*Overall rating will be the lowest grade that the recipes have in each cookbook\\n{'='*50}\")\n",
    "\n",
    "def generate_cookbook_table(cookbooks: list, endpoints: list, results: dict) -> None:\n",
    "    \"\"\"\n",
    "    Generate and display a table with the cookbook benchmarking results.\n",
    "\n",
    "    This function creates a table that includes the index, cookbook name, recipe name, and the results\n",
    "    for each endpoint.\n",
    "\n",
    "    The cookbook names are prefixed with \"Cookbook:\" and are displayed with their overall grades. Each recipe under a\n",
    "    cookbook is indented and prefixed with \"Recipe:\" followed by its individual grades for each endpoint. If there are\n",
    "    no results for a cookbook, a row with dashes across all endpoint columns is added to indicate this.\n",
    "\n",
    "    Args:\n",
    "        cookbooks (list): A list of cookbook names to display in the table.\n",
    "        endpoints (list): A list of endpoints for which results are to be displayed.\n",
    "        results (dict): A dictionary containing the benchmarking results for cookbooks and recipes.\n",
    "\n",
    "    Returns:\n",
    "        None: The function prints the table to the console but does not return any value.\n",
    "    \"\"\"\n",
    "    table = Table(\n",
    "        title=\"Cookbook Result\", show_lines=True, expand=True, header_style=\"bold\"\n",
    "    )\n",
    "    table.add_column(\"No.\", width=2)\n",
    "    table.add_column(\"Cookbook (with its recipes)\", justify=\"left\", width=78)\n",
    "    for endpoint in endpoints:\n",
    "        table.add_column(endpoint, justify=\"center\")\n",
    "\n",
    "    index = 1\n",
    "    for cookbook in cookbooks:\n",
    "        # Get cookbook result\n",
    "        cookbook_result = next(\n",
    "            (\n",
    "                result\n",
    "                for result in results[\"results\"][\"cookbooks\"]\n",
    "                if result[\"id\"] == cookbook\n",
    "            ),\n",
    "            None,\n",
    "        )\n",
    "\n",
    "        if cookbook_result:\n",
    "            # Add the cookbook name with the \"Cookbook: \" prefix as the first row for this section\n",
    "            endpoint_results = []\n",
    "            for endpoint in endpoints:\n",
    "                # Find the evaluation summary for the endpoint\n",
    "                evaluation_summary = next(\n",
    "                    (\n",
    "                        temp_eval\n",
    "                        for temp_eval in cookbook_result[\"overall_evaluation_summary\"]\n",
    "                        if temp_eval[\"model_id\"] == endpoint\n",
    "                    ),\n",
    "                    None,\n",
    "                )\n",
    "\n",
    "                # Get the grade from the evaluation_summary, or use \"-\" if not found\n",
    "                grade = \"-\"\n",
    "                if evaluation_summary and evaluation_summary[\"overall_grade\"]:\n",
    "                    grade = evaluation_summary[\"overall_grade\"]\n",
    "                endpoint_results.append(grade)\n",
    "            table.add_row(\n",
    "                str(index),\n",
    "                f\"Cookbook: [blue]{cookbook}[/blue]\",\n",
    "                *endpoint_results,\n",
    "                end_section=True,\n",
    "            )\n",
    "\n",
    "            for recipe in cookbook_result[\"recipes\"]:\n",
    "                endpoint_results = []\n",
    "                for endpoint in endpoints:\n",
    "                    # Find the evaluation summary for the endpoint\n",
    "                    evaluation_summary = next(\n",
    "                        (\n",
    "                            temp_eval\n",
    "                            for temp_eval in recipe[\"evaluation_summary\"]\n",
    "                            if temp_eval[\"model_id\"] == endpoint\n",
    "                        ),\n",
    "                        None,\n",
    "                    )\n",
    "\n",
    "                    # Get the grade from the evaluation_summary, or use \"-\" if not found\n",
    "                    grade = \"-\"\n",
    "                    if (\n",
    "                        evaluation_summary\n",
    "                        and \"grade\" in evaluation_summary\n",
    "                        and \"avg_grade_value\" in evaluation_summary\n",
    "                        and evaluation_summary[\"grade\"]\n",
    "                    ):\n",
    "                        grade = f\"{evaluation_summary['grade']} [{evaluation_summary['avg_grade_value']}]\"\n",
    "                    endpoint_results.append(grade)\n",
    "\n",
    "                # Add the recipe name indented under the cookbook name\n",
    "                table.add_row(\n",
    "                    \"\",\n",
    "                    f\"  └──  Recipe: [blue]{recipe['id']}[/blue]\",\n",
    "                    *endpoint_results,\n",
    "                    end_section=True,\n",
    "                )\n",
    "\n",
    "            # Increment index only after all recipes of the cookbook have been added\n",
    "            index += 1\n",
    "        else:\n",
    "            # If no results for the cookbook, add a row indicating this with the \"Cookbook: \" prefix\n",
    "            # and a dash for each endpoint column\n",
    "            table.add_row(\n",
    "                str(index),\n",
    "                f\"Cookbook: {cookbook}\",\n",
    "                *([\"-\"] * len(endpoints)),\n",
    "                end_section=True,\n",
    "            )\n",
    "            index += 1\n",
    "\n",
    "    # Display table\n",
    "    console.print(table)\n",
    "\n",
    "if result_info:\n",
    "    show_cookbook_results(\n",
    "        cookbooks, endpoints, result_info, result_info[\"metadata\"][\"duration\"]\n",
    "    )\n",
    "else:\n",
    "    raise RuntimeError(\"no run result generated\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
