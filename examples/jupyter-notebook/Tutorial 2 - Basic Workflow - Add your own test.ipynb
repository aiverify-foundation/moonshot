{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdc5b948-4919-47fe-9598-0d4c1bfaf8ea",
   "metadata": {},
   "source": [
    "# Tutorial 2 - Basic Workflow - Add Your Own Tests \n",
    "\n",
    "**Scenario**: You are a model developer and you want to evaluate your custom chatbot in your CI/CD pipeline as your team continuously train and improve the system. In this case, you have already identified a dataset that does not exist in Moonshot to benchmark your model's performance. How can you add this new dataset into Moonshot and run it with your system?\n",
    "\n",
    "In this tutorial, you will learn how to:\n",
    "\n",
    "- Add your own `dataset` into Moonshot\n",
    "- Create and run your own `recipe`\n",
    "- Create and run your own `cookbook`\n",
    "\n",
    "**Prerequisite**:\n",
    "\n",
    "1. You have added your OpenAI connector configuration named `my-openai-endpoint` in Moonshot. If you are unsure how to do it, please refer to \"Tutorial 1\" in the same folder.\n",
    "\n",
    "**Before starting this tutorial, please make sure you have already installed `moonshot` and `moonshot-data`.** Otherwise, please follow this tutorial to install and configure Moonshot first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb78f9e-6842-4524-add7-7434687cc7cc",
   "metadata": {},
   "source": [
    "## Import Moonshot Library API\n",
    "\n",
    "In this section, we prepare our Jupyter notebook environment by importing necessary libraries required to execute an existing benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbde151f-34fb-4e52-88cc-62c87834cea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moonshot Framework API Imports\n",
    "# These imports from the Moonshot framework allow us to interact with the API, \n",
    "# creating and managing various components such as recipes, cookbooks, and endpoints.\n",
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import sys\n",
    "\n",
    "# Ensure that the root of the Moonshot framework is in the system path for module importing.\n",
    "sys.path.insert(0, '../../')\n",
    "\n",
    "from moonshot.api import (\n",
    "    api_get_all_recipe,\n",
    "    api_create_recipe,\n",
    "    api_create_cookbook,\n",
    "    api_get_all_runner,\n",
    "    api_load_runner,\n",
    "    api_read_result,\n",
    "    api_set_environment_variables\n",
    ")\n",
    "\n",
    "# modify moonshot_path to point to your own copy of moonshot-data\n",
    "moonshot_path = \"./data/\"\n",
    "env = {\n",
    "    \"ATTACK_MODULES\": os.path.join(moonshot_path, \"attack-modules\"),\n",
    "    \"BOOKMARKS\": os.path.join(moonshot_path, \"generated-outputs/bookmarks\"),\n",
    "    \"CONNECTORS\": os.path.join(moonshot_path, \"connectors\"),\n",
    "    \"CONNECTORS_ENDPOINTS\": os.path.join(moonshot_path, \"connectors-endpoints\"),\n",
    "    \"CONTEXT_STRATEGY\": os.path.join(moonshot_path, \"context-strategy\"),\n",
    "    \"COOKBOOKS\": os.path.join(moonshot_path, \"cookbooks\"),\n",
    "    \"DATABASES\": os.path.join(moonshot_path, \"generated-outputs/databases\"),\n",
    "    \"DATABASES_MODULES\": os.path.join(moonshot_path, \"databases-modules\"),\n",
    "    \"DATASETS\": os.path.join(moonshot_path, \"datasets\"),\n",
    "    \"IO_MODULES\": os.path.join(moonshot_path, \"io-modules\"),\n",
    "    \"METRICS\": os.path.join(moonshot_path, \"metrics\"),\n",
    "    \"PROMPT_TEMPLATES\": os.path.join(moonshot_path, \"prompt-templates\"),\n",
    "    \"RECIPES\": os.path.join(moonshot_path, \"recipes\"),\n",
    "    \"RESULTS\": os.path.join(moonshot_path, \"generated-outputs/results\"),\n",
    "    \"RESULTS_MODULES\": os.path.join(moonshot_path, \"results-modules\"),\n",
    "    \"RUNNERS\": os.path.join(moonshot_path, \"generated-outputs/runners\"),\n",
    "    \"RUNNERS_MODULES\": os.path.join(moonshot_path, \"runners-modules\"),\n",
    "}\n",
    "\n",
    "# Apply the environment variables to configure the Moonshot framework.\n",
    "api_set_environment_variables(env)\n",
    "\n",
    "# Note: there will be no printout if the environment variables are set successfully"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38df293d-0758-45cd-9181-cfc5ea39d7ff",
   "metadata": {},
   "source": [
    "## Prepare the Dataset\n",
    "\n",
    "In this section, we show how to prepare Moonshot dataset. Supposed you have a list of \"fruits\" questions to ask your chatbot, you need to prepare them into the data schema that is compatible with Moonshot.\n",
    "\n",
    "- `name` (str): name of the data\n",
    "- `description` (str): description of the dataset\n",
    "- `license` (str): license of the data\n",
    "- `reference` (str): a link/reference to where the dataset is from (or author of the dataset)\n",
    "- `examples` (list): A list of dictionary containing the prompt (`input`) and ground truth (`target`). A `target` can be left blank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2c801e9-4050-4dbb-ba42-22e31d5458d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'test-dataset' has been created.\n"
     ]
    }
   ],
   "source": [
    "test_dataset = {\n",
    "    \"name\": \"Fruits Dataset\",\n",
    "    \"description\":\"Measures whether the model knows what is a fruit\",\n",
    "    \"license\": \"MIT license\",\n",
    "    \"reference\": \"\",\n",
    "    \"examples\": [\n",
    "        {\n",
    "            \"input\": \"Is Lemon a Fruit? Answer Yes or No.\",\n",
    "            \"target\": \"Yes.\"\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"Is Apple a Fruit? Answer Yes or No.\",\n",
    "            \"target\": \"Yes.\"\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"Is Bak Choy a Fruit? Answer Yes or No.\",\n",
    "            \"target\": \"No.\"\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"Is Bak Kwa a Fruit? Answer Yes or No.\",\n",
    "            \"target\": \"No.\"\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"Is Dragonfruit a Fruit? Answer Yes or No.\",\n",
    "            \"target\": \"Yes.\"\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"Is Orange a Fruit? Answer Yes or No.\",\n",
    "            \"target\": \"Yes.\"\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"Is Coke Zero a Fruit? Answer Yes or No.\",\n",
    "            \"target\": \"No.\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "in_file = f\"{moonshot_path}/datasets/test-dataset.json\"\n",
    "json.dump(test_dataset, open(in_file, \"w+\"), indent=2)\n",
    "if os.path.exists(in_file):\n",
    "     print(f\"Dataset 'test-dataset' has been created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7acdff5-c59c-4179-badb-ab68560793a6",
   "metadata": {},
   "source": [
    "## Create a new `recipe`\n",
    "\n",
    "To run this dataset, you need to create a new `recipe`. A `recipe` contains all the details required to run a benchmark. A `recipe` guides Moonshot on what data to use, and how to evaluate the model's responses.\n",
    "\n",
    "To create a new recipe, you need the following elements:\n",
    "\n",
    "1. **Name**: A unique name for the recipe.\n",
    "2. **Description**: An explanation of what the recipe does and what it's for.\n",
    "3. **Tags**: Keywords that categorise the recipe, making it easier to find and group with similar recipes.\n",
    "4. **Categories**: Broader classifications that help organise recipes into collections.\n",
    "5. **Datasets**: The data that will be used when running the recipe. This could be a set of prompts, questions, or any input that the model will respond to.\n",
    "6. **Prompt Templates**: Pre-prompt or post-prompt static text that will be appended to the prompt.\n",
    "7. **Metrics**: Criteria or measurements used to evaluate the model's responses, such as accuracy, fluency, or adherence to a prompt.\n",
    "8. **Grading Scale**: A set of thresholds or criteria used to grade or score the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1e855bb-1322-4cc5-b775-ddd0dcbdcfcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipe 'fruit-questions' has been created.\n"
     ]
    }
   ],
   "source": [
    "test_recipe = api_create_recipe(\n",
    "    \"Fruit Questions\", # name (mandatory)\n",
    "    \"This recipe is created to test model's ability in answering fruits question.\", # description (mandatory)\n",
    "    [\"chatbot\"], # tags (optional)\n",
    "    [\"capability\"], # category (optional)\n",
    "    [\"test-dataset\"], # filename of the dataset (mandatory)\n",
    "    [], # prompt templates (optional)\n",
    "    [\"exactstrmatch\"], # metrics (mandatory)\n",
    "    { # grading scale (optional)\n",
    "        \"A\": [\n",
    "            80,\n",
    "            100\n",
    "        ],\n",
    "        \"B\": [\n",
    "            60,\n",
    "            79\n",
    "        ],\n",
    "        \"C\": [\n",
    "            40,\n",
    "            59\n",
    "        ],\n",
    "        \"D\": [\n",
    "            20,\n",
    "            39\n",
    "        ],\n",
    "        \"E\": [\n",
    "            0,\n",
    "            19\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Recipe '{test_recipe}' has been created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4e7fdb-6de1-4f3a-ac71-9605256f7a29",
   "metadata": {},
   "source": [
    "## Run your new recipe\n",
    "\n",
    "With this new recipe, you can run this on your `connector endpoint`. We will run this on endpoint `my-openai-endpoint`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86c6164b-fc57-457a-a640-f11c451fcdd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-08 18:51:52,102 [INFO][runner.py::run_recipes(354)] [Runner] my-new-recipe-runner - Running benchmark recipe run...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-08 18:51:52,346 [INFO][benchmarking.py::generate(156)] [Benchmarking] Running recipes (['fruit-questions'])...\n",
      "2024-11-08 18:51:52,347 [INFO][benchmarking.py::generate(160)] [Benchmarking] Running recipe fruit-questions... (1/1)\n",
      "2024-11-08 18:51:52,352 [INFO][connector.py::get_prediction(348)] [Connector ID: my-openai-endpoint] Predicting Prompt Index 0.\n",
      "2024-11-08 18:51:52,356 [INFO][connector.py::get_prediction(348)] [Connector ID: my-openai-endpoint] Predicting Prompt Index 1.\n",
      "2024-11-08 18:51:52,356 [INFO][connector.py::get_prediction(348)] [Connector ID: my-openai-endpoint] Predicting Prompt Index 3.\n",
      "2024-11-08 18:51:52,357 [INFO][connector.py::get_prediction(348)] [Connector ID: my-openai-endpoint] Predicting Prompt Index 5.\n",
      "2024-11-08 18:51:52,357 [INFO][connector.py::get_prediction(348)] [Connector ID: my-openai-endpoint] Predicting Prompt Index 6.\n",
      "2024-11-08 18:51:56,874 [INFO][benchmarking.py::generate(190)] [Benchmarking] Run took 4.5275s\n",
      "2024-11-08 18:51:56,875 [INFO][benchmarking.py::generate(245)] [Benchmarking] Preparing results took 0.0000s\n",
      "2024-11-08 18:51:56,879 [INFO][benchmarking-result.py::generate(58)] [BenchmarkingResult] Generate results took 0.0031s\n",
      "2024-11-08 18:51:56,880 [INFO][runner.py::run_recipes(380)] [Runner] my-new-recipe-runner - Benchmark recipe run completed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"metadata\": {\n",
      "    \"id\": \"my-new-recipe-runner\",\n",
      "    \"start_time\": \"2024-11-08 18:51:52\",\n",
      "    \"end_time\": \"2024-11-08 18:51:56\",\n",
      "    \"duration\": 4,\n",
      "    \"status\": \"completed\",\n",
      "    \"recipes\": [\n",
      "      \"fruit-questions\"\n",
      "    ],\n",
      "    \"cookbooks\": null,\n",
      "    \"endpoints\": [\n",
      "      \"my-openai-endpoint\"\n",
      "    ],\n",
      "    \"num_of_prompts\": 5,\n",
      "    \"random_seed\": 0,\n",
      "    \"system_prompt\": \"\"\n",
      "  },\n",
      "  \"results\": {\n",
      "    \"recipes\": [\n",
      "      {\n",
      "        \"id\": \"fruit-questions\",\n",
      "        \"details\": [\n",
      "          {\n",
      "            \"model_id\": \"my-openai-endpoint\",\n",
      "            \"dataset_id\": \"test-dataset\",\n",
      "            \"prompt_template_id\": \"no-template\",\n",
      "            \"data\": [\n",
      "              {\n",
      "                \"prompt\": \"Is Lemon a Fruit? Answer Yes or No.\",\n",
      "                \"predicted_result\": {\n",
      "                  \"response\": \"Yes\",\n",
      "                  \"context\": []\n",
      "                },\n",
      "                \"target\": \"Yes.\",\n",
      "                \"duration\": 0.6383928749710321\n",
      "              },\n",
      "              {\n",
      "                \"prompt\": \"Is Apple a Fruit? Answer Yes or No.\",\n",
      "                \"predicted_result\": {\n",
      "                  \"response\": \"Yes\",\n",
      "                  \"context\": []\n",
      "                },\n",
      "                \"target\": \"Yes.\",\n",
      "                \"duration\": 1.4445381249533966\n",
      "              },\n",
      "              {\n",
      "                \"prompt\": \"Is Bak Kwa a Fruit? Answer Yes or No.\",\n",
      "                \"predicted_result\": {\n",
      "                  \"response\": \"No.\",\n",
      "                  \"context\": []\n",
      "                },\n",
      "                \"target\": \"No.\",\n",
      "                \"duration\": 2.4665249999961816\n",
      "              },\n",
      "              {\n",
      "                \"prompt\": \"Is Orange a Fruit? Answer Yes or No.\",\n",
      "                \"predicted_result\": {\n",
      "                  \"response\": \"Yes\",\n",
      "                  \"context\": []\n",
      "                },\n",
      "                \"target\": \"Yes.\",\n",
      "                \"duration\": 3.389552000036929\n",
      "              },\n",
      "              {\n",
      "                \"prompt\": \"Is Coke Zero a Fruit? Answer Yes or No.\",\n",
      "                \"predicted_result\": {\n",
      "                  \"response\": \"No.\",\n",
      "                  \"context\": []\n",
      "                },\n",
      "                \"target\": \"No.\",\n",
      "                \"duration\": 4.511920167016797\n",
      "              }\n",
      "            ],\n",
      "            \"metrics\": [\n",
      "              {\n",
      "                \"accuracy\": 40.0,\n",
      "                \"grading_criteria\": {\n",
      "                  \"accuracy\": 40.0\n",
      "                }\n",
      "              }\n",
      "            ]\n",
      "          }\n",
      "        ],\n",
      "        \"evaluation_summary\": [\n",
      "          {\n",
      "            \"model_id\": \"my-openai-endpoint\",\n",
      "            \"num_of_prompts\": 5,\n",
      "            \"avg_grade_value\": 40.0,\n",
      "            \"grade\": \"C\"\n",
      "          }\n",
      "        ],\n",
      "        \"grading_scale\": {\n",
      "          \"A\": [\n",
      "            80,\n",
      "            100\n",
      "          ],\n",
      "          \"B\": [\n",
      "            60,\n",
      "            79\n",
      "          ],\n",
      "          \"C\": [\n",
      "            40,\n",
      "            59\n",
      "          ],\n",
      "          \"D\": [\n",
      "            20,\n",
      "            39\n",
      "          ],\n",
      "          \"E\": [\n",
      "            0,\n",
      "            19\n",
      "          ]\n",
      "        },\n",
      "        \"total_num_of_prompts\": 5\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from slugify import slugify\n",
    "from moonshot.api import api_get_all_run, api_create_runner, api_get_all_runner_name\n",
    "\n",
    "name = \"my new recipe runner\" # Indicate the name\n",
    "recipes = [\"fruit-questions\"] # Test one recipe fruit-questions. You can add more recipes in the list to test as well\n",
    "endpoints = [\"my-openai-endpoint\"]  #Test against 1 endpoint, my-openai-endpoint\n",
    "num_of_prompts = 5 # The number of prompt(s) to run from EACH dataset in the cookbook; 0 means using all prompts in dataset\n",
    "\n",
    "# Below are the optional fields\n",
    "random_seed = 0   # Default: 0; this allows for randomness in dataset selection when num_of_prompts are set\n",
    "system_prompt = \"\"  # Default: \"\"; this allows setting the system prompt for the endpoints\n",
    "\n",
    "# Advanced user - Modify runner processing module and result processing module\n",
    "# Default: benchmarking and benchmarking-result\n",
    "runner_proc_module = \"benchmarking\"  # Default: \"benchmarking\"\n",
    "result_proc_module = \"benchmarking-result\"  # Default: \"benchmarking-result\"\n",
    "\n",
    "# Run the recipe with the defined endpoint(s)\n",
    "# If the id exists, it will perform a load on the runner, instead of creating a new runner.\n",
    "# Using an existing runner allows the new run to possibly use cached results from previous runs, which greatly reduces the run time\n",
    "slugify_id = slugify(name, lowercase=True)\n",
    "if slugify_id in api_get_all_runner_name():\n",
    "    rec_runner = api_load_runner(slugify_id)\n",
    "else:\n",
    "    rec_runner = api_create_runner(name, endpoints)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "# run_cookbooks is an async function. Currently there is no sync version.\n",
    "# We will get an existing event loop and execute the run cookbooks process.\n",
    "await rec_runner.run_recipes(\n",
    "    recipes,\n",
    "    num_of_prompts,\n",
    "    random_seed,\n",
    "    system_prompt,\n",
    "    runner_proc_module,\n",
    "    result_proc_module,\n",
    ")\n",
    "await rec_runner.close()  # Perform a close on the runner to allow proper cleanup.\n",
    "\n",
    "# Display results\n",
    "runner_runs = api_get_all_run(rec_runner.id)\n",
    "result_info = runner_runs[-1].get(\"results\")\n",
    "if result_info:\n",
    "    print(json.dumps(result_info, indent=2))\n",
    "else:\n",
    "    raise RuntimeError(\"no run result generated\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90595255-2237-40d7-a06b-90d526322a00",
   "metadata": {},
   "source": [
    "## Beautifying Test Results\n",
    "\n",
    "The result above is shown in our raw JSON file. To beautify the results, we have provided these helper functions to them into a nice table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2ca3dd8-6b5a-479f-85b1-4e74f4482889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                                  Recipes Result                                                   </span>\n",
       "┏━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> No. </span>┃<span style=\"font-weight: bold\"> Recipe                                                                              </span>┃<span style=\"font-weight: bold\"> my-openai-endpoint  </span>┃\n",
       "┡━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ 1   │ Recipe: <span style=\"color: #000080; text-decoration-color: #000080\">fruit-questions</span>                                                             │      C [40.0]       │\n",
       "└─────┴─────────────────────────────────────────────────────────────────────────────────────┴─────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                                  Recipes Result                                                   \u001b[0m\n",
       "┏━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mNo.\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mRecipe                                                                             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mmy-openai-endpoint \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ 1   │ Recipe: \u001b[34mfruit-questions\u001b[0m                                                             │      C [40.0]       │\n",
       "└─────┴─────────────────────────────────────────────────────────────────────────────────────┴─────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">==================================================\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">Time taken to run: 4s</span>\n",
       "*Overall rating will be the lowest grade that the recipes have in each cookbook\n",
       "==================================================\n",
       "</pre>\n"
      ],
      "text/plain": [
       "==================================================\n",
       "\u001b[34mTime taken to run: 4s\u001b[0m\n",
       "*Overall rating will be the lowest grade that the recipes have in each cookbook\n",
       "==================================================\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich.columns import Columns\n",
    "from rich.console import Console\n",
    "from rich.panel import Panel\n",
    "from rich.table import Table\n",
    "console = Console()\n",
    "\n",
    "def show_recipe_results(recipes, endpoints, recipe_results, duration):\n",
    "    \"\"\"\n",
    "    Show the results of the recipe benchmarking.\n",
    "\n",
    "    This function takes the recipes, endpoints, recipe results, results file, and duration as arguments.\n",
    "    If there are any recipe results, it generates a table to display them using the generate_recipe_table function.\n",
    "    It also prints the location of the results file and the time taken to run the benchmarking.\n",
    "    If there are no recipe results, it prints a message indicating that there are no results.\n",
    "\n",
    "    Args:\n",
    "        recipes (list): A list of recipes that were benchmarked.\n",
    "        endpoints (list): A list of endpoints that were used in the benchmarking.\n",
    "        recipe_results (dict): A dictionary with the results of the recipe benchmarking.\n",
    "        duration (float): The time taken to run the benchmarking in seconds.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if recipe_results:\n",
    "        # Display recipe results\n",
    "        generate_recipe_table(recipes, endpoints, recipe_results)\n",
    "    else:\n",
    "        console.print(\"[red]There are no results.[/red]\")\n",
    "\n",
    "    # Print run stats\n",
    "    console.print(f\"{'='*50}\\n[blue]Time taken to run: {duration}s[/blue]\\n*Overall rating will be the lowest grade that the recipes have in each cookbook\\n{'='*50}\")\n",
    "\n",
    "def generate_recipe_table(recipes: list, endpoints: list, results: dict) -> None:\n",
    "    \"\"\"\n",
    "    Generate and display a table of recipe results.\n",
    "\n",
    "    This function creates a table that lists the results of running recipes against various endpoints.\n",
    "    Each row in the table corresponds to a recipe, and each column corresponds to an endpoint.\n",
    "    The results include the grade and average grade value for each recipe-endpoint pair.\n",
    "\n",
    "    Args:\n",
    "        recipes (list): A list of recipe IDs that were benchmarked.\n",
    "        endpoints (list): A list of endpoint IDs against which the recipes were run.\n",
    "        results (dict): A dictionary containing the results of the benchmarking.\n",
    "\n",
    "    Returns:\n",
    "        None: This function does not return anything. It prints the table to the console.\n",
    "    \"\"\"\n",
    "    # Create a table with a title and headers\n",
    "    table = Table(\n",
    "        title=\"Recipes Result\", show_lines=True, expand=True, header_style=\"bold\"\n",
    "    )\n",
    "    table.add_column(\"No.\", width=2)\n",
    "    table.add_column(\"Recipe\", justify=\"left\", width=78)\n",
    "    # Add a column for each endpoint\n",
    "    for endpoint in endpoints:\n",
    "        table.add_column(endpoint, justify=\"center\")\n",
    "\n",
    "    # Iterate over each recipe and populate the table with results\n",
    "    for index, recipe_id in enumerate(recipes, start=1):\n",
    "        # Attempt to find the result for the current recipe\n",
    "        recipe_result = next(\n",
    "            (\n",
    "                result\n",
    "                for result in results[\"results\"][\"recipes\"]\n",
    "                if result[\"id\"] == recipe_id\n",
    "            ),\n",
    "            None,\n",
    "        )\n",
    "\n",
    "        # If the result exists, extract and format the results for each endpoint\n",
    "        if recipe_result:\n",
    "            endpoint_results = []\n",
    "            for endpoint in endpoints:\n",
    "                # Find the evaluation summary for the endpoint\n",
    "                evaluation_summary = next(\n",
    "                    (\n",
    "                        eval_summary\n",
    "                        for eval_summary in recipe_result[\"evaluation_summary\"]\n",
    "                        if eval_summary[\"model_id\"] == endpoint\n",
    "                    ),\n",
    "                    None,\n",
    "                )\n",
    "\n",
    "                # Format the grade and average grade value, or use \"-\" if not found\n",
    "                grade = \"-\"\n",
    "                if (\n",
    "                    evaluation_summary\n",
    "                    and \"grade\" in evaluation_summary\n",
    "                    and \"avg_grade_value\" in evaluation_summary\n",
    "                    and evaluation_summary[\"grade\"]\n",
    "                ):\n",
    "                    grade = f\"{evaluation_summary['grade']} [{evaluation_summary['avg_grade_value']}]\"\n",
    "                endpoint_results.append(grade)\n",
    "\n",
    "            # Add a row for the recipe with its results\n",
    "            table.add_row(\n",
    "                str(index),\n",
    "                f\"Recipe: [blue]{recipe_result['id']}[/blue]\",\n",
    "                *endpoint_results,\n",
    "                end_section=True,\n",
    "            )\n",
    "        else:\n",
    "            # If no result is found, add a row with placeholders\n",
    "            table.add_row(\n",
    "                str(index),\n",
    "                f\"Recipe: [blue]{recipe_id}[/blue]\",\n",
    "                *([\"-\"] * len(endpoints)),\n",
    "                end_section=True,\n",
    "            )\n",
    "\n",
    "    # Print the table to the console\n",
    "    console.print(table)\n",
    "\n",
    "if result_info:\n",
    "    show_recipe_results(\n",
    "            recipes, endpoints, result_info, result_info[\"metadata\"][\"duration\"]\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a28bf55-0e7e-4c2e-b372-97294e0e34eb",
   "metadata": {},
   "source": [
    "## Create a new `cookbook`\n",
    "\n",
    "We can also create a new `cookbook` and add existing recipes together with our new recipe. A `cookbook` in Moonshot is a curated collection of `recipes` designed to be executed together.\n",
    "\n",
    "To create a new cookbook, you need the following fields:\n",
    "\n",
    "1. **Name**: A unique name for the cookbook.\n",
    "2. **Description**: A detailed explanation of the cookbook's purpose and the recipe(s) it contains.\n",
    "3. **Recipes**: A list of recipe(s) that are included in the cookbook. Each recipe represents a specific test or benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "288bb18b-a5ee-49b6-97dd-7d33c336c3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cookbook 'test-cookbook' has been created.\n"
     ]
    }
   ],
   "source": [
    "cookbook_id = api_create_cookbook(\n",
    "    \"test-cookbook\",\n",
    "    \"This cookbook tests both fruits questions and general science questions.\",\n",
    "    [\"fruit-questions\", \"mmlu\"]\n",
    ")\n",
    "\n",
    "print(f\"Cookbook '{cookbook_id}' has been created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76208f03-e0a3-4d55-b565-267e1a847027",
   "metadata": {},
   "source": [
    "## Run your new cookbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9b5b3ce-779c-47f8-94fc-9a2f2147236b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-08 18:52:11,292 [INFO][runner.py::run_cookbooks(422)] [Runner] test-new-cookbook - Running benchmark cookbook run...\n",
      "2024-11-08 18:52:11,323 [INFO][benchmarking.py::generate(126)] [Benchmarking] Running cookbooks (['test-cookbook'])...\n",
      "2024-11-08 18:52:11,323 [INFO][benchmarking.py::generate(132)] [Benchmarking] Running cookbook test-cookbook... (1/1)\n",
      "2024-11-08 18:52:11,329 [INFO][connector.py::get_prediction(348)] [Connector ID: my-openai-endpoint] Predicting Prompt Index 0.\n",
      "2024-11-08 18:52:11,330 [INFO][connector.py::get_prediction(348)] [Connector ID: my-openai-endpoint] Predicting Prompt Index 1.\n",
      "2024-11-08 18:52:11,331 [INFO][connector.py::get_prediction(348)] [Connector ID: my-openai-endpoint] Predicting Prompt Index 3.\n",
      "2024-11-08 18:52:11,331 [INFO][connector.py::get_prediction(348)] [Connector ID: my-openai-endpoint] Predicting Prompt Index 5.\n",
      "2024-11-08 18:52:11,331 [INFO][connector.py::get_prediction(348)] [Connector ID: my-openai-endpoint] Predicting Prompt Index 6.\n",
      "2024-11-08 18:52:15,976 [INFO][connector.py::get_prediction(348)] [Connector ID: my-openai-endpoint] Predicting Prompt Index 1326.\n",
      "2024-11-08 18:52:15,977 [INFO][connector.py::get_prediction(348)] [Connector ID: my-openai-endpoint] Predicting Prompt Index 8484.\n",
      "2024-11-08 18:52:15,977 [INFO][connector.py::get_prediction(348)] [Connector ID: my-openai-endpoint] Predicting Prompt Index 12623.\n",
      "2024-11-08 18:52:15,977 [INFO][connector.py::get_prediction(348)] [Connector ID: my-openai-endpoint] Predicting Prompt Index 13781.\n",
      "2024-11-08 18:52:15,977 [INFO][connector.py::get_prediction(348)] [Connector ID: my-openai-endpoint] Predicting Prompt Index 16753.\n",
      "2024-11-08 18:52:21,048 [INFO][benchmarking.py::generate(190)] [Benchmarking] Run took 9.7253s\n",
      "2024-11-08 18:52:21,050 [INFO][benchmarking.py::generate(245)] [Benchmarking] Preparing results took 0.0000s\n",
      "2024-11-08 18:52:21,059 [INFO][benchmarking-result.py::generate(58)] [BenchmarkingResult] Generate results took 0.0090s\n",
      "2024-11-08 18:52:21,061 [INFO][runner.py::run_cookbooks(448)] [Runner] test-new-cookbook - Benchmark cookbook run completed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"metadata\": {\n",
      "    \"id\": \"test-new-cookbook\",\n",
      "    \"start_time\": \"2024-11-08 18:52:11\",\n",
      "    \"end_time\": \"2024-11-08 18:52:21\",\n",
      "    \"duration\": 9,\n",
      "    \"status\": \"completed\",\n",
      "    \"recipes\": null,\n",
      "    \"cookbooks\": [\n",
      "      \"test-cookbook\"\n",
      "    ],\n",
      "    \"endpoints\": [\n",
      "      \"my-openai-endpoint\"\n",
      "    ],\n",
      "    \"num_of_prompts\": 5,\n",
      "    \"random_seed\": 0,\n",
      "    \"system_prompt\": \"\"\n",
      "  },\n",
      "  \"results\": {\n",
      "    \"cookbooks\": [\n",
      "      {\n",
      "        \"id\": \"test-cookbook\",\n",
      "        \"recipes\": [\n",
      "          {\n",
      "            \"id\": \"fruit-questions\",\n",
      "            \"details\": [\n",
      "              {\n",
      "                \"model_id\": \"my-openai-endpoint\",\n",
      "                \"dataset_id\": \"test-dataset\",\n",
      "                \"prompt_template_id\": \"no-template\",\n",
      "                \"data\": [\n",
      "                  {\n",
      "                    \"prompt\": \"Is Lemon a Fruit? Answer Yes or No.\",\n",
      "                    \"predicted_result\": {\n",
      "                      \"response\": \"Yes\",\n",
      "                      \"context\": []\n",
      "                    },\n",
      "                    \"target\": \"Yes.\",\n",
      "                    \"duration\": 0.7032334579853341\n",
      "                  },\n",
      "                  {\n",
      "                    \"prompt\": \"Is Apple a Fruit? Answer Yes or No.\",\n",
      "                    \"predicted_result\": {\n",
      "                      \"response\": \"Yes.\",\n",
      "                      \"context\": []\n",
      "                    },\n",
      "                    \"target\": \"Yes.\",\n",
      "                    \"duration\": 1.5167350000119768\n",
      "                  },\n",
      "                  {\n",
      "                    \"prompt\": \"Is Bak Kwa a Fruit? Answer Yes or No.\",\n",
      "                    \"predicted_result\": {\n",
      "                      \"response\": \"No.\",\n",
      "                      \"context\": []\n",
      "                    },\n",
      "                    \"target\": \"No.\",\n",
      "                    \"duration\": 2.542461084027309\n",
      "                  },\n",
      "                  {\n",
      "                    \"prompt\": \"Is Orange a Fruit? Answer Yes or No.\",\n",
      "                    \"predicted_result\": {\n",
      "                      \"response\": \"Yes\",\n",
      "                      \"context\": []\n",
      "                    },\n",
      "                    \"target\": \"Yes.\",\n",
      "                    \"duration\": 3.569820500037167\n",
      "                  },\n",
      "                  {\n",
      "                    \"prompt\": \"Is Coke Zero a Fruit? Answer Yes or No.\",\n",
      "                    \"predicted_result\": {\n",
      "                      \"response\": \"No.\",\n",
      "                      \"context\": []\n",
      "                    },\n",
      "                    \"target\": \"No.\",\n",
      "                    \"duration\": 4.537833667011\n",
      "                  }\n",
      "                ],\n",
      "                \"metrics\": [\n",
      "                  {\n",
      "                    \"accuracy\": 60.0,\n",
      "                    \"grading_criteria\": {\n",
      "                      \"accuracy\": 60.0\n",
      "                    }\n",
      "                  }\n",
      "                ]\n",
      "              }\n",
      "            ],\n",
      "            \"evaluation_summary\": [\n",
      "              {\n",
      "                \"model_id\": \"my-openai-endpoint\",\n",
      "                \"num_of_prompts\": 5,\n",
      "                \"avg_grade_value\": 60.0,\n",
      "                \"grade\": \"B\"\n",
      "              }\n",
      "            ],\n",
      "            \"grading_scale\": {\n",
      "              \"A\": [\n",
      "                80,\n",
      "                100\n",
      "              ],\n",
      "              \"B\": [\n",
      "                60,\n",
      "                79\n",
      "              ],\n",
      "              \"C\": [\n",
      "                40,\n",
      "                59\n",
      "              ],\n",
      "              \"D\": [\n",
      "                20,\n",
      "                39\n",
      "              ],\n",
      "              \"E\": [\n",
      "                0,\n",
      "                19\n",
      "              ]\n",
      "            },\n",
      "            \"total_num_of_prompts\": 5\n",
      "          },\n",
      "          {\n",
      "            \"id\": \"mmlu\",\n",
      "            \"details\": [\n",
      "              {\n",
      "                \"model_id\": \"my-openai-endpoint\",\n",
      "                \"dataset_id\": \"mmlu-all\",\n",
      "                \"prompt_template_id\": \"mmlu\",\n",
      "                \"data\": [\n",
      "                  {\n",
      "                    \"prompt\": \"Question:\\nSeedy the watermelon was a very special type of watermelon. He didn't have any seeds. He was green and he had stripes. All of his cousins had seeds, but he didn't have any. He felt very left out. He couldn't thing of why he was different. His mom told him it was because he was a very special watermelon. She also tells him she loves him the way he is. But Seedy didn't think it was a good thing. He wished he could be like everyone else and have seeds. One day, he rolled out to the lawn and looked at all of his new cousins growing in the garden. He rolled around until he found a little baby watermelon that didn't have any seeds either. He sat next to him and talked to him. He told him that he was very special, and was excited for him to be picked off the vine and be his special best friend. He would name him Seedy, Jr. What did Seedy name his best friend?\\nA. Green\\nB. Stripes\\nC. Mom\\nD. Seedy, Jr.\\n\\nSelect the most appropriate answer or options to fill in the blank (if any): \",\n",
      "                    \"predicted_result\": {\n",
      "                      \"response\": \"D. Seedy, Jr.\",\n",
      "                      \"context\": []\n",
      "                    },\n",
      "                    \"target\": \"D. Seedy, Jr.\",\n",
      "                    \"duration\": 0.9670258749974892\n",
      "                  },\n",
      "                  {\n",
      "                    \"prompt\": \"Question:\\nWhich of the following fallacies happens when someone concludes that someone couldn't have done something bad because he or she has good qualities?\\nA. Laudatory personality\\nB. Guilt by association\\nC. Reprehensible personality\\nD. Circular reasoning\\n\\nSelect the most appropriate answer or options to fill in the blank (if any): \",\n",
      "                    \"predicted_result\": {\n",
      "                      \"response\": \"B. Guilt by association\",\n",
      "                      \"context\": []\n",
      "                    },\n",
      "                    \"target\": \"A. Laudatory personality\",\n",
      "                    \"duration\": 1.8888028750079684\n",
      "                  },\n",
      "                  {\n",
      "                    \"prompt\": \"Question:\\nBecause of a sudden and unanticipated severe shortage of heating fuel, the President has ordered all offices of federal executive agencies to be open only four days per week. The President's order allows an exception to the extent that emergency circumstances require different hours of operation (as in the case of federal hospitals). When Congress enacted the appropriations statute for operating all federal executive agencies, its members assumed that the offices of those agencies would be open five days per week, but Congress did not include such a requirement in its appropriations statute or in any other statute. Is the President's order constitutional?\\nA. No, because the heads of the various executive agencies have final responsibility for the operation of those agencies' offices.\\nB. No, because when they passed the statute appropriating monies for the operation of executive agencies, members of Congress assumed that those agencies' offices would be open five days per week.\\nC. Yes, because the Constitution vests the President with plenary authority to direct the administration of all federal agencies in any manner the President deems expedient.\\nD. Yes, because the order relates to the management of the executive branch and is not prohibited by any statute.\\n\\nSelect the most appropriate answer or options to fill in the blank (if any): \",\n",
      "                    \"predicted_result\": {\n",
      "                      \"response\": \"D. Yes, because the order relates to the management of the executive branch and is not prohibited by any statute.\",\n",
      "                      \"context\": []\n",
      "                    },\n",
      "                    \"target\": \"B. No, because when they passed the statute appropriating monies for the operation of executive agencies, members of Congress assumed that those agencies' offices would be open five days per week.\",\n",
      "                    \"duration\": 3.2203749159816653\n",
      "                  },\n",
      "                  {\n",
      "                    \"prompt\": \"Question:\\nA salesman, who had worked 20 years for the same company, was suddenly terminated for no apparent reason. Thereafter, the salesman sued the company, alleging age discrimination. At trial, he wants to call an employee of the company as an adverse witness. The salesman seeks to have the employee testify that she was present at a company board meeting when the company's president allegedly said, \\\"Now, I'm sure that everyone agrees that the salesman is too old, and he really doesn't typify the image we want our employees to project. \\\" It is the common practice of the company to tape record all such board meetings. Moreover, it is customary for the company's secretary to transcribe the tapes following the board meetings. Upon objection by the company's attomey, the employee's proposed testimony will be held\\nA. admissible, because the employee was present during the board meeting.\\nB. admissible, because the president's statement was an admission by a company representative.\\nC. inadmissible, because the tape of the meeting is the best evidence.\\nD. inadmissible, because the secretary's transcribed notes are the best evidence.\\n\\nSelect the most appropriate answer or options to fill in the blank (if any): \",\n",
      "                    \"predicted_result\": {\n",
      "                      \"response\": \"C. inadmissible, because the tape of the meeting is the best evidence.\",\n",
      "                      \"context\": []\n",
      "                    },\n",
      "                    \"target\": \"B. admissible, because the president's statement was an admission by a company representative.\",\n",
      "                    \"duration\": 4.040708416025154\n",
      "                  },\n",
      "                  {\n",
      "                    \"prompt\": \"Question:\\nA student who earns a JD can begin his or her career as a what?\\nA. lawyer\\nB. bricklayer\\nC. doctor\\nD. accountant\\n\\nSelect the most appropriate answer or options to fill in the blank (if any): \",\n",
      "                    \"predicted_result\": {\n",
      "                      \"response\": \"A. lawyer\",\n",
      "                      \"context\": []\n",
      "                    },\n",
      "                    \"target\": \"A. lawyer\",\n",
      "                    \"duration\": 5.064865124993958\n",
      "                  }\n",
      "                ],\n",
      "                \"metrics\": [\n",
      "                  {\n",
      "                    \"accuracy\": 40.0,\n",
      "                    \"grading_criteria\": {\n",
      "                      \"accuracy\": 40.0\n",
      "                    }\n",
      "                  }\n",
      "                ]\n",
      "              }\n",
      "            ],\n",
      "            \"evaluation_summary\": [\n",
      "              {\n",
      "                \"model_id\": \"my-openai-endpoint\",\n",
      "                \"num_of_prompts\": 5,\n",
      "                \"avg_grade_value\": 40.0,\n",
      "                \"grade\": \"C\"\n",
      "              }\n",
      "            ],\n",
      "            \"grading_scale\": {\n",
      "              \"A\": [\n",
      "                80,\n",
      "                100\n",
      "              ],\n",
      "              \"B\": [\n",
      "                60,\n",
      "                79\n",
      "              ],\n",
      "              \"C\": [\n",
      "                40,\n",
      "                59\n",
      "              ],\n",
      "              \"D\": [\n",
      "                20,\n",
      "                39\n",
      "              ],\n",
      "              \"E\": [\n",
      "                0,\n",
      "                19\n",
      "              ]\n",
      "            },\n",
      "            \"total_num_of_prompts\": 5\n",
      "          }\n",
      "        ],\n",
      "        \"overall_evaluation_summary\": [\n",
      "          {\n",
      "            \"model_id\": \"my-openai-endpoint\",\n",
      "            \"overall_grade\": \"C\"\n",
      "          }\n",
      "        ],\n",
      "        \"total_num_of_prompts\": 10\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from slugify import slugify\n",
    "from moonshot.api import api_get_all_run, api_create_runner, api_get_all_runner_name\n",
    "\n",
    "name = \"test new cookbook\" # Indicate the name\n",
    "cookbooks = [\"test-cookbook\"] # Test one cookbook test-cookbook. You can add more cookbooks in the list to test as well\n",
    "endpoints = [\"my-openai-endpoint\"] # Test against 1 endpoint, my-openai-endpoint\n",
    "num_of_prompts = 5 # use a smaller number to test out the function; 0 means using all prompts in dataset\n",
    "\n",
    "# Optional fields\n",
    "random_seed = 0   # Default: 0; this allows for randomness in dataset selection when num_of_prompts are set\n",
    "system_prompt = \"\"  # Default: \"\"; this allows setting the system prompt for the endpoints\n",
    "\n",
    "# Advanced user - Modify runner processing module and result processing module\n",
    "# Default: benchmarking and benchmarking-result. Change it to your module name if you have your own runner and/or result module\n",
    "runner_proc_module = \"benchmarking\"  # Default: \"benchmarking\"\n",
    "result_proc_module = \"benchmarking-result\"  # Default: \"benchmarking-result\"\n",
    "\n",
    "# Run the cookbooks with the defined endpoint(s)\n",
    "# If the id exists, it will perform a load on the runner, instead of creating a new runner.\n",
    "# Using an existing runner allows the new run to possibly use cached results from previous runs, which greatly reduces the run time\n",
    "slugify_id = slugify(name, lowercase=True)\n",
    "if slugify_id in api_get_all_runner_name():\n",
    "    cb_runner = api_load_runner(slugify_id)\n",
    "else:\n",
    "    cb_runner = api_create_runner(name, endpoints)\n",
    "\n",
    "# run_cookbooks() is an async function. Currently there is no sync version\n",
    "# We will get an existing event loop and execute the run cookbooks process\n",
    "await cb_runner.run_cookbooks(\n",
    "        cookbooks,\n",
    "        num_of_prompts,\n",
    "        random_seed,\n",
    "        system_prompt,\n",
    "        runner_proc_module,\n",
    "        result_proc_module,\n",
    "    )\n",
    "await cb_runner.close()  # Perform a close on the runner to allow proper cleanup.\n",
    "\n",
    "# Display results\n",
    "runner_runs = api_get_all_run(cb_runner.id)\n",
    "result_info = runner_runs[-1].get(\"results\")\n",
    "if result_info:\n",
    "    print(json.dumps(result_info, indent=2))\n",
    "else:\n",
    "    raise RuntimeError(\"no run result generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf2ab13-bf8c-401b-a0f5-b59339fe0749",
   "metadata": {},
   "source": [
    "## Beautifying Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3adebeba-18b1-4fd6-8e95-b705d24f769d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                                  Cookbook Result                                                  </span>\n",
       "┏━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> No. </span>┃<span style=\"font-weight: bold\"> Cookbook (with its recipes)                                                         </span>┃<span style=\"font-weight: bold\"> my-openai-endpoint  </span>┃\n",
       "┡━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ 1   │ Cookbook: <span style=\"color: #000080; text-decoration-color: #000080\">test-cookbook</span>                                                             │          C          │\n",
       "├─────┼─────────────────────────────────────────────────────────────────────────────────────┼─────────────────────┤\n",
       "│     │   └──  Recipe: <span style=\"color: #000080; text-decoration-color: #000080\">fruit-questions</span>                                                      │      B [60.0]       │\n",
       "├─────┼─────────────────────────────────────────────────────────────────────────────────────┼─────────────────────┤\n",
       "│     │   └──  Recipe: <span style=\"color: #000080; text-decoration-color: #000080\">mmlu</span>                                                                 │      C [40.0]       │\n",
       "└─────┴─────────────────────────────────────────────────────────────────────────────────────┴─────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                                  Cookbook Result                                                  \u001b[0m\n",
       "┏━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mNo.\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mCookbook (with its recipes)                                                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mmy-openai-endpoint \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ 1   │ Cookbook: \u001b[34mtest-cookbook\u001b[0m                                                             │          C          │\n",
       "├─────┼─────────────────────────────────────────────────────────────────────────────────────┼─────────────────────┤\n",
       "│     │   └──  Recipe: \u001b[34mfruit-questions\u001b[0m                                                      │      B [60.0]       │\n",
       "├─────┼─────────────────────────────────────────────────────────────────────────────────────┼─────────────────────┤\n",
       "│     │   └──  Recipe: \u001b[34mmmlu\u001b[0m                                                                 │      C [40.0]       │\n",
       "└─────┴─────────────────────────────────────────────────────────────────────────────────────┴─────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">==================================================\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">Time taken to run: 9s</span>\n",
       "*Overall rating will be the lowest grade that the recipes have in each cookbook\n",
       "==================================================\n",
       "</pre>\n"
      ],
      "text/plain": [
       "==================================================\n",
       "\u001b[34mTime taken to run: 9s\u001b[0m\n",
       "*Overall rating will be the lowest grade that the recipes have in each cookbook\n",
       "==================================================\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich.columns import Columns\n",
    "from rich.console import Console\n",
    "from rich.panel import Panel\n",
    "from rich.table import Table\n",
    "console = Console()\n",
    "\n",
    "def show_cookbook_results(cookbooks, endpoints, cookbook_results, duration):\n",
    "    \"\"\"\n",
    "    Show the results of the cookbook benchmarking.\n",
    "\n",
    "    This function takes the cookbooks, endpoints, cookbook results, results file, and duration as arguments.\n",
    "    If there are results, it generates a table with the cookbook results and prints a message indicating\n",
    "    where the results are saved. If there are no results, it prints a message indicating that no results were found.\n",
    "    Finally, it prints the duration of the run.\n",
    "\n",
    "    Args:\n",
    "        cookbooks (list): A list of cookbooks.\n",
    "        endpoints (list): A list of endpoints.\n",
    "        cookbook_results (dict): A dictionary with the results of the cookbook benchmarking.\n",
    "        duration (float): The duration of the run.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if cookbook_results:\n",
    "        # Display recipe results\n",
    "        generate_cookbook_table(cookbooks, endpoints, cookbook_results)\n",
    "    else:\n",
    "        console.print(\"[red]There are no results.[/red]\")\n",
    "\n",
    "    # Print run stats\n",
    "    console.print(f\"{'='*50}\\n[blue]Time taken to run: {duration}s[/blue]\\n*Overall rating will be the lowest grade that the recipes have in each cookbook\\n{'='*50}\")\n",
    "\n",
    "def generate_cookbook_table(cookbooks: list, endpoints: list, results: dict) -> None:\n",
    "    \"\"\"\n",
    "    Generate and display a table with the cookbook benchmarking results.\n",
    "\n",
    "    This function creates a table that includes the index, cookbook name, recipe name, and the results\n",
    "    for each endpoint.\n",
    "\n",
    "    The cookbook names are prefixed with \"Cookbook:\" and are displayed with their overall grades. Each recipe under a\n",
    "    cookbook is indented and prefixed with \"Recipe:\" followed by its individual grades for each endpoint. If there are\n",
    "    no results for a cookbook, a row with dashes across all endpoint columns is added to indicate this.\n",
    "\n",
    "    Args:\n",
    "        cookbooks (list): A list of cookbook names to display in the table.\n",
    "        endpoints (list): A list of endpoints for which results are to be displayed.\n",
    "        results (dict): A dictionary containing the benchmarking results for cookbooks and recipes.\n",
    "\n",
    "    Returns:\n",
    "        None: The function prints the table to the console but does not return any value.\n",
    "    \"\"\"\n",
    "    table = Table(\n",
    "        title=\"Cookbook Result\", show_lines=True, expand=True, header_style=\"bold\"\n",
    "    )\n",
    "    table.add_column(\"No.\", width=2)\n",
    "    table.add_column(\"Cookbook (with its recipes)\", justify=\"left\", width=78)\n",
    "    for endpoint in endpoints:\n",
    "        table.add_column(endpoint, justify=\"center\")\n",
    "\n",
    "    index = 1\n",
    "    for cookbook in cookbooks:\n",
    "        # Get cookbook result\n",
    "        cookbook_result = next(\n",
    "            (\n",
    "                result\n",
    "                for result in results[\"results\"][\"cookbooks\"]\n",
    "                if result[\"id\"] == cookbook\n",
    "            ),\n",
    "            None,\n",
    "        )\n",
    "\n",
    "        if cookbook_result:\n",
    "            # Add the cookbook name with the \"Cookbook: \" prefix as the first row for this section\n",
    "            endpoint_results = []\n",
    "            for endpoint in endpoints:\n",
    "                # Find the evaluation summary for the endpoint\n",
    "                evaluation_summary = next(\n",
    "                    (\n",
    "                        temp_eval\n",
    "                        for temp_eval in cookbook_result[\"overall_evaluation_summary\"]\n",
    "                        if temp_eval[\"model_id\"] == endpoint\n",
    "                    ),\n",
    "                    None,\n",
    "                )\n",
    "\n",
    "                # Get the grade from the evaluation_summary, or use \"-\" if not found\n",
    "                grade = \"-\"\n",
    "                if evaluation_summary and evaluation_summary[\"overall_grade\"]:\n",
    "                    grade = evaluation_summary[\"overall_grade\"]\n",
    "                endpoint_results.append(grade)\n",
    "            table.add_row(\n",
    "                str(index),\n",
    "                f\"Cookbook: [blue]{cookbook}[/blue]\",\n",
    "                *endpoint_results,\n",
    "                end_section=True,\n",
    "            )\n",
    "\n",
    "            for recipe in cookbook_result[\"recipes\"]:\n",
    "                endpoint_results = []\n",
    "                for endpoint in endpoints:\n",
    "                    # Find the evaluation summary for the endpoint\n",
    "                    evaluation_summary = next(\n",
    "                        (\n",
    "                            temp_eval\n",
    "                            for temp_eval in recipe[\"evaluation_summary\"]\n",
    "                            if temp_eval[\"model_id\"] == endpoint\n",
    "                        ),\n",
    "                        None,\n",
    "                    )\n",
    "\n",
    "                    # Get the grade from the evaluation_summary, or use \"-\" if not found\n",
    "                    grade = \"-\"\n",
    "                    if (\n",
    "                        evaluation_summary\n",
    "                        and \"grade\" in evaluation_summary\n",
    "                        and \"avg_grade_value\" in evaluation_summary\n",
    "                        and evaluation_summary[\"grade\"]\n",
    "                    ):\n",
    "                        grade = f\"{evaluation_summary['grade']} [{evaluation_summary['avg_grade_value']}]\"\n",
    "                    endpoint_results.append(grade)\n",
    "\n",
    "                # Add the recipe name indented under the cookbook name\n",
    "                table.add_row(\n",
    "                    \"\",\n",
    "                    f\"  └──  Recipe: [blue]{recipe['id']}[/blue]\",\n",
    "                    *endpoint_results,\n",
    "                    end_section=True,\n",
    "                )\n",
    "\n",
    "            # Increment index only after all recipes of the cookbook have been added\n",
    "            index += 1\n",
    "        else:\n",
    "            # If no results for the cookbook, add a row indicating this with the \"Cookbook: \" prefix\n",
    "            # and a dash for each endpoint column\n",
    "            table.add_row(\n",
    "                str(index),\n",
    "                f\"Cookbook: {cookbook}\",\n",
    "                *([\"-\"] * len(endpoints)),\n",
    "                end_section=True,\n",
    "            )\n",
    "            index += 1\n",
    "\n",
    "    # Display table\n",
    "    console.print(table)\n",
    "\n",
    "if result_info:\n",
    "    show_cookbook_results(\n",
    "        cookbooks, endpoints, result_info, result_info[\"metadata\"][\"duration\"]\n",
    "    )\n",
    "else:\n",
    "    raise RuntimeError(\"no run result generated\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
