{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1adab3f9-616e-4efb-9217-ea01014d4e03",
   "metadata": {},
   "source": [
    "# Tutorial 1 - Basic Workflow - Execute Existing Tests \n",
    "\n",
    "**Scenario**: You are a model developer and you are told to deploy a system that uses a Large Language Model. However, you are uncertain which model performs best for your use case and you want to assess potential models' capabilities using the pre-built benchmarks in Moonshot. How can you do this? \n",
    "\n",
    "In this tutorial, you will learn how to:\n",
    "\n",
    "- Add your own `connector_endpoints` into Moonshot\n",
    "- List and run an existing `cookbook` in Moonshot\n",
    "\n",
    "Prerequisite:\n",
    "\n",
    "1. Your own copy of `moonshot-data`. You will be setting its path to the `moonshot_path` variable in the first cell\n",
    "2. Your OpenAI key, which you will set to the placeholder `ADD_NEW_TOKEN_HERE` in a cell later\n",
    "\n",
    "**Before starting this tutorial, please make sure you have already installed `moonshot` and `moonshot-data`.** Otherwise, please follow [this tutorial](https://aiverify-foundation.github.io/moonshot/getting_started/quick_install) to install and configure Moonshot first.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9890dfc2-cd4a-405f-b90f-b9284e50dca6",
   "metadata": {},
   "source": [
    "## Import and configure Moonshot\n",
    "\n",
    "In this section, we prepare our Jupyter notebook environment by importing necessary libraries required to execute an existing benchmark.\n",
    "\n",
    "> ⚠️ **Check:** that `moonshot_data_path` below matches the location where you installed `moonshot-data` - and edit the code to match your location if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0dfafc2-097d-4a17-b8ef-dd964ede8b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python built-ins:\n",
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import sys\n",
    "\n",
    "# IF you're running this notebook from the moonshot/examples/jupyter-notebook folder, the below\n",
    "# line will enable you to import moonshot from the local source code. If you installed moonshot\n",
    "# from pip, you can remove this:\n",
    "sys.path.insert(0, '../../')\n",
    "\n",
    "# Import moonshot utilities:\n",
    "from moonshot.api import (\n",
    "    api_create_endpoint,\n",
    "    api_get_all_endpoint,\n",
    "    api_get_all_cookbook,\n",
    "    api_load_runner,\n",
    "    api_read_result,\n",
    "    api_set_environment_variables,\n",
    ")\n",
    "\n",
    "# modify moonshot_data_path to point to your own copy of moonshot-data\n",
    "moonshot_path = \"./moonshot-data\"\n",
    "env = {\n",
    "    \"ATTACK_MODULES\": os.path.join(moonshot_path, \"attack-modules\"),\n",
    "    \"BOOKMARKS\": os.path.join(moonshot_path, \"generated-outputs/bookmarks\"),\n",
    "    \"CONNECTORS\": os.path.join(moonshot_path, \"connectors\"),\n",
    "    \"CONNECTORS_ENDPOINTS\": os.path.join(moonshot_path, \"connectors-endpoints\"),\n",
    "    \"CONTEXT_STRATEGY\": os.path.join(moonshot_path, \"context-strategy\"),\n",
    "    \"COOKBOOKS\": os.path.join(moonshot_path, \"cookbooks\"),\n",
    "    \"DATABASES\": os.path.join(moonshot_path, \"generated-outputs/databases\"),\n",
    "    \"DATABASES_MODULES\": os.path.join(moonshot_path, \"databases-modules\"),\n",
    "    \"DATASETS\": os.path.join(moonshot_path, \"datasets\"),\n",
    "    \"IO_MODULES\": os.path.join(moonshot_path, \"io-modules\"),\n",
    "    \"METRICS\": os.path.join(moonshot_path, \"metrics\"),\n",
    "    \"PROMPT_TEMPLATES\": os.path.join(moonshot_path, \"prompt-templates\"),\n",
    "    \"RECIPES\": os.path.join(moonshot_path, \"recipes\"),\n",
    "    \"RESULTS\": os.path.join(moonshot_path, \"generated-outputs/results\"),\n",
    "    \"RESULTS_MODULES\": os.path.join(moonshot_path, \"results-modules\"),\n",
    "    \"RUNNERS\": os.path.join(moonshot_path, \"generated-outputs/runners\"),\n",
    "    \"RUNNERS_MODULES\": os.path.join(moonshot_path, \"runners-modules\"),\n",
    "}\n",
    "\n",
    "# Check user has set moonshot_data_path correctly:\n",
    "if not os.path.isdir(env[\"ATTACK_MODULES\"]):\n",
    "    raise ValueError(\n",
    "        \"Configured path %s does not exist. Is moonshot-data installed at %s?\"\n",
    "        % (env[\"ATTACK_MODULES\"], moonshot_data_path)\n",
    "    )\n",
    "\n",
    "# Apply the environment variables to configure the Moonshot framework.\n",
    "api_set_environment_variables(env)\n",
    "\n",
    "# Note: there will be no printout if the environment variables are set successfully"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0792527a-ab68-4826-b4c2-3d2f2a0b2a59",
   "metadata": {},
   "source": [
    "## Define the target model endpoint / API\n",
    "\n",
    "Moonshot provides [connectors](https://aiverify-foundation.github.io/moonshot/api_reference/api_connector/) to a range of different LLM hosting providers - such as OpenAI (direct or Azure), Hugging Face, Amazon Bedrock, and Google Gemini.\n",
    "\n",
    "There are some [example endpoint configurations](https://github.com/aiverify-foundation/moonshot-data/tree/main/connectors-endpoints) provided in `moonshot-data`, but they don't include API keys or other credentials: So you'll usually need to edit these configurations, or add your own, to connect to your target LLM.\n",
    "\n",
    "You can register new Moonshot endpoints directly from Python, as shown below.\n",
    "\n",
    "▶️ **TODO: Edit the cell below to configure your own LLM.**\n",
    "\n",
    "> If you're using OpenAI, you'll just need to replace `ADD_YOUR_TOKEN_HERE` below with your own OpenAI token.\n",
    ">\n",
    "> If you're using a different provider, check out the [list of connector IDs](https://github.com/aiverify-foundation/moonshot-data/tree/main/connectors) provided by `moonshot-data`. Different connectors have different required parameters. For example, the `amazon-bedrock-connector` can automatically pick up credentials configured in the AWS CLI - so you'll usually leave `token` blank for this connector type.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4f87c6-8ad8-4b3f-b233-5d9143bf43dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_id = api_create_endpoint(\n",
    "    \"my-openai-endpoint\",    # name: Assign a unique name to identify this endpoint later.\n",
    "    \"openai-connector\",      # connector_type: Specify the connector type for the model you want to evaluate.\n",
    "    \"\",                      # uri: Leave blank as the OpenAI library handles the connection.\n",
    "    \"ADD_NEW_TOKEN_HERE\",    # token: Insert your OpenAI API token here.\n",
    "    1,                       # max_calls_per_second: Set the maximum number of calls allowed per second.\n",
    "    1,                       # max_concurrency: Set the maximum number of concurrent calls.\n",
    "    \"gpt-3.5-turbo\",         # model: Define the model version to use. \n",
    "    \n",
    "    # params: Include any additional parameters required for this model.\n",
    "    {\n",
    "        \"timeout\": 300,      # timeout: Set the timeout for API calls in seconds.\n",
    "        \"max_attempts\": 3,   # max_attempts: Set the max number of retry attempts. \n",
    "        \"temperature\": 0.5,  # temperature: Set the temperature for response variability.\n",
    "    }  \n",
    ")\n",
    "print(f\"The newly created endpoint id: {endpoint_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d04074",
   "metadata": {},
   "source": [
    "You'll see running the above creates a new configuration file under your Moonshot data `CONNECTORS_ENDPOINTS` folder.\n",
    "\n",
    "These stored endpoint IDs are what we'll reference when running tests in Moonshot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdb703b-94ca-4515-85e6-9dde0bfb9c69",
   "metadata": {},
   "source": [
    "## Run a test using our predefined `cookbook`\n",
    "\n",
    "Moonshot comes with a list of `cookbooks` and `recipes`. A `recipe` contains one or more benchmark datasets and evaluation metrics. A `cookbook` contains one or more `recipes`. To execute an existing test, we can select either a `recipe` or `cookbook`.\n",
    "\n",
    "In this tutorial, we will run a `cookbook` called `leaderboard-cookbook`. This cookbook contains a set of popular benchmarks (e.g., `mmlu`) that can be used to assess the capability of the model. \n",
    "\n",
    "*For the purpose of this tutorial, we will configure our `runner` to run 1 prompt from every recipe in this cookbook - on the endpoint we created*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b6cab8-bbcb-47a2-b61f-62ebc4581345",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from slugify import slugify\n",
    "from moonshot.api import api_get_all_run, api_create_runner, api_get_all_runner_name\n",
    "\n",
    "name = \"sample-cookbook-runner\" # Indicate the name\n",
    "cookbooks = [\"leaderboard-cookbook\"] # Test one cookbook leaderboard-cookbook. You can add more cookbooks in the list to test as well\n",
    "endpoints = [\"my-openai-endpoint\"] # Test against 1 endpoint, my-openai-endpoint\n",
    "num_of_prompts = 1 # The number of prompt(s) to run from EACH dataset in the cookbook; 0 means using all prompts in dataset\n",
    "\n",
    "# Below are the optional fields\n",
    "random_seed = 0   # Default: 0; this allows for randomness in dataset selection when num_of_prompts are set\n",
    "system_prompt = \"\"  # Default: \"\"; this allows setting the system prompt for the endpoints\n",
    "\n",
    "# Advanced user - Modify runner processing module and result processing module\n",
    "# Default: benchmarking and benchmarking-result. Change it to your module name if you have your own runner and/or result module\n",
    "runner_proc_module = \"benchmarking\"  # Default: \"benchmarking\"\n",
    "result_proc_module = \"benchmarking-result\"  # Default: \"benchmarking-result\"\n",
    "\n",
    "# Run the cookbooks with the defined endpoint(s)\n",
    "# If the id exists, it will perform a load on the runner, instead of creating a new runner\n",
    "# Using an existing runner allows the new run to possibly use cached results from previous runs, which greatly reduces the run time\n",
    "slugify_id = slugify(name, lowercase=True)\n",
    "if slugify_id in api_get_all_runner_name():\n",
    "    cb_runner = api_load_runner(slugify_id)\n",
    "else:\n",
    "    cb_runner = api_create_runner(name, endpoints)\n",
    "\n",
    "# run_cookbooks() is an async function. Currently there is no sync version\n",
    "# We will get an existing event loop and execute the run cookbooks process\n",
    "await cb_runner.run_cookbooks(\n",
    "        cookbooks,\n",
    "        num_of_prompts,\n",
    "        random_seed,\n",
    "        system_prompt,\n",
    "        runner_proc_module,\n",
    "        result_proc_module,\n",
    "    )\n",
    "await cb_runner.close()  # Perform a close on the runner to allow proper cleanup.\n",
    "\n",
    "# Display results in JSON\n",
    "runner_runs = api_get_all_run(cb_runner.id)\n",
    "result_info = runner_runs[-1].get(\"results\")\n",
    "if result_info:\n",
    "    print(json.dumps(result_info, indent=2))\n",
    "else:\n",
    "    raise RuntimeError(\"no run result generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd2a0d5-d5f7-48f5-bd6f-73ca2d6a0430",
   "metadata": {},
   "source": [
    "## Beautifying the results\n",
    "\n",
    "The result above is shown in our raw JSON file. To beautify the results, you can use the `rich` library to put them into a nice table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7f1b98-497d-4f92-acc5-838e11bd0434",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.columns import Columns\n",
    "from rich.console import Console\n",
    "from rich.panel import Panel\n",
    "from rich.table import Table\n",
    "console = Console()\n",
    "\n",
    "def show_cookbook_results(cookbooks, endpoints, cookbook_results, duration):\n",
    "    \"\"\"\n",
    "    Show the results of the cookbook benchmarking.\n",
    "\n",
    "    This function takes the cookbooks, endpoints, cookbook results, results file, and duration as arguments.\n",
    "    If there are results, it generates a table with the cookbook results and prints a message indicating\n",
    "    where the results are saved. If there are no results, it prints a message indicating that no results were found.\n",
    "    Finally, it prints the duration of the run.\n",
    "\n",
    "    Args:\n",
    "        cookbooks (list): A list of cookbooks.\n",
    "        endpoints (list): A list of endpoints.\n",
    "        cookbook_results (dict): A dictionary with the results of the cookbook benchmarking.\n",
    "        duration (float): The duration of the run.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if cookbook_results:\n",
    "        # Display recipe results\n",
    "        generate_cookbook_table(cookbooks, endpoints, cookbook_results)\n",
    "    else:\n",
    "        console.print(\"[red]There are no results.[/red]\")\n",
    "\n",
    "    # Print run stats\n",
    "    console.print(f\"{'='*50}\\n[blue]Time taken to run: {duration}s[/blue]\\n*Overall rating will be the lowest grade that the recipes have in each cookbook\\n{'='*50}\")\n",
    "\n",
    "def generate_cookbook_table(cookbooks: list, endpoints: list, results: dict) -> None:\n",
    "    \"\"\"\n",
    "    Generate and display a table with the cookbook benchmarking results.\n",
    "\n",
    "    This function creates a table that includes the index, cookbook name, recipe name, and the results\n",
    "    for each endpoint.\n",
    "\n",
    "    The cookbook names are prefixed with \"Cookbook:\" and are displayed with their overall grades. Each recipe under a\n",
    "    cookbook is indented and prefixed with \"Recipe:\" followed by its individual grades for each endpoint. If there are\n",
    "    no results for a cookbook, a row with dashes across all endpoint columns is added to indicate this.\n",
    "\n",
    "    Args:\n",
    "        cookbooks (list): A list of cookbook names to display in the table.\n",
    "        endpoints (list): A list of endpoints for which results are to be displayed.\n",
    "        results (dict): A dictionary containing the benchmarking results for cookbooks and recipes.\n",
    "\n",
    "    Returns:\n",
    "        None: The function prints the table to the console but does not return any value.\n",
    "    \"\"\"\n",
    "    table = Table(\n",
    "        title=\"Cookbook Result\", show_lines=True, expand=True, header_style=\"bold\"\n",
    "    )\n",
    "    table.add_column(\"No.\", width=2)\n",
    "    table.add_column(\"Cookbook (with its recipes)\", justify=\"left\", width=78)\n",
    "    for endpoint in endpoints:\n",
    "        table.add_column(endpoint, justify=\"center\")\n",
    "\n",
    "    index = 1\n",
    "    for cookbook in cookbooks:\n",
    "        # Get cookbook result\n",
    "        cookbook_result = next(\n",
    "            (\n",
    "                result\n",
    "                for result in results[\"results\"][\"cookbooks\"]\n",
    "                if result[\"id\"] == cookbook\n",
    "            ),\n",
    "            None,\n",
    "        )\n",
    "\n",
    "        if cookbook_result:\n",
    "            # Add the cookbook name with the \"Cookbook: \" prefix as the first row for this section\n",
    "            endpoint_results = []\n",
    "            for endpoint in endpoints:\n",
    "                # Find the evaluation summary for the endpoint\n",
    "                evaluation_summary = next(\n",
    "                    (\n",
    "                        temp_eval\n",
    "                        for temp_eval in cookbook_result[\"overall_evaluation_summary\"]\n",
    "                        if temp_eval[\"model_id\"] == endpoint\n",
    "                    ),\n",
    "                    None,\n",
    "                )\n",
    "\n",
    "                # Get the grade from the evaluation_summary, or use \"-\" if not found\n",
    "                grade = \"-\"\n",
    "                if evaluation_summary and evaluation_summary[\"overall_grade\"]:\n",
    "                    grade = evaluation_summary[\"overall_grade\"]\n",
    "                endpoint_results.append(grade)\n",
    "            table.add_row(\n",
    "                str(index),\n",
    "                f\"Cookbook: [blue]{cookbook}[/blue]\",\n",
    "                *endpoint_results,\n",
    "                end_section=True,\n",
    "            )\n",
    "\n",
    "            for recipe in cookbook_result[\"recipes\"]:\n",
    "                endpoint_results = []\n",
    "                for endpoint in endpoints:\n",
    "                    # Find the evaluation summary for the endpoint\n",
    "                    evaluation_summary = next(\n",
    "                        (\n",
    "                            temp_eval\n",
    "                            for temp_eval in recipe[\"evaluation_summary\"]\n",
    "                            if temp_eval[\"model_id\"] == endpoint\n",
    "                        ),\n",
    "                        None,\n",
    "                    )\n",
    "\n",
    "                    # Get the grade from the evaluation_summary, or use \"-\" if not found\n",
    "                    grade = \"-\"\n",
    "                    if (\n",
    "                        evaluation_summary\n",
    "                        and \"grade\" in evaluation_summary\n",
    "                        and \"avg_grade_value\" in evaluation_summary\n",
    "                        and evaluation_summary[\"grade\"]\n",
    "                    ):\n",
    "                        grade = f\"{evaluation_summary['grade']} [{evaluation_summary['avg_grade_value']}]\"\n",
    "                    endpoint_results.append(grade)\n",
    "\n",
    "                # Add the recipe name indented under the cookbook name\n",
    "                table.add_row(\n",
    "                    \"\",\n",
    "                    f\"  └──  Recipe: [blue]{recipe['id']}[/blue]\",\n",
    "                    *endpoint_results,\n",
    "                    end_section=True,\n",
    "                )\n",
    "\n",
    "            # Increment index only after all recipes of the cookbook have been added\n",
    "            index += 1\n",
    "        else:\n",
    "            # If no results for the cookbook, add a row indicating this with the \"Cookbook: \" prefix\n",
    "            # and a dash for each endpoint column\n",
    "            table.add_row(\n",
    "                str(index),\n",
    "                f\"Cookbook: {cookbook}\",\n",
    "                *([\"-\"] * len(endpoints)),\n",
    "                end_section=True,\n",
    "            )\n",
    "            index += 1\n",
    "\n",
    "    # Display table\n",
    "    console.print(table)\n",
    "\n",
    "if result_info:\n",
    "    show_cookbook_results(\n",
    "        cookbooks, endpoints, result_info, result_info[\"metadata\"][\"duration\"]\n",
    "    )\n",
    "else:\n",
    "    raise RuntimeError(\"no run result generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb23bf7-6705-4e19-9ffd-ea350731cfe7",
   "metadata": {},
   "source": [
    "## List all the Cookbook\n",
    "\n",
    "If you are curious what are the other cookbooks available, you can use `api_get_all_cookbook()`.\n",
    "\n",
    "Here's how it will look like in the output. To run these cookbooks, just replace `leaderboard-cookbook` with one of the cookbook IDs or you can append more cookbook IDs to the list in the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1ab399-82ee-430f-8477-24f6e40ab9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cookbook_ids = api_get_all_cookbook()\n",
    "print(\"Total number of cookbooks: {0}\".format(len(cookbook_ids)))\n",
    "print(\"Showing the first three cookbooks below...\")\n",
    "print(json.dumps(cookbook_ids[0:3], indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-virtualenv-name-2",
   "language": "python",
   "name": "my-virtualenv-name-2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
